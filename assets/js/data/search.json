[ { "title": "大型任务的多进程优化过程记录，从3个月优化到3天", "url": "/posts/%E8%AE%B0%E4%B8%80%E6%AC%A1%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E8%BF%87%E7%A8%8B/", "categories": "课程", "tags": "性能优化", "date": "2022-12-27 00:00:00 +0800", "snippet": "事情起因是这样的，一天学长找到我说:”学弟啊，你有没有空啊。现在跑机器学习实验，跑3个月结果还跑不出来，你能不能帮我优化一下代码啊？”我心想：“一次实验跑三个月，万一写了一点bug，实验不又得重头再来三个月，这也太令人头皮发麻了吧？“于是我开始了实验代码优化之旅。1.整体思路该项目整体代码量非常多，实验室内已经维护了3~4年，整个项目构建在Pytorch上，会到大数据集进行聚类学习。一次实验涉及到的数据量很大，基本是TB级别。仔细看了实验项目框架和执行过程，一时间没有优化思路。问了学长，目前所有实验执行都是单核CPU，没有多线程多进程。那优化思路不就有了——并行化，如果可以开多个进程同时跑实验，效率直接可以翻几十倍（当然最终受限于硬件资源）。要代码并行化，要满足以下几个条件 有大量的、重复的、类似的任务，CPU密集型任务 目前实验时间开销都主要用在聚类操作中，一次实验大概要做800w次聚类，平均一次聚类1s。单核跑大概需要3个月。 并行任务之间不能有依赖关系 现有聚类之间有弱依赖关系，但是通过代码逻辑修改可以去掉依赖关系。 因此想到可以把聚类运算给单独拿出来，用多线程聚类来提升效率。多线程并行化跑大量任务，很容易就想到MapReduce计算框架。 将计算过程分为Map，Reduce两个过程。将大任务分为多个子任务，提前准备好每个子任务的输入数据。Map过程多个Worker（线程、进程）执行执行，完成子任务。Reduce过程将所有子任务结果聚合，形成最终大任务结果。那么整体的实现思路就有啦： 数据准备：输出聚类输入数据 任务拆分：将聚类输入分为多个子任务 多机执行：多进程、多机器并行执行聚类 结果合并：汇聚聚类结果得到最终结果2.数据准备要跑并行化代码，首先要把任务所需要的输入数据准备好，即导出聚类输入数据。代码执行过程分析聚类数据导出是比较典型的IO密集型任务，可是测试了一下，发现把聚类数据导出就要几天时间。理论上磁盘IO速度没有这么慢，因此怀疑数据输出过程中有复杂的CPU运算。尝试用pyinstrument工具分析了代码的执行过程和每个过程的执行时间。 Pyinstrument 是一个Python 分析器。 以可视化的形式帮助您优化代码，使其更快。 输出信息包含了记录时间、线程数、总耗时、单行代码耗时、CPU执行时间等信息。这一分析就发现一个大问题，可视化结果如下图。输出聚类函数也就是_out_clustering_values执行了311s，其中有137s的时间浪费在一个unique函数上。仔细分析了对应的代码，unique函数每次都要对几百万的数据做去重，仅仅只为了计算聚类数。然而这根本没有必要，因为一般聚类数远小于聚类输入数据（比如100w数据 聚成128类），这样可以把聚类数设为一个定值。果断将unique函数去掉，整体效率提升45%。除开unique函数，代码执行过程中还发现一些其他没有必要的小函数，也一并做了优化或去除。IO密集型优化对于数据导出，没有复杂的CPU运算，是比较典型的IO密集型任务。对于IO密接型任务的优化主要从两方面进行优化 提升单次磁盘写入数量： 仔细看了学长的代码，写入文件用的是默认的buffer，只有8192byte。而导出的数据量级为TB，buffer太小导致IO次数过多。 通过加上一个大IO缓冲buffer（比如64MB），大幅减少IO次数。 减少总体数据大小： 实验用到的聚类数据都是Float型，学长将Float转为字符形式然后写入文件。但是更优化的方法是转为二进制形式，相比于字符形式表示的Float，二进制空间减少接近4倍。 根据计算，输出的数据总共有10TB，而服务器的磁盘可用容量只有7T，优化后输出数据为2.1TB。 这期间还尝试过把数据先压缩后再写入磁盘，但发现压缩本身相较于磁盘IO更耗费时间，遂放弃。 3. 任务拆分一次实验聚类数据有2T左右，不可能一次性load进内存进行运算，因此需要对聚类数据做拆分，形成一个个小任务。小任务并发执行，然后将小任务的执行结果进行合并。任务拆分本身逻辑比较简单，但是有几个注意点： 聚类数据间有前后顺序，因此要对拆分的任务做好编号，之后需要按顺序合并回去。 任务拆分大小要适中 任务过大会导致一次任务执行过长，单次失败成本高、此外内存开销大。 任务过小导致每次任务执行太快，CPU忙于切换进程而影响性能，此外任务数量太多不方便查看 最终的实现是分两层对数据做拆分，第一层根据机器学习模型的各层划分，第二层按照聚类数据，每64~256MB切分为一个子任务。分层的任务划分可以给任务带上业务属性，方便后续重复实验。因为实验优化往往是针对某几层做优化，这样只需要重新跑这几层的任务即可。4. 多机执行任务拆分好后，就可以使用多进程同时执行任务，充分压榨硬件性能。这里面也有两步优化，第一步是单机多进程，第二步是多机多进程单机多进程python由于GIL(全局解释器锁)的存在，没法真正意义上的并发执行多线程。所以需要使用python多进程，这里选择了python官方库multiprocessing中的进程池管理多进程。除开实现多进程聚类的功能需求外，还自己实现了一些非功能性需求： 日志输出： 对于多进程，同时读写一个日志文件会导致竞争问题。这里选择了concurrent_log_handler对日志文件进行加锁，避免并行冲突。 支持随时启停： 任务执行过程中可能由于各自原因失败了，要支持任务从失败的地方继续跑。 解决思路是建立tmp目录，执行过程中的数据保存在tmp目录下，任务完成后将执行结果拷贝到result文件夹下。 此外单个任务切分不要太大，这样可以降低执行失败时的成本。 失败任务重试： 任务执行过程中可能由于各自原因失败了，将自动重试几次。失败任务将单独打印日志，便于分析失败原因。 结果完整性检查： 多进程任务执行完后，需要对结果文件总数量进行校验。如果结果文件少于预期，将自动重新执行未完成的任务。 其他一些统计功能： 支持查看进度、总任务数。 支持测算时间、预计时间。 看了下实验室服务器有16个核，32个线程，效率应该可以翻32倍，理论上3个月的任务，3天应该可以跑完。下面是任务执行输出的日志部分截图：多机多进程实验室有多台机器，如果多机一起跑聚类，可以进一步提升效率。多台机器同时执行任务，首先要解决的问题是聚类数据同步到多个机器上。这里有几个选择： 直接通过移动硬盘拷贝，把输入数据copy到其他机器上 不可行：当时正值10-1国庆期间，实验室管机房的老师都下班了，机房进不去。 在其他机器上重新导出一份聚类输入数据 理论可行，但不是最优的：几台机器配置各不相同，有些机器硬盘容量只有500G，而整个待聚类数据有2.1T。如果只导出部分聚类数据，需要针对不同机器改代码，颇为繁琐且容易出错。 其他机器到主机器上下载子任务，下载多少就执行多少 这是最终选择的方案，其他机器通过ssh下载主服务器上的任务数据，每下载一个子任务，就启动一个进程执行。这样屏蔽了各个机器的配置差异，且支持添加新机器，老机器下线。 于是写了一份工具类File_Transfer，使用ssh 从远端拉取数据的代码。现在相当于获取任务数据有两种方式，一种是从本地直接读取，一种是从远端通过网络拉取。 这期间还尝试过很多种下载数据的方法，比如rsync 、scp等命令。 但这些命令方法没法和多进程脚本结合起来，且不好管理数据量，因此最后还是自己写了一份python代码，做更精细化的管理。多机跑出实验结果后，先各自进行结果文件的合并，然后多机再把结果文件传到主服务器上进行第二次合并。因为结果文件比较小，只有100MB不到，因此也不需要额外的优化。终于并行化实验顺顺利利的跑起来了，整个聚类过程从原来的3个月跑不出来，到现在3天跑出结果。性能优化明显。不过这只是整个实验的一小步，距离做完整个实验，似乎还有一大段路要走。其他在这次优化过程中，还学到了许多linux指令，用到了许多曾经学到过的知识 统计当前目录下文件的个数（包括子目录），用来统计任务总数和执行情况 ls -lR| grep &quot;^-&quot; | wc -l 统计文件大小 du -sh linux查看网络使用情况 iftop -n linux查看磁盘IO情况 iotop python 内存占用分析 Memray python 耗时分析 pyinstrument " }, { "title": "k8s日志采集框架搭建", "url": "/posts/k8s%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA/", "categories": "k8s", "tags": "k8s", "date": "2022-12-06 00:00:00 +0800", "snippet": "学校信息办需要搭建统一的日志中心，将大部分服务的日志都迁移到日志中心上来。之前查看日志都是登录到应用部署的机器上，手动查看日志文件，很不方便。有了日志中心后，可以在一处查看所有服务的日志，方便运维管理，快速排查问题。这些天基于ELK+Filebeat在k8s集群上搭建了一套日志采集+日志存储+日志检索的日志中心服务，整个中间也遇到了很多问题，也学到了许多，在此做些记录。日志采集架构k8s官方提供了三种日志采集的解决方案，分别是节点日志采集代理，以 sidecar 容器收集日志，直接从应用程序收集日志。节点日志采集代理这种方案是在k8s每个节点都部署一个日志采集代理，节点上运行的容器将日志输出到文件，然后日志采集代理分析收集日志文件，并发送给远端的日志系统，比如Elasticsearch。常见的日志采集代理有Logstash，Filebeat，Flunetd。可以参考日志采集代理的对比。以 sidecar 容器收集日志在应用部署的同一个pod下部署日志采集代理收集应用产生的日志。和节点日志采集不同，这个粒度更细，但操作起来更繁琐。直接从应用程序收集日志最后一种是让应用重新直接将发送日志到日志系统，比如通过tcp、udp协议。但这种方案对应用程序性能有影响，应用程序端有感知，产生耦合。Filebeat日志采集Filebeat是一款轻量型日志采集器，相比于logstash需要消耗500M左右内存，Filebeat只需要10M左右内存就可以完成工作。Filebeat和Elasticsearch的结合也非常不错。在我们的方案中选择Filebeat在节点级别做日志采集代理。Filebeat部署配置Filebeat如何在k8s上部署，官方文档给出了教程。官方教程以DaemonSet形式部署Filebeat。DaemonSet是在k8s每个节点上都会运行相同的容器，只需要在一处配置，就可以覆盖所有节点。非常适合我们节点级日志采集方案，按照官方教程走基本没有问题，但我部署时遇到几个坑记录下。连接不上ES最开始部署的时候，Filebeat一切正常，单看日志没有任何问题，但是在Elasticsearch搜不到日志。出现这种情况，可以用下面命令测试Filebeat 的输出是否正常。首先进入Filebeat部署的容器，默认在fielbeat的安装目录。执行下面命令filebeat test output#可以看到输出，下面表示链接正常elasticsearch: https://xxxx:15003... parse url... OK connection... parse host... OK dns lookup... OK addresses: xxxx dial up... OK TLS... security... WARN server&#39;s certificate chain verification is disabled handshake... OK TLS version: TLSv1.3 dial up... OK talk to server... OK version: 7.10.2如果链接不上，会给出报错信息，可以据此搜索解决。除此之外，第一次配置建议将Filebeat的日志级别改为Debug（默认为info），这样可以帮助我们很快的排查问题。具体设置如下：在filebeat.yml文件中加上一行logging.level: debugFilebeat版本不对Filebeat的版本需要和Elasticsearch保持一致。我在部署时，Elasticsearch使用的是7.10.2的版本，而Filebeat选择了最新版8.3，就会导致如下错误。将Filebeat换成同样换成7.10.2的版本就没有这个问题了。{&quot;error&quot;:{&quot;root_cause&quot;:[{&quot;type&quot;:&quot;invalid_index_name_exception&quot;,&quot;reason&quot;:&quot;Invalid index name [_license], must not start with &#39;_&#39;.&quot;还需要注意的是，Filebeat分为商业版和社区版（后缀带OSS），如果Elasticsearch使用的社区版，那么Filebeat就只能使用社区版。使用商业版会提示以下错误：Connection marked as failed because the onConnect callback failed: Filebeat requires the default distribution of Elasticsearch. Please update to the default distribution of Elasticsearch for full access to all free features, or switch to the OSS distribution of Filebeat.容器日志不在标准位置Filebeat报错找不到文件 No such file or directory，但进入容器后ls查看，发现目录下存在文件，百思不得其解。这可能的原因是容器日志并未输出在默认位置/var/lib/docker/containers，而是被移动到其他目录下。而filebeat在pod收集的日志文件，需要通过软链接的方式，指向到/var/lib/docker/containers目录中。导致日志文件没有被挂载到filebeat pod中，软链接指向到了一个不存在的文件。这可以通过查看日志文件实际软连接的位置来确定。具体命令如下：查看软连接实际指向位置namei /var/log/containers/xxx.log然后修改filebeat的挂载目录，挂载日志实际指向目录。可以参考https://blog.csdn.net/zss_89/article/details/122232538Filebeat 配置文件设置我希望实现只收集属于我的项目的日志，根据环境不同输出到Elasticsearch的不同index。测试环境以test开头，生产环境以prod开头。参考官网文档设置processss配置下方配置主要添加了kubernetes相关的元数据，然后去掉了一些无用字段。通过kubernetes元数据，可以根据命名空间和部署应用名不同来输出到不同的index中。 processors: - add_kubernetes_metadata: host: ${NODE_NAME} matchers: - logs_path: logs_path: &quot;/var/log/containers/&quot; #删除的多余字段 - drop_fields: fields: [&quot;host&quot;, &quot;tags&quot;, &quot;ecs&quot;, &quot;log&quot;, &quot;prospector&quot;, &quot;agent&quot;, &quot;input&quot;, &quot;beat&quot;, &quot;offset&quot;] ignore_missing: true - drop_fields: fields: [&quot;kubernetes.pod.uid&quot;,&quot;kubernetes.namespace_uid&quot;,&quot;kubernetes.namespace_labels&quot;, &quot;kubernetes.node.uid&quot;,&quot;kubernetes.node.labels&quot;,&quot;kubernetes.replicaset&quot;, &quot;kubernetes.labels.pod-template-hash&quot;,&quot;kubernetes.labels.tcs_region&quot;,&quot;kubernetes.labels.tcs_zone&quot;] ignore_missing: true - drop_fields: fields: [&quot;container&quot;] ignore_missing: trueindexs配置可以通过indices配置，以任意日志中的数据项作为筛选，输出到不同的index中。比如下方，通过判断kubernetes.namespace命名空间来设置不同前缀。 output.elasticsearch: hosts: [&#39;https://${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}&#39;] username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} indices: - index: &quot;test-%{[kubernetes.labels.app]}&quot; when.equals: kubernetes.namespace: &quot;open-platform-test&quot; - index: &quot;prod-%{[kubernetes.labels.app]}&quot; when.equals: kubernetes.namespace: &quot;open-platform&quot;最终效果花费了几天时间，不断查找资料终于实现了比较满意的结果。最终效果可以在Kibana页面统一查看所有服务的日志。每个服务按照环境分开单独生成一份日志index。由于日志数量不大，暂定每个月进行一次分片，后续观察日志的大小再做调整。" }, { "title": "Raft分布式一致性协议的实现思路", "url": "/posts/MIT%E5%88%86%E5%B8%83%E5%BC%8F%E8%AF%BE%E7%A8%8B%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF/", "categories": "分布式", "tags": "分布式", "date": "2022-11-03 00:00:00 +0800", "snippet": "暑期的时候参加了麻省理工大学的分布式系统课程（MIT 6.824 ），其中的课程实验要求实现Raft分布式一致性协议，一致性协议的应用是为了维护各节点数据的一致性，换句话说，就是使集群里的节点存储的数据是一模一样的。实现Raft协议后，需要在此基础上实现基于Raft的kv数据库。项目开发语言是GO，之前并没有接触过，也顺便跟着课程实验学一学。目前已经完成了前三个课程实验，实现了一个基于Raft的分布式KV数据库。实现代码已经开源，项目地址：https://github.com/TjFish/MIT-6.824-2022已完成内容如下： lab1 MapReduce lab2 Raft 2A leader election 2B log 2C persistence 2D log compaction lab3 kvraft lab4 shardkvRaft 协议代码设计与实现测试结果参考资料 Raft论文 Raft协议详解 - 丁凯的文章 - 知乎 Students’ Guide to Raft Lab guidance yzongyue/6.824-golabs-2020Figure2 镇楼代码整体交互流程多线程编程项目整体架构就是多节点，每个节点有多个线程。多线程编程就会涉及到线程竞争、线程安全的问题。一般通过加锁或者条件等待的形式来保证线程安全。这里介绍下我总结的多线程加锁的代码实现模式，供大家参考。首先一个准则是：所有修改状态的操作前，都需要先检查当前状态是否满足修改条件。或者说，修改状态应该在当前线程完成，不能委托给其他线程。 常见的模式一（互斥锁）： 获取状态锁 判断修改条件是否满足 修改状态 释放锁 常见的模式二（条件等待）： 获取状态锁 循环检查条件，不满足则释放锁，等待被唤醒Cond.Wait() 被唤醒，循环检查条件是否满足 满足，跳出循环 不满足，继续循环 修改状态 释放锁 一个不容易发现的错误模式 首先获取状态锁 判断修改条件满足后 启动新线程去修改状态 释放锁 这有并发性问题，因为新线程启动去修改状态时，可能当前状态已经被其他线程修改，原先的修改条件已经不满足。 RPC消息线程代码实现模式Raft实现中，需要发送AppendEntries 和 RequestVote 两类RPC消息。在【Lab2D】，还需要实现Install Snapshot RPC消息。这些消息如何发送和接收处理，是代码实现最核心也是最困难的部分，许多Bug都隐藏在这里。这里介绍下我总结的统一RPC消息线程的代码实现模式，供大家参考。由于发送消息会阻塞本线程（需要等待消息回复），因此一般需要在一个新线程中发送消息。同时MIT的老师建议：在接收消息回复后直接在本线程中进行处理，最好不要将消息回复传递给其他线程处理。因此RPC消息的基本实现模式是，在同一个线程内： 获取锁 生成RPC消息参数 释放锁（因为发送RPC消息会导致线程阻塞，需要释放锁） 发送RPC消息，等待消息回复 再次获取锁 处理消息回复 释放锁再考虑何时触发发送RPC消息，即消息发送条件，这有两种选择全部实现在一个线程中：在RPC消息线程中加入循环检查，判断是否满足消息发送条件，一旦满足条件则发送RPC消息，代码流程如下： 获取锁 判断RPC消息发送条件是否满足 生成RPC消息参数 释放锁 发送RPC消息，等待消息回复 再次获取锁 处理消息回复 释放锁 循环分为两个线程：检查线程和工作线程：检查线程负责循环检查发送条件，一旦满足条件则新启动一个工作线程发送RPC消息，工作线程负责发送消息并处理消息。代码流程如下： 条件检查线程 获取锁 判断RPC消息发送条件是否满足 生成RPC消息参数 启动RPC消息线程 释放锁 循环 RPC消息线程 发送RPC消息 等待消息回复 获取锁 处理消息回复 释放锁 采用第一种方案的好处是可以避免多线程交互，虽然看上去似乎简单直接，但实际实现起来很容易猪脑过载，因为在一个线程内塞进去了大量逻辑和条件判断语句，整个代码非常不好理解。之后调试已经增加新的逻辑时很痛苦。第二种方案将职责进行了分离，代码实现起来相对比较简单。但引入了多个线程，需要格外考虑线程交互的问题。思考：当RPC消息线程被调度工作时，有可能此时消息发送条件已经不满足了，该怎么办？ 这并不会有问题，因为Raft协议是支持处理过期消息的（前提是消息本身是正确的）。即使发送消息时条件已经不满足，接收方也能判断出消息已经过期，忽略此消息。从另外一个角度想，消息在网络中传递也需要时间，由于网络波动，即使消息发出时是有效的，消息抵达时也可能已经过期。所以无论检查多少次发送条件都没有意义，消息都有可能过期。我们需要时刻保证消息本身是正确的，这意味着在条件检查线程中生成正确的RPC消息参数，交由RPC消息线程。RPC消息线程不再检查是否满足条件，直接发送消息。切忌不可在RPC消息线程内生成消息参数，如果要这么做就需要再次检查消息发送条件，但这就退化成了第一种方案。同时这也提醒我们在代码实现时，需要格外注意，过期的消息，重复的消息，乱序的消息。我们的代码需要足够的鲁棒性能够处理这些错误。Log数据结构设计思路论文中使用log数组顺序存放所有LogEntry，数组 index 从 1 开始。而在代码实现中，Go数组index 从 0开始。直接Go数组存放log会带来两个问题： 代码中充斥大量的index转换逻辑，如log index = array index + 1， array index = log index -1，很容易把人给弄晕，而往往bug就是隐藏在这 加一，减一的逻辑中（血的教训~~~）。 log数组有可能为空（这是正常的，比如刚启动时）。然而Raft中许多逻辑都依赖于log的状态，需要从log数组取值，比如最新的log(last log)。这导致代码中需要大量if else的判断语句来处理log数组为空的情况，称为特殊逻辑。针对这两个问题，设计数据结构LogCache将日志的逻辑都封装起来，整体设计思路如下： 内置index转换：LogCache只接收log index，内部封装log index 与 array index的转换逻辑。 数组永远不为空：初始化时，在数组 index=0 的位置存入临界值（index=0，Term=0）这样使用统一的代码就可以处理数组为空的情况。 提供语义接口：提供更高语义的接口，封装重复且复杂的数组操作。外部不能直接访问底层数组，提高代码可读性。 get(index): 获取log index = index 的日志 after(index): 获取log index&amp;gt; index的所有日志。 last(): 获取最后一条日志。 此外，在【Lab2D】中，将会删除已保存的日志。LogCache可以很好兼容，将array index与 log index的偏移从 1 变为 lastIncludeIndex即可。注意事项有且只有三个情况需要重置timer，其他情况不要重置timer 收到AppendEntries RPC消息，且消息未过期 开始选举时 投票给其他Candidate时 【Lab2D新增】收到Install Snapshot RPC消息（原因类似于AppendEntries RPC）注意避免死锁 在获取锁后一定记得释放锁 建议在函数进入时获取锁，使用defer语句在函数返回前释放锁。 【Lab2D】需要实现对Raft状态的快照（snapshot)，具体有以下几个需求 实现Snapshot()保存快照，删除内存多余的log。 实现InstallSnapshot RPC，当leader发现内存中log不足以同步follower时，使用快照同步状态。follower接收到快照时，能够根据快照恢复状态，并且删除过期log。 奔溃重启时，从保存快照恢复状态机，从保存的Raft State恢复Raft数据。可视化日志调试设置由于分布式系统开发调试起来非常困难，打印的日志太多眼睛都看花，因此很有必要整理下日志输出，设置可视化界面来帮助分析找bug。这篇文章介绍了如何设置可视化日志。最终效果：go代码改动在src/raft/util.go 下加入以下代码const debug = truetype logTopic stringconst ( dClient logTopic = &quot;CLNT&quot; dCommit logTopic = &quot;CMIT&quot; dDrop logTopic = &quot;DROP&quot; dError logTopic = &quot;ERRO&quot; dInfo logTopic = &quot;INFO&quot; dLeader logTopic = &quot;LEAD&quot; dLog logTopic = &quot;LOG1&quot; dLog2 logTopic = &quot;LOG2&quot; dPersist logTopic = &quot;PERS&quot; dSnap logTopic = &quot;SNAP&quot; dTerm logTopic = &quot;TERM&quot; dTest logTopic = &quot;TEST&quot; dTimer logTopic = &quot;TIMR&quot; dTrace logTopic = &quot;TRCE&quot; dVote logTopic = &quot;VOTE&quot; dWarn logTopic = &quot;WARN&quot;)// Retrieve the verbosity level from an environment variablefunc getVerbosity() int { v := os.Getenv(&quot;VERBOSE&quot;) level := 0 if v != &quot;&quot; { var err error level, err = strconv.Atoi(v) if err != nil { log.Fatalf(&quot;Invalid verbosity %v&quot;, v) } } return level}var debugStart time.Timevar debugVerbosity intfunc init() { debugVerbosity = getVerbosity() debugStart = time.Now() log.SetFlags(log.Flags() &amp;amp;^ (log.Ldate | log.Ltime))}func Debug(topic logTopic, format string, a ...interface{}) { if debug { time := time.Since(debugStart).Microseconds() time /= 100 prefix := fmt.Sprintf(&quot;%06d %v &quot;, time, string(topic)) format = prefix + format log.Printf(format, a...) }}在你想要打印日志的地方调用Debug函数，比如在初始化raft的make函数中加入func Make(peers []*labrpc.ClientEnd, me int, //.... go rf.ticker() Debug(dInfo, &quot;Make: %d&quot;, rf.me) // 加入日志打印函数 return rf}python脚本设置首先安装python和pipsudo apt-get install python3sudo apt-get install python3-pip然后安装python脚本所需依赖pip install typerpip install rich下载脚本，将脚本设置为可执行wget https://gist.github.com/JJGO/e64c0e8aedb5d464b5f79d3b12197338chmod +x dslogs注意，如果本地只有python3环境，需要修改脚本第一行#!/usr/bin/env python改为-&amp;gt;#!/usr/bin/env python3将脚本加入环境变量sudo vi /etc/profile# 将这行加在文件最后，记得替换地址export PATH=$PATH:/daslogs文件所在地址/source /etc/profile测试下效果cd src/raftVERBOSE=1 go test -run InitialElection | dslogs效果如下批量测试脚本设置批量测试脚本可以帮我们并行，重复跑多个测试。效果如下：下载脚本并设置可执行cd /项目地址wget https://gist.githubusercontent.com/JJGO/0d73540ef7cc2f066cb535156b7cbdab/raw/079a847924bcb538cb9336d381961ce608ac1244/dstestchmod +x dstest注意，如果本地只有python3环境，需要修改脚本第一行#!/usr/bin/env python改为-&amp;gt;#!/usr/bin/env python3查看使用手册dstest --help测试下cd src/raftdstest InitialElection效果如下" }, { "title": "领域驱动设计学习总结与记录", "url": "/posts/DDD%E5%85%A5%E9%97%A8%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-10-18 00:00:00 +0800", "snippet": "下面的文章是我看过后，非常推荐的领域驱动设计文章，值得用心学习记录。 领域驱动设计学习总结与记录 详解DDD系列 第一讲 - Domain Primitive 详解DDD系列 第二讲 - 应用架构 详解DDD系列 第三讲 - Repository模式 详解DDD系列 第四讲 - 领域层设计规范 详解DDD系列 第五讲 - 聊聊如何避免写流水账代码 结合电商支付业务一文搞懂DDD CQRS与EventSourcing的架构设计 设计模式-何时使用继承？详细了解装饰器模式 设计模式-策略模式，封装复杂的领域知识领域驱动设计的概念 软件开发不是一蹴而就的事情，我们不可能在不了解产品(或行业领域)的前提下进行软件开发，在开发前通常需要进行大量的业务知识梳理，然后才能到软件设计的层面，最后才是开发。而在业务知识梳理的过程中，必然会形成某个领域知识，根据领域知识来一步步驱动软件设计，就是领域驱动设计(DDD,Domain-Driven Design)的基本概念 。为什么需要 DDD　在业务初期，功能大都非常简单，普通的 CRUD 就基本能满足要求，此时系统是清晰的。但随着产品的不断迭代和演化，业务逻辑变得越来越复杂，我们的系统也越来越冗杂。各个模块之间彼此关联，甚至到后期连相应的开发者都很难说清模块的具体功能和意图到底是啥。这就会导致在想要修改一个功能时，要追溯到这个功能需要修改的点就要很长时间，更别提修改带来的不可预知的影响面。 比如下图所示:　　订单服务中提供了查询、创建订单相关的接口，也提供了订单评价、支付的接口。同时订单表是个大表，包含了非常多字段。我们在维护代码时，将会导致牵一发而动全身，很可能原本我们只是想改下评价相关的功能，却影响到了创建订单的核心流程。虽然我们可以通过测试来保证功能的完备性，但当我们在订单领域有大量需求同时并行开发时将会出现改动重叠、恶性循环、疲于奔命修改各种问题的局面，而且大量的全量回归会给测试带来不可接受的灾难。　　但现实中绝大部分公司都是这样一个状态，然后一般他们的解决方案是不断的重构系统，让系统的设计随着业务成长也进行不断的演进。通过重构出一些独立的类来存放某些通用的逻辑解决混乱问题，但是我们很难给它一个业务上的含义，只能以技术纬度进行描述，那么带来的问题就是其他人接手这块代码的时候不知道这个的含义或者只能通过修改通用逻辑来达到某些需求。领域模型追本溯源　实际上，领域模型本身也不是一个陌生的单词，说直白点，在早期开发中，领域模型就是数据库设计。因为你想：我们做传统项目的流程或者说包括现在我们做项目的流程，都是首先讨论需求，然后是数据库建模，在需求逐步确定的过程不断的去变更数据库的设计，接着我们在项目开发阶段，发现有些关系没有建、有些字段少了、有些表结构设计不合理，又在不断的去调整设计，最后上线。在传统项目中，数据库是整个项目的根本，数据模型出来以后后续的开发都是围绕着数据展开，然后形成如下的一个架构：很显然，这其中存在的问题如下： Service层很重，所有逻辑处理基本都放在service层。 POJO作为 Service 层非常重要的一个实体，会因为不同场景的需求做不同的变化和组合，就会造成 POJO 的几种不同模型(失血、贫血、充血)，以此用来形容领域模型太胖或者太瘦。 随着业务变得复杂以后，包括数据结构的变化，那么各 个模块就需要进行修改，原本清晰的系统经过不断的演 化变得复杂、冗余、耦合度高，后果就非常严重。　我们试想一下如果一个软件产品不依赖数据库存储设备，那我们怎么去设计这个软件呢？如果没有了数据存储，那么我们的领域模型就得基于程序本身来设计。那这个就是 DDD 需要去考虑的问题。DDD中的基本概念实体（Entity）　当一个对象由其标识(而不是属性)区分时，这种对象称为实体(Entity)。比如当两个对象的标识不同时，即使两个对象的其他属性全都相同，我们也认为他们是两个完全不同的实体。值对象（Value Object）　当一个对象用于对事物进行描述而没有唯一标识时，那么它被称作值对象。因为在领域中并不是任何时候一个事物都需要有一个唯一的标识，也就是说我们并不关心具体是哪个事物，只关心这个事物是什么。比如下单流程中，对于配送地址来说，只要是地址信息相同，我们就认为是同一个配送地址。由于不具有唯一标示，我们也不能说”这一个”值对象或者”那一个”值对象。聚合及聚合根（Aggregate，Aggregate Root）　聚合是通过定义领域对象之间清晰的所属关系以及边界来实现领域模型的内聚，以此来避免形成错综复杂的、难以维护的对象关系网。聚合定义了一组具有内聚关系的相关领域对象的集合，我们可以把聚合看作是一个修改数据的单元。　聚合根属于实体对象，它是领域对象中一个高度内聚的核心对象。(聚合根具有全局的唯一标识，而实体只有在聚合内部有唯一的本地标识，值对象没有唯一标识，不存在这个值对象或那个值对象的说法)　若一个聚合仅有一个实体，那这个实体就是聚合根；但要有多个实体，我们就要思考聚合内哪个对象有独立存在的意义且可以和外部领域直接进行交互。领域服务（Domain Service）　一些重要的领域行为或操作，它们不太适合建模为实体对象或者值对象，它们本质上只是一些操作，并不是具体的事物，另一方面这些操作又涉及到多个领域对象的操作，它们只负责来协调这些领域对象完成操作而已，那么我们可以归类它们为领域服务。它实现了全部业务逻辑并且通过各种校验手段保证业务的正确性。同时呢，它也能避免在应用层出现领域逻辑。应用服务（Application Service）​ 很薄的一层，定义软件要完成的任务，类似于USE CASE，通过编排各类领域服务完成服务。对外为展现层提供各种应用功能（包括查询或命令），对内调用领域层（领域对象或领域服务）完成各种业务逻辑，应用层不包含业务逻辑。工厂（Factory）　DDD中的工厂也是一种封装思想的体现。引入工厂的原因是：有时创建一个领域对象是一件相对比较复杂的事情，而不是简单的new操作。工厂的作用是隐藏创建对象的细节。事实上大部分情况下，领域对象的创建都不会相对太复杂，故我们仅需使用简单的构造函数创建对象就可以。隐藏创建对象细节的好处是显而易见的，这样就可以不会让领域层的业务逻辑泄露到应用层，同时也减轻应用层负担，它只要简单调用领域工厂来创建出期望的对象就可以了。仓储（Repository）　Repository封装了基础设施来提供查询和持久化聚合操作。这样能够让我们始终关注在模型层面，把对象的存储和访问都委托给资源库来完成。DDD 关心的是领域内的模型，而不是数据库的操作。​ 这种常见的设计模式叫做Anti-Corruption Layer（防腐层或ACL），防腐层是领域层与基础设施之间的桥梁。很多时候我们的系统会去依赖其他的系统，而被依赖的系统可能包含不合理的数据结构、API、协议或技术实现，如果对外部系统强依赖，会导致我们的系统被”腐蚀“。这个时候，通过在系统间加入一个防腐层，能够有效的隔离外部依赖和内部逻辑，无论外部如何变更，内部代码可以尽可能的保持不变。程序设计中DDD的分层结构DDD是一种建模思想，在实际程序设计中也可以对DDD的各类概念进行分层。虽然是分层结构，但是我更推荐称其为六边形结构，如下图。按照层次划分，各类DDD基本概念可以被分到如下层次中领域层（Domain Layer）领域建模中最重要的一层，包含所有领域知识和领域规则。 实体 Entity 值对象 Value Object 聚合根 Aggregate 领域服务 Domain Service 工厂 Factory应用层（Application Layer）定义应用的功能，复杂编排各类领域服务和基础服务完成应用功能。 应用服务 Application Service基础设施层（Infrastructure Layer）提供基础设施能力，这些能力不属于本系统领域所关注的。如数据存储、邮件发送、日志管理等 防腐层（Anti-Corruption Layer） 存储 Repository 其他基础服务 Infrastructure Service 各类基础服务具体实现 数据库 外部第三方系统 接口层（Interface Layer）为外部应用提供统一的接口，属于应用的门面Facade。调用应用层完成请求处理。 网页前端接口 Web Controller 内部服务接口 Thrift Controller" }, { "title": "《clean code》读后感", "url": "/posts/clean-code%E8%AF%BB%E5%90%8E%E6%84%9F/", "categories": "领域驱动设计DDD", "tags": "设计模式, 读后感", "date": "2022-10-06 00:00:00 +0800", "snippet": "读完《clean code》一书，有些想法，想要记录下来。《clean code》在一年前字节实习的时候就听同事推荐过，但一直没有下决心来看。本学期软件建模课程的刘岩老师又一次提起了它，在第一节课上，她提了两本书建议我们看看：《Domain Driven Design》和《clean code》。第一本书是关于领域驱动设计的，我看过后备受启发。这本书打开了我对面向对象的大门，让我发现了我之前写的代码（MVC）不过是一堆贫血模型和CRUD。本文不是谈DDD的，因此对《Domain Driven Design》也就点到为止。对于《clean code》，我在开始看前的刻板印象都是些繁碎教条，就像每个项目都有的《编码风格规范》一样。然而因为第一本书《DDD》的巨大“成功”，我开始尝试看《clean code》。相较于《Domain Driven Design》从宏观的、架构角度去谈软件设计，《clean code》本书从具体的、代码角度去谈软件开发，目标很明确，就是怎么写出“整洁”的代码。那么，何谓整洁的代码呢？我现在的理解是：权责分明，命名丰富，抽象到位。关于权责分明，指的就是单一职责的设计原则，即一个类或函数应该只做一件事，只承担一个职责。另一条设计原则是开闭原则，即对修改封闭，对扩展开放。只有当你的类做到很好地划分，权责分明时，你才可能做到对修改封闭，代码修改都将集中在一处。借助设计模式，你可以做到对扩展开放，新功能可以比较容易的添加。似乎这两个设计原则都很耳熟？似乎都听过但从来都不知道如何实践？是的我也是如此。在读《clean code》之前，我对这些设计原则嗤之以鼻，这就像置在高阁的圣经，都是对的，但有什么用呢？不过我现在开始逐渐理解了，或许是之前的编程的经验不够吧。《clean code》此书中作者给出了他开发遇到的现实的例子，来讲解为什么这样的代码是不好的、为什么修改后的代码是好的、哪些情况表明了职责过多、而怎样才体现了职责单一、哪里体现了开闭原则、又如何把握开闭原则。在这里我并不尝试去讲解这两个设计原则，因为我不认为我短短篇幅能讲得清楚，我建议你去亲自读读这本书。关于命名丰富，指的是对编码中的各类命名都要精心思考，每一次命名都是我们重新表达自己的机会。一个常见的反例是变量命名为a，b，c这种单字母，让读者不知所云。写代码和写散文没什么区别，你应该考虑读者的感受，更不用说下一次读这个代码的人很可能就是你自己。一个好的命名，可以让你代码读起来像一篇散文，函数名就像一个段落，说着一个新话题。我们需要珍惜每一次的命名，因为每一次命名都是我们重新表达自己的机会。原来的我，不知道如何去取个好名字，由于英语水平有限，我往往需要取很长的名字才能表达我的意思，我会怀疑取太长的名字不太好。而这本书给出了答案，短的但含义模糊的命名远比长的但含义明确的命名糟糕。关于抽象到位，我印象最深刻的点是“在同一抽象层次编码”。听到抽象，大家的印象往往是大层次上的抽象，比如架构层次抽象，设计模式抽象，类继承抽象这些概念。但抽象其实还能细致到函数、代码中间。函数间的抽象指的是，函数关注的点应该是单一的。一个函数不应该又负责底层细节操作，又调用同层其他函数。常见的情形就是胶水代码，A函数中要调用B函数，但A函数的输入参数不符合B函数的输入参数，因此A函数需要先对输入参数做转换，然后调用B函数，中间这样的转换就是胶水代码。如果这样的转换很简单，并且函数逻辑并不复杂，这样的代码也还能接受。如果胶水代码本身就很复杂了，读者就会淹没在转换的细节中，而没法理解函数真正的意图。这时候正确的做法是，将胶水代码抽离出去，作为一个单独函数。更进一步的是，将胶水代码涉及到数据结构抽象为一个单独的类，专门负责数据转换。你看，这里也有单一职责原则的体现哦。读完《clean code》，就像睁开了一只眼。有很多新想法就不细写了，包括单元测试是多么美妙，注释是一剂苦药。书籍的含金量远高于单独的帖子，很多网上的帖子都是拾人牙慧，倒不如追根溯源，去看看源头的书。系统性的书籍可比一个个单独的帖子好太多了。此外《clean code》的中文翻译得很不错，感谢翻译的付出。" }, { "title": "TLinux2.4系统安装与配置", "url": "/posts/TLinux2.4%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/", "categories": "k8s", "tags": "k8s", "date": "2022-09-25 00:00:00 +0800", "snippet": "学校k8s管理平台TCS需要新加机器，由于政策原因，只能使用国产系统tlinux。我之前玩过双系统，对系统安装这块不陌生，所以这次由我负责给新机器安装tlinux2.4系统，并配置好网络。系统安装tlinux iso下载tlinux2.4 下载地址 https://mirrors.cloud.tencent.com/tlinux/2.4/iso/页面上有TK3和TK4两个版本可选。调研了一下，两者主要是linux内核版本区别，这里选择最新版TK4。挂载iso到虚拟机本地需要安装vmware workstation，这样才能将本地iso文件挂载到远端虚拟机上。另外，安装系统需要从EFI上启动，首先要将虚拟机配置中的secure boot（安全引导）选项关闭。安装后再打开此选项。安装系统正常以EFI模式启动，学校建议系统只有一个分区，所以进入引导程序前要修改引导命令rootsize=max。这样安装后将不挂载/data目录，系统盘不分多个区，全部分在根目录下。大概等待5min等待系统安装完成，期间需要网络上传完整的iso文件。所有安装选项均采用默认选项，这些之后都可以再修改。配置网络系统安装完成后，还需要配置本机IP地址，网关地址，和DNS，否则没法访问网络。tlinux和centos7很类似，所以我参考的centos7的配置方法，阿里云的文档-如何在Linux实例中设置静态IP地址。（~我没找到腾讯的官方文档）首先进入网络配置文件目录，ls查看所有配置文件，一般命名方式是ifcfg-xx，我这里修改ifcfg-eth0。cd /etc/sysconfig/network-scriptsls修改ifcfg-eth0，将BOOTPROTO的值修改为static，并将以上步骤中记录的IP地址、网关信息、子网掩码填写到该配置文件中，并填写DNS1和DNS2地址，修改之后的内容如下所示vi ifcfg-eth0BOOTPROTO=staticDEVICE=eth0ONBOOT=yesSTARTMODE=autoTYPE=EthernetUSERCTL=noIPADDR=192.168.X.68NETMASK=255.255.255.0GATEWAY=192.168.X.253PEERDNS=noDNS1=[$DNS1]DNS2=[$DNS2] 说明： BOOTPROTO：静态IP，需要将其修改为static。 eth0：配置的网卡名称 IPADDR：配置的IP地址，得看虚拟机配置 NETMASK：配置的子网掩码，得看虚拟机子网设置 GATEWAY：配置的网关地址，得自己确定下 DNS1和DNS2：自定义配置的DNS信息，这里我配的学校的DNS服务器。 修改完成后，重启网络服务systemctl restart network这里我遇到了 Failed to start LSB: Bring up/down networking.的错误，分析发现是网卡MAC地址有错误。另外建议删掉多余的ifcfg-xx配置文件，避免影响正常配置文件。可以参考 各种姿势解决CentOS 7下无法启动网络的问题。网络服务正常启动后，测试网络是否正常。首先ifconfig命令查看网络设置，然后ping baidu看下是否联通。ifconfigping www.baidu.com参考资料 vSphere 虚拟机管理指南 TencentOS Server 操作系统安装指南 如何在Linux实例中设置静态IP地址 各种姿势解决CentOS 7下无法启动网络的问题 " }, { "title": "什么是分布式事务?", "url": "/posts/%E4%BB%80%E4%B9%88%E6%98%AF%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/", "categories": "分布式", "tags": "分布式", "date": "2022-09-19 00:00:00 +0800", "snippet": "分布式事务什么是分布式事务事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。事务的四个属性 Atomic，原子性。它意味着，事务可能有多个步骤，比如说写多个数据记录，尽管可能存在故障，但是要么所有的写数据都完成了，要么没有写数据能完成。不应该发生类似这种情况：在一个特定的时间发生了故障，导致事务中一半的写数据完成并可见，另一半的写数据没有完成，这里要么全有，要么全没有（All or Nothing）。 Consistent，一致性。它通常是指数据库会强制某些应用程序定义的数据不变，保持正常。 Isolated，隔离性。这一点还比较重要。这是一个属性，它表明两个同时运行的事务，在事务结束前，能不能看到彼此的更新，能不能看到另一个事务中间的临时的更新。目标是不能。隔离在技术上的具体体现是，事务需要串行执行。但是总结起来，事务不能看到彼此之间的中间状态，只能看到完成的事务结果。 Durable，持久化的。这意味着，在事务提交之后，在客户端或者程序提交事务之后，在数据库中的修改是持久化的，它们不会因为一些错误而被擦除。在实际中，这意味着数据需要被写入到一些非易失的存储（Non-Volatile Storage），持久化的存储，例如磁盘。而什么是分布式事务呢？ 事务更多指的是单机版、单数据库的概念。 分布式事务 指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上 。换成比较容易理解的话，就是多个事务之间再保持事务的特性，也就是多个事务之间保证结果的一致性。 分布式事务方案要实现分布式事务，主要考虑两个点，第一个是并发控制（Concurrency Control）第二个是原子提交（Atomic Commit）。并发控制实际上就是用来实现隔离性。前置知识对于事务的并发控制模型，一般有两个方向：悲观并发控制和乐观并发控制。前者适合于数据竞争严重且重试代价大的场景，后者适用于数据竞争不严重且重试代价不大的场景。悲观并发控制：使用锁。获取数据前加锁。其他事务如果也要使用相同的数据，就必须等待锁释放。在悲观系统中，如果有锁冲突，比如其他事务持有了锁，就会造成延时等待。所以这里需要为正确性而牺牲性能。乐观并发控制：你不用担心其他的事务是否正在读写你要使用的数据，你直接继续执行你的读写操作，通常来说这些执行会在一些临时区域，只有在事务最后的时候，你再检查是不是有一些其他的事务干扰了你。如果没有这样的其他事务，那么你的事务就完成了，并且你也不需要承受锁带来的性能损耗，因为操作锁的代价一般都比较高；但是如果有一些其他的事务在同一时间修改了你关心的数据，并造成了冲突，那么你必须要Abort当前事务，并重试。两阶段锁（Two-Phase Locking）：两阶段锁通常用来对单个事务进行并发控制，实现原子性。 扩展阶段：在执行任何数据的读写之前，先获取锁。持有并不断的累积所有的锁。 收缩阶段：持有锁直到事务结束。事务必须持有任何已经获得的锁，直到事务提交或者Abort，不允许在事务的中间过程释放锁。可能出现双方事务互相持有对方需要的锁，导致死锁问题。死锁的解决办法，超时释放重试，逐个锁释放等等。两阶段提交两阶段提交（Two-Phase Commit，2PC）是实现分布式事务中原子性的常见方案。两阶段提交中有协调者和参与者。协调者决定事务的开始、提交、回滚。参与者负责执行具体事务。下面是两阶段提交的正常流程，协调者和参与者都没有出现问题。参与者执行事务时通过两阶段锁进行并发控制。在回复协调者前，参与者将事务以日志形式写入磁盘，使得故障恢复时能够继续执行事务。如果参与者集群中任意节点出现问题，协调者需要对事务进行回滚。如果协调者出现问题，此时参与者事务已经执行但未提交，需要一直等待协调者的提交或者回滚消息。两阶段提交的问题： 效率低下：协调者和参与者之间需要多轮消息交互，参与者需要将事务写入磁盘才能回复协调者，效率低。 阻塞：如果协调者奔溃，所有参与者需要持有锁无限期等待协调者恢复。 容错低：如果任何一个参与者奔溃，事务要回滚。如果协调者奔溃，参与者要阻塞并等待。除开两阶段提交，还有三阶段提交（3PC），具体方案和2PC差距不大。由于两阶段提交的容错性很低，可以使用Raft一致性协议来组成集群，提升参与者和协调者的容错性。如下图，协调者TC由多台机器组成一个协调者集群，内部使用Raft协议达成一致，提升容错性。参考文章： 我还不懂什么是分布式事务 数据库的隔离级别与2PL/MVCC算法原理 WALWAL 全称是Write Ahead Log，预写日志。是数据库系统中常见的一种手段，用来提升性能，满足容错性。具体做法：考虑数据库事务场景，事务执行可能成功可能失败，如果失败需要回滚事务，撤销事务做的所有修改。 在事务提交前将预写日志写入磁盘持久化，日志中记录事务对数据的修改。 完成对内存中数据的修改。 最后提交事务。 在WAL累积到一定长度后，批量将数据持久化写入磁盘，清空WAL日志。考虑容错性： 如果提交事务前数据库奔溃，客户端重试即可。因为事务没有提交，一切修改都是内存中修改，数据库将自动忽略掉未提交的WAL日志。 如果提交事务后数据库奔溃，数据库重启时读取WAL日志，重新修改数据（redo)，恢复到奔溃前的状态。 WAL的优点： 如果不使用WAL，那么每次数据修改都需要写回磁盘，性能较差。 使用WAL，可以将多次磁盘写累积起来，将单次写入变成批量写入，将磁盘随机读写变更为顺序读写，从而提升性能。checkpoint： WAL一般和checkpoint（snapshot）一起使用。 WAL累积到一定长度后，批量将数据持久化写入磁盘，并清空WAL日志，这称为checkpoint。 WAL不可能无限累积下去，因为这会导致奔溃重启时，要很长时间来重放（redo）WAL日志。WAL的应用举例： Raft的日志和Snapshot机制就是典型的WAL。 Mysql中的 redo、undo 日志。 Zookeeper中的WAL日志。参考文章： 什么是 WAL" }, { "title": "GFS - Zookeeper-论文阅读", "url": "/posts/GFS-Zookeeper-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/", "categories": "分布式", "tags": "分布式", "date": "2022-09-12 00:00:00 +0800", "snippet": "GFSGFS是Google于2003年发表的论文，在真实系统上构建了大范围的分布式文件系统。GFS 的地位相当之高，与MapReduce 以及 BigTable 作为早年间Google的大数据三驾马车，虽然这些年随着时代都逐渐被取代了。下面是Google 文件系统发展的系统分支。GFS的研究动机：Google拥有远超过单个磁盘容量的数据，例如从整个互联网爬出来的网页，YouTube视频，用来构建搜索索引的中间文件，Web服务器中的大量日志文件等。基于此，Google的目标是构建一个大型的，快速的文件系统。并且这个文件系统是全局有效的，这样各种不同的应用程序都可以从中读取数据。背景与假设 背景 1：分布式组件经常发生错误，应当将此视为常态而不是意外； 组件（也就是分布式系统中的 commodity hardware，可以理解为配置不高的商用主机）发生错误的原因有很多，比如：应用或者操作系统 BUG，人为错误、硬盘、内存、网络甚至是电源故障； 背景 2：文件通常是大文件，而不是小文件； 这里的大文件是相对于传统标准而言的，比如 GB 在这属于大文件，KB 级别属于小文件；论文指出，应当操作大文件而不是小文件，即使系统本身支持 billion 级别（十亿个）的 kB 大小的文件的操作，但是这种操作也是不明智的，因为会明显巨大的额外开销。 这是一个非常重要的假设，因为后面会提到 GFS 中直接将 chuksize 设置为 64 MB，因为我们面对的是大文件，因此即使以 64MB 这么大的空间作为存储的管理单元，磁盘空间的浪费也不会很严重。 背景 3：大部分文件（主要是指字节数量占比高，而不是操作次数）通过 append（在已有的文件末尾追加）新数据的方式实现修改，而不是直接重写现有数据； 这是基于 Google 对 GFS 的应用场景的判断而做出的背景，因为大部分操作都是 append，因此 GFS 系统优化的中心也放在 record append 操作上。而对于随机写 GFS 仅仅给出一个存在不一致性问题的解决方案。 总之，这种假设下 I/O 操作都是顺序 I/O，效率比随机 I/O 高很多。 背景 4：协同设计应用以及文件系统可以提高系统整体灵活性，最终使整个系统收益 GFS的架构一个 GFS cluster（集群）分为两个组件： 单个 master 节点； 多个 chunkserver 节点；一个 GFS 集群同时可以被多个 client（客户）节点访问，架构图如下所示：可见 GFS 集群是一个典型 Master + Worker 结构。 Master + Worker 结构说的是存在一个 Master 来管理任务、分配任务，而 Worker 是真正干活的节点。在这里干的活自然是数据的存储和读取Chunk ServersGFS将一个大文件（比如10GB）分为多个Chunk，Chunk大小为64MB。Chunk分散存储在多台Chunk Server上。并且可以对文件做备份，一个Chunk可以复制多份分散存储。下图体现了 GFS 分布式系统架构的分布式结构以及文件的分块存储特性：MasterMaster 周期性通过 HeartBeat 机制和每一个 chunkserver 进行通信，进行指令的发送以及状态信息的接收。Master记录每个Chunk 的存储位置，相当于配置文件服务器（类似于中间件的名称服务器）。Master本身不做代理，由Client直接与chunkserver通讯。GFS 仅仅只有一个 Master 服务器，这极大地简化了设计难度。 当然，这样也会引入问题。比如 GFS 的 Master 节点在故障以后，并没有自我恢复功能，虽然需要额外的机制来保证Master节点的高容错。论文中Master 节点需要人工地进行故障恢复，这会导致小时级别的 GFS 系统不可写Master节点配置文件管理具体来说： 客户端向 Master 节点请求的 metadata 数据直接存储于 Master 的内存中，避免每次请求都需要进行磁盘 I/O； Master 节点使用日志（WAL Write-Ahead Log） + checkpoint 的方式来确保数据的持久化；（snapshot of memory） 一致性放弃了强一致性Zookeeper介绍 Zookeeper是一个构建在Zab协议上的分布式独立服务，用来帮助人们构建分布式系统，极大的减轻构建分布式应用的痛苦。 Zookeeper内部的Zab协议和Raft协议非常相似，功能都是提供分布式一致性，内部实现也分为选举和日志同步。相比Raft来说，Raft不是一个你可以直接交互的独立的服务，你必须要设计你自己的应用程序来与Raft库交互。 Zookeeper通过放弃线性一致性来解决性能和一致性的矛盾。提供更弱的一致性，带来更强的性能，更多的服务器可以提供更强读性能。一致性保证Zookeeper提供以下的一致性保证： 写请求是线性一致性的：写请求都经过leader节点，保证线性一致。读请求可以发送到任意副本节点，不保证线性一致。 对于单个客户端是线性一致的 任何一个客户端的请求，都会按照客户端指定的顺序来执行，论文里称之为FIFO（First In First Out）客户端序列。 对于多个客户端并访问，不能保证一致性。即A客户端写入数据后，B客户端立即来读该数据，可能读取不到A客户端写入的数据。也就是说从Zookeeper读到的数据不能保证是最新的。 从实现角度，Zookeeper给每条日志加上zxid的标签，响应请求时服务端返回最新zxid。客户端记录并在发送请求时带上该zxid，表明应该至少在zxid 的日志之后执行这个请求。 API设计Zookeeper的API设计使得它可以成为一个通用的服务，从而分担一个分布式系统所需要的大量工作。那么为什么Zookeeper的API是一个好的设计？具体来看，因为它实现了一个值得去了解的概念：mini-transaction。之所以称之为mini-transaction，是因为Zookeeper并不能提供一个完整的数据库事务（transaction）。一个真正的数据库可以使用完整的通用的事务，你可以指定事务的开始，然后执行任意的数据读写，之后结束事务。数据库可以聪明的将所有的操作作为一个原子事务提交。一个真实的事务可能会非常复杂，而Zookeeper支持非常简单的事务，使得我们可以对于一份数据实现原子操作。这对于计数器或者其他的一些简单功能足够了。所以，这里的事务并不通用，但是的确也提供了原子性，所以它被称为mini-transaction。Zookeeper可以用来做很多有用的事情 用来发布其他服务器使用的配置信息。例如，向某些Worker节点发布当前Master的IP地址。 用来选举Master。master节点挂掉后，需要让所有的节点都认可同一个新的Master节点。比如用到GFS中，用来实现Master切换。 MapReduce系统Worker的注册，任务的分配。复制像前面提到的GFS与Zookeeper系统，都致力于解决数据的同步问题，或者说是数据间复制的问题。复制（Replication）：为了提供高容错高可用性，最常见的解决方式就是复制。一个很有意思的问题是：复制能处理什么样的问题？复制并不是万能的。 解决fail-stop故障：如果某些东西出了故障，就会单纯停止运行。 不能处理软件中的bug和硬件设计中缺陷复制的方式有两种 状态转移（State Transfer）：主服务器将完整状态都传输给备份服务器。这样的方案会较为简单，但需传输的数据量较大。 举例：Ctrl+C， Ctrl+V Raft协议中拷贝快照，快照迁移 备份状态机（Replicated State Machine）：将需要备份的服务器视为一个确定性状态机，服务器执行相同的命令进行状态更新，只要执行的命令相同，最终状态一定相同。方案较为复杂，但网络传输量小很多。 举例：Raft协议 VMware 虚拟机主从备份 主从复制主从复制（Primary-Backup Replication）是一种常用的复制手段：主服务器负责写请求，从服务器负责读请求，从服务器时刻和主服务保持联系，与主服务器保持一致，这样当主服务器失效后备份服务器才能立刻接管。链式复制链式复制（Chain Replication），是使用非常广泛的复制模型，要将数据复制到全部节点之后，再向client应答成功。链式复制在发展过程中，从基本链式复制发展出了多种改进版本，来改进复制延迟。链式复制中，所有服务节点组成一条链。 Client写请求发送给Head节点，Head节点写入之后再转发给下一个节点，依次转发直到Tail节点。只有Tail节点更新成功后该写请求才commit。 Client写请求发送给Tail节点，Tail节点是最后一个更新，可以读取到所有已经commit的数据，保证强一致性。 参考链接 什么是数据库的一致性？一致性弱意味着什么？NoSQL 的弱一致性又为什么是可以被接受的？ - 华为云开发者联盟的回答 - 知乎 https://www.zhihu.com/question/20113030/answer/1834857103 etcd 和传统的数据库有什么区别，为什么分布式架构都要用这个玩意儿？ - 达的回答 - 知乎 https://www.zhihu.com/question/283164721/answer/2315076332 " }, { "title": "分布式MapReduce", "url": "/posts/%E5%88%86%E5%B8%83%E5%BC%8FMapReduce/", "categories": "分布式", "tags": "分布式", "date": "2022-08-01 00:00:00 +0800", "snippet": "最近暑假在家有空，跟着MIT分布式系统课程学一学分布式系统，顺便跟着课程实验学一学go，希望能在开学前学完吧。分布式系统介绍分布式系统：多台计算机合作提供服务。为什么要构建分布式系统？ 通过并行增加处理容量 通过复制来容忍故障 通过隔离实现安全 和现实世界物理设备（例如传感器）的分布相匹配分布式系统的困难？ 许多并发部分，复杂的交互过程 必须容忍部分机器的故障 性能受限本门课程的主要内容： 故障容忍（fault tolerance） 分布式一致性（consistency） 性能目标（performance） 权衡（tradeoffs） 代码实现（implementation）MapReduce介绍论文链接 https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdfMapReduce简单理解一个MapReduce例子（统计字母出现的次数）：Input1 -&amp;gt; Map -&amp;gt; a,1 b,1 Input2 -&amp;gt; Map -&amp;gt; b,1 Input3 -&amp;gt; Map -&amp;gt; a,1 c,1 | | | | | -&amp;gt; Reduce -&amp;gt; c,1 | -----&amp;gt; Reduce -&amp;gt; b,2 ---------&amp;gt; Reduce -&amp;gt; a,2使用者角度理解： 初始条件：用户需要提供两个函数：Map函数和Reduce函数，Map函数将大数据文件计算后分为多个桶，Reduce函数是将多个桶的结果聚合起来。 Map (k1,v1) → list(k2,v2) Reduce (k2,list(v2)) → list(v2) 输入事先被分为M份文件。MR框架将每一份文件作为Map函数的输入，输出&amp;lt;k2,v2&amp;gt;的中间数据。 MR框架聚集所有k2相同的的中间数据交由Reduce函数执行，得到&amp;lt;k2,v3&amp;gt;的输出。 MapReduce流程从MapReduce框架实现角度理解： 所有输入事先被分割成了M份文件。 对于每一份输入文件，启动一个worker遍历输入文件的key/value作为Map函数的输入，将输出的中间key/value分桶写入R份中间文件中。共有M份输入文件，所以总共需要执行M次map任务，得到M*R份输出中间文件。 执行R次reduce任务，每次任务读取相应编号的M份中间文件，执行Reduce函数，将结果写入output文件中。Lab1 实现MapReduce框架项目地址https://pdos.csail.mit.edu/6.824/labs/lab-mr.html主体流程： coordinator 启动 worker启动 worker发送RPC消息请求coordinator 分配map任务 coordinator 返回map任务的相关信息 worker开始执行map任务 重复3-6。 直到coordinator不再分配新的map任务，worker等待所有map任务执行结束 worker发送RPC消息请求coordinator 分配reduce任务 coordinator 返回reduce任务的相关信息 worker开始执行reduce任务 重复7-9。 直到coordinator不再分配reduce任务，worker等待coordinator结束消息，然后退出结束任务调度体系从主体流程可以看出来，map和reduce任务可以抽象为统一的概念任务，需要实现的是任务调度体系，整个MapReduce过程可以抽象为统一的任务调度过程，即：worker向coordinator 请求任务，coordinator分配任务，然后worker执行任务。通过任务调度体系来抽象MapReduce有几个好处： 抽象底层服务，减少代码重复：map和reduce过程大量任务管理的代码重复，这部分应该抽离出来形成更基层的服务，即任务调度体系。 方便做任务重试：在分布式系统需要考虑到容错，机器可能奔溃，离线等。worker接收任务后没法完成任务。这时需要coordinator重新分配任务。这部分逻辑可以在任务调度体系统一完成。 统一日志，方便调试：分布式+多线程比较复杂，在一个地方统一输出任务日志和调用情况，方便调试。任务调度体系的主要类图如下：服务端启动TaskCoordinator处理任务请求，Worker分为MapTaskWorker和ReduceTaskWorker，不断向服务端请求任务并执行，完成任务后通知TaskCoordinator任务完成。在TaskCoordinator中封装了任务管理的逻辑，包括如下数据结构： 所有需要执行的任务tasks 等待执行任务队列waitingTaskList 已经完成任务队列doneTaskList在TaskCoordinator中统一完成任务重试，基本逻辑是：一个任务开始后设定10s后的触发器，检查任务是否完成，如果任务没有完成则认为Worker已经奔溃，将此任务重新加入等待任务队列。此外需要确定任务是否由最新分配的Worker完成。考虑以下情况，Aworker因为网络波动没有在10s内发送任务完成消息，TaskCoordinator将此任务重新分配给了Bworker。之后Aworker网络恢复向TaskCoordinator发送任务完成消息，此时应该忽略这条消息，因为Aworker的任务已经被重新分配了。具体做法是：任务分配时以当前时间戳生成一个Token发给Worker，任务完成时Worker携带Token交由TaskCoordinator检验是否一致。实验效果 cd ./src/main/ # 编译 go build -race -buildmode=plugin ../mrapps/wc.go #跑测试用例 bash test-mr.sh" }, { "title": "k8s的第一次尝试", "url": "/posts/k8s%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/", "categories": "k8s", "tags": "k8s", "date": "2022-07-03 00:00:00 +0800", "snippet": "尝试k8s同济大学信息办向腾讯采购了tcs平台，由于我在帮信息办老师干活，所以有幸被邀请参加tcs平台的测试，去部署同济信息开放平台等项目。在这里把这一个多月的工作做一下记录。对于我而言，也是第一次接触k8s，之前从未真正地去操作过分布式集群部署。特别这次可以直接去操作学校的20多台机器，内心还是很兴奋的。腾讯TCS平台腾讯tcs平台是一个基于 Kubernetes 及成熟中间件构建的敏捷 PaaS 平台，用来管理的k8s和各类中间件的一个统一服务。之前虽然使用过docker，知道一些镜像容器部署的概念，但k8s相当于在docker基础上再提升一层，是用来管理多个docker容器，多个主机的平台服务。随着对tcs平台和k8s了解的深入，逐渐发现tcs平台其实就是对k8s做了一个前端页面服务，让用户可以可视化的形式去配置各类服务和应用。在使用过程中发现tcs平台前端存在许多小bug，有些还挺影响体验的。比如配置deployment时，有些很重要的配置没法在前端配置，只能通过手动改yaml文件来实现。还有时候修改yaml文件没法正确保存，前端配置的和yaml文件配置的会产生冲突，有时候挺烦人的。不过tcs平台能以前端形式展示出所有服务，应用，网络配置这些，就已经挺节省时间了。其次tcs平台还集成了一些常用的中间件，比如日志服务，镜像仓库服务，这些挺不错的。服务部署我从最基础的springboot java项目开始，逐渐在tcs平台上部署了 基于krakend的同济信息开放网关 基于ELK的统一日志中心 基于jenkins的自动化部署服务 基于springboot的后端项目部署过程中由于对这些中间件，对k8s不熟悉，几乎每一个服务都是新接触的，需要多次翻阅官方文档，查找大量资料。这中间也有遇到一些很蛋疼的问题，出现过一直配置不成功的情况。虽然说这些部署过程是我在帮学校打工，但其实换个角度说也是学校交学费让我免费学习。如果没有这次机会，我或许之后很难接触到k8s，云平台这些服务。回想起我之前在字节跳动实习时，都是直接使用公司提供的中间件和云平台。对我而言基本只有部署、回滚两个按钮，背后的原理一概不知。这次机会让我好好的学习一番，有机会去尝试分布式平台和云原生服务，了解背后的原理，着实发现了一片新天地。后续打算现在遇到的问题是有大量的yaml配置文件需要管理，这些文件有些是直接在tcs前端页面部署的，没有编写yaml文件。有些是通过yaml文件部署的。这些文件如何管理，如何维护和升级是我最近在思考的问题。后续打算学习下Helm。Helm相当于k8s的包管理软件，可以将现有服务配置成一个个应用，统一管理各个应用的配置文件。通过Helm可以大量减少配置文件管理的难度，也方便后续服务更新和维护。之后把项目交接给其他老师同学时也较少沟通和学习成本。" }, { "title": "设计模式-策略模式，封装复杂的领域知识", "url": "/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8FStrategy/", "categories": "领域驱动设计DDD", "tags": "设计模式", "date": "2022-06-20 00:00:00 +0800", "snippet": "参考文章 美团技术-设计模式二三事 美团技术-设计模式在外卖营销业务中的实践 设计模式之美-策略模式策略模式（Strategy或Policy）是一个很通用的设计模式，在DDD架构中会经常出现，其核心就是封装领域规则。策略模式主要由这三个角色组成，环境角色(Context)、抽象策略角色(Strategy)和具体策略角色(ConcreteStrategy)。 环境角色(Context)：持有一个策略类的引用，提供给客户端使用。 抽象策略角色(Strategy)：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。 具体策略角色(ConcreteStrategy)：包装了相关的算法或行为。一个Strategy是一个无状态的单例对象，通常需要至少2个方法： canApply 业务执行方法其中，canApply方法用来判断一个Strategy是否适用于当前的上下文，如果适用则调用方会去触发业务方法。通常，为了降低一个Strategy的可测试性和复杂度，Strategy不应该直接操作对象，而是通过返回计算后的值，在Domain Service里对对象进行操作。策略模式的应用情形 最基础的策略模式情形 策略处理过程 输入参数是一样的，输出结果是一样的，也就是说多种策略可以完美的继承基类策略。 只选择单一策略进行处理 策略的选择过程不算复杂 解决方式： 定义一个策略基类，所有策略都继承基类，各自实现。（作用：将策略定义代码分离出业务逻辑） 处理过程优化为使用策略基类统一处理 可选：选择过程可以优化为查表法 可选：策略可以提取创建好，存到表中（配合查表法或者工厂方法） 复杂策略模式 策略处理过程 输入参数是不一样的，但输出结果是一样的，不好直接继承基类策略。 只选择单一策略进行处理 策略的选择过程不算复杂 解决方式： 定义一个策略基类，所有策略都继承基类，各自实现。（作用：将策略定义代码分离出业务逻辑） 若输入参数不复杂，可以采用Map的形式进行传递 若输入参数复杂，则定义一个策略上下文基类，然后为每个策略定义策略上下文子类。 从业务逻辑生成对应策略上下文的胶水代码可以汇总到一个类中，作为DP。 处理过程优化为使用策略基类+策略上下文基类（或者Map）统一处理 可选：选择过程可以优化为查表法 可选：策略可以提取创建好，存到表中（配合查表法或者工厂方法） 更复杂的策略模式 策略处理过程 输入参数是不一样的，但输出结果也不一定一样，不好直接继承基类策略。 可以选择多种策略进行处理 策略的选择过程复杂 解决方式：此时应该采用职责链模式去解决。 canApply 一个业务方法 这个涵盖了上述情况，都可以解决。 " }, { "title": "常用软件安装配置", "url": "/posts/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/", "categories": "小技巧", "tags": "go, docker, pip", "date": "2022-06-02 00:00:00 +0800", "snippet": "常用软件安装配置go安装需要定制版本上https://go.dev/dl/查看，这里给出go1.14版本# 下载go1.14，不行就手动下载再上传wget https://golang.org/doc/install?download=go1.14.7.linux-amd64.tar.gz# 解压到/usr/local目录下tar -C /usr/local -zxvf go1.14.7.linux-amd64.tar.gz#配置Go环境变量sudo vim /etc/profile# 写入/etc/profileexport GOROOT=/usr/local/goexport GOPATH=$GOROOT/gopathexport PATH=$GOPATH/bin:$GOROOT/bin:$PATH# 退出保存，使得环境变量生效source /etc/profile#查看go版本go version换国内源go env -w GOPROXY=https://goproxy.cn,directdockerCentos/Ubuntu 安装dockercurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyunsudo service docker start查看所有容器docker ps -a查看容器日志docker logs -f da6743d61e1a进入容器docker exec -i 69d1 bash更换阿里源提升下载速度首先登陆 https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors查看自己的专属加速器地址，根据地址执行下面命令sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&#39;EOF&#39;{ &quot;registry-mirrors&quot;: [&quot;https://hv2pm8pp.mirror.aliyuncs.com&quot;]}EOF# 重启dockersudo systemctl daemon-reloadsudo systemctl restart dockerpip指定源安装命令后面加-i即可pip install tensorflow==2.6.0 -i https://mirrors.aliyun.com/pypi/simple/默认换成国内源# 阿里源pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/# 或：# 清华源pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple# 腾讯源pip config set global.index-url http://mirrors.cloud.tencent.com/pypi/simple# 豆瓣源pip config set global.index-url http://pypi.douban.com/simple/pip安装制定版本pip install tensorflow==1.2.1 (==后面为所要安装的版本号)" }, { "title": "解决 Git 在 windows 下中文乱码的问题", "url": "/posts/git%E8%AE%BE%E7%BD%AE%E8%A7%A3%E5%86%B3windows%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/", "categories": "小技巧", "tags": "github, git", "date": "2022-05-31 00:00:00 +0800", "snippet": "解决 Git 在 windows 下中文乱码的问题原因中文乱码的根源在于 windows 基于一些历史原因无法全面支持 utf-8 编码格式，并且也无法通过有效手段令其全面支持。解决方案在命令行下输入以下命令：git config --global core.quotepath false # 显示 status 编码git config --global gui.encoding utf-8 # 图形界面编码git config --global i18n.commit.encoding utf-8 # 提交信息编码git config --global i18n.logoutputencoding utf-8 # 输出 log 编码配置windows环境变量因为 git log 默认使用 less 分页，所以需要 bash 对 less 命令进行 utf-8 编码新增加环境变量 LESSCHARSET = UTF-8查看效果关掉所有终端，重新打开查看效果解决warning: LF will be replaced by CRLF”参考链接https://stackoverflow.com/questions/17628305/windows-git-warning-lf-will-be-replaced-by-crlf-is-that-warning-tail-backwarwindows问题git config --global core.autocrlf false" }, { "title": "字节跳动实习报告", "url": "/posts/%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%AE%9E%E4%B9%A0%E6%8A%A5%E5%91%8A/", "categories": "课程", "tags": "字节跳动", "date": "2022-05-28 00:00:00 +0800", "snippet": "时隔两年，在翻阅历史文档时不经意发现两年前写的实习报告，颇觉诙谐有趣，于是发出来邀诸君共赏。我的实习单位是字节跳动上海商业变现部门，在为期六个月的实习中，我主要从事后端开发工作，时间为2020年8月至2021年1月。这个部门的主要职能是为字节跳动各项业务提供广告变现结算服务，包括对账单，发票，税率，催款，坏账等业务场景。我的实习岗位是后端开发实习生，主要负责维护税率，返点模块，迭代开发回款，催款模块。工作内容包括文档编写，跟进产品需求，技术设计和迭代开发。八月中旬我的字节跳动实习生活开始了，我是我们部门第20号人物（现在超80人），我们组第3人（现在已15人），也是年龄最小的那位。第一次走进字节跳动的大门，我还懵懵懂懂，对公司种种新鲜事物既感到好奇，又因无知而害怕。整个八月，我都在浑浑噩噩中度过。我没有被分配到开发任务，初来乍到，我的leader让我先熟悉熟悉环境，了解了解业务，实则是实在没时间管我。本来公司规定每个实习生都有一名导师负责带着入门，但我们部门实在是太忙了。那段时间正值美国封禁Tiktok事件中，整个部门为此事成天开会，而我的导师恰好是部门leader，更是整天见不到一面。部门本来人就少，加上开会多，工位上竟时常只有我一人。每天晚上，我端坐于工位之上，环顾四周无一人，茫茫然不知所从，或对着业务文档发呆，或看着项目代码挠头，想问人却没人，想睡觉又不敢，现在想来，颇觉可怜可悲。整个八月，入职两周，只是觉得公司食堂饭菜不错，不枉费我每日出勤罢了。九月份情况有了一些好转，一是我们新进了几位资历较深的同事，组员达到了5人，大家没那么忙了，我便有时间去请教了。二是我终于在北京的同事的悉心指导下完成了几个开发需求，颇觉事有小成，一扫颓废之气。刚开始我即写Java又写Python，两个语言在学校里都写过，感觉还算熟悉。不过在看到大佬同事们的代码后，颇觉吃惊，一是代码量惊人，二是模块数量惊人，三是代码手法惊人，最重要的是我全然看不懂。我们部门业务属性非常重，对业务理解不够，代码是完全看不明白的。什么税率，对账单，客户，广告主，返点，上游下游各种新名词层出不穷，加上部门新成立文档少，但项目又是老项目，历史包袱重，一时半会当真搞不明白。当时写需求只感觉是盲人摸象，这个需求要实现啥，我就写啥，会不会对其他模块造成影响，会不会上下游造成问题只能询问组内同事。按组内同事的话来说就是：能跑就行。十月份情况好起来了。在北京的PM团队来了一趟上海，一连5天详细地为我们开发介绍了整个部门的业务背景，业务流程，业务知识。在这之中我才对部门整体业务有了一个全面的认识，顿时感觉之前都是管中窥豹，坐井观天。同时，虽然部门还在大力招人，但小组面对诸多业务需求人手还是不够，因此开始让我独立负责回款模块的后端开发。回款模块之前需求很少，但随着国外业务的增长，业务对回款效率有了更高的要求，需要有更多催款工具进行催款，更多回款分析工具来帮助业务人员管理回款账目。记得国庆假期过后，我开始着手自己的第一个大需求，体验完整的需求提出和评审，技术设计和评审，代码实现，前后端联调，自测和提测，代码预发布和正式发布的开发流程。整个过程代码开发时间可能30%不到，更多的时间是用来和组内，组外，部门间同事的沟通协作，和PM探讨需求。最后需求按时顺利上线，虽然现在来看当时需求并不难，但当时却颇觉任务艰巨，需求上线还捏着一把汗，怕出线上事故呢。十一月份至一月份渐入佳境。我们小组当时扩充至12人，在这期间虽然需求开发上还会遇到一些问题，但都可以向组员们沟通解决。我更多的时间是在学习整个部门的业务流程，学习公司的组织方法，探索部门用到的技术框架和中间件，以及向同事们学习探讨各种生活、学习上的问题。小组内氛围非常好，我是当中最小的，其他同事大多25至35之间，阅历比较丰富。每日中饭晚饭散步之际，我或者向他们请教生活学习工作上的问题，或者悉心听他们谈论工作经验、分析时事等，从中对现代社会，工作生活学到了许多。一月回校以后，我便来到史扬老师实验室中参加课题组，一边准备毕业设计相关事宜，一边学习密码学相关知识。学校内课题组和字节跳动实习感觉不太一样，在字节跳动时，虽说工作氛围好，但需求必须按时完成，还是很有压力的。在实验室实习时，压力就小些了，没有必须完成的压迫感。另外字节实习时早10晚10，疏于锻炼，对身体健康颇有影响。回到学校后，才能逐渐做到工作学习和生活的平衡，对身心健康对大有裨益。在实验室里，我参与项目组基础设施的搭建，包括Gitlab，Maven，SonarQube的配置，服务器的管理，项目的开发等等。整个过程循序渐进，涉及方面十分广泛，我从中学到许多，也感谢老师和学长们这几个月的指导和帮助。" }, { "title": "CQRS与EventSourcing", "url": "/posts/CQRS%E4%B8%8EEventSourcing/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-04-25 00:00:00 +0800", "snippet": "随着业务不断发展，软件系统的架构也越来越复杂，但无论多复杂的业务最终在系统中实现的时候，无非是读写操作。用户根据业务规则写入商业数据，再根据查询规则获取想要的结果。通常而言我们会讲这些读写的数据放到一个数据库中保存，通过一套模型对其进行读写操作。而在大型系统中往往查询操作远远多于写入操作，于是就有了读写分离的思想，将读操作和写操作的模型分开定义并且提供不同的通道供用户使用。CQRS(Command-Query Responsibility Segregation) 就是基于这一思想提供的一种模式读写分离的模式。传统模式传统的系统请求从最左边的Client开始，沿着红线往右通过Application Service对系统进行请求。这里Application Service 可以理解为系统的门面，或者是Controller层负责接收客户端的请求，此时请求的内容比较简单基本和数据库中的信息一致，因此这里使用DTO(Data Transfer Object)直接请求。DTO经过Domain Model 以后直接到达Database，从而沿着蓝色的线条返回给Client端。传统的请求方式部分读操作和写操作，都使用同样的数据模型和一套Domain Model以及相同的数据库。传统模式在系统庞大后存在几点问题 DTO会逐渐变得臃肿，面面俱到 Domain Model 读写的数据模型不一样，导致读写逻辑混杂在一起CQRS模式将读写分离，客户端的请求分为两类，一类为Command，为写入操作。一类为Query，为只读操作。这样的好处是，分离读写职责，模型界限明确。随着系统规模扩大，将读库和写库分离是必然的选择，例如现在的分布式系统，多库读写。但这会带来数据一致性的问题，这个问题的解决就可以靠**Event Sourcing **模式。Event Sourcing 模式Event Sourcing的思想是记录变更过程而非最终结果。把所有写入操作（也叫事件）记录下来，通过事件形成的序列可以计算出数据的最终状态。任何实体的最终状态都是通过事件的叠加和还原确认的Event Sourcing 的优点： 溯源事件与重现操作：天然支持审计，事件序列就是一条条的审计日志。 追踪和修复Bug：有着所有历史数据，修复Bug造成的影响小。 提高性能：提高写入性能，只有新增，没有删除更新操作，适合日志型数据库。Event Sourcing 的缺点： 转变思路：首先需要按照领域建模思想，将系统的操作都建模为一个个事件。这有一定技术难度。 兼容性问题：当系统发展，事件的数据结构会发生变化，如何去兼容历史事件。 处理幂等事件：事件需要保持幂等性，即执行多次的结果仍然一样。 数据库存储：将所有事件存储下来，数据库的容量膨胀的很快。CQRS与Event Sourcing的结合将读写分离的CQRS和Event Sourcing结合起来。Command作为写入操作，形成一个个事件，Event Store保存事件，并将事件发送给读库。读库根据事件更新数据状态。Query端查询读库获取最新的数据状态。什么时候使用Event Sourcing使用Event Sourcing有它的优点也有缺点，那么什么时候该使用Event Sourcing模式呢？ 首先是系统类型，如果你的系统有大量的CRUD，也就是增删改查类型的业务，那么就不适合使用Event Sourcing模式。Event Sourcing模式比较适用于有复杂业务的应用系统。 如果对你的系统来说，业务数据产生的过程比结果更重要，或者说更有意义，那就应该使用Event Sourcing。你可以使用Event Sourcing的事件数据来分析数据产生的过程，解决bug，也可以用来分析用户的行为。 如果你需要系统提供业务状态的历史版本，例如一个内容管理系统，如果我想针对内容实现版本管理，版本回退等操作，那就应该使用Event Sourcing。参考文章https://www.51cto.com/article/644144.html" }, { "title": "设计模式-何时使用继承？详细了解装饰器模式", "url": "/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/", "categories": "领域驱动设计DDD", "tags": "设计模式", "date": "2022-04-20 00:00:00 +0800", "snippet": "在面向对象编程时，继承是一种经常用于扩展对象功能的方法。然而时至今日，随着设计模式的发展，继承开始被认为是一种设计异味。事实上，已经证明使用继承来扩展对象通常会导致类层次结构爆炸。此外，Java 和 C# 等几种流行的编程语言不支持多重继承，这限制了继承的拓展。装饰器模式为扩展对象功能提供了一种灵活的继承替代方案。这种设计模式的方法是多个装饰器可以堆叠在一起，每个装饰器都添加新的功能。与继承相比，装饰器可以对某个单独接口进行拓展，从而避免了对整个类层次结构进行子类化。此外，使用装饰器模式可以生成干净且可测试的代码（请参阅 可测试性和其他优点章节）。遗憾的是，今天的大部分软件开发人员对装饰器模式的理解有限。这一部分是由于缺乏相关的培训，另一部分也是因为编程语言没有跟上面向对象设计原则的发展，以鼓励开发人员学习和使用装饰器模式。本文将讨论使用装饰器模式相对于继承的优点，并建议在面向对象的编程语言中，装饰器模式应该具有原生支持。事实上，相比于继承，我相信装饰器模式可以编写更干净和可测试的代码。继承带来的类层次结构爆炸当向给定的类层次结构添加新功能所需的类数量呈指数增长时，就会出现类层次结构的爆炸式增长。让我们考虑以下接口：public interface IEmailService{ void send(Email email); Collection&amp;lt;EmailInfo&amp;gt; listEmails(int indexBegin, int indexEnd); Email downloadEmail(EmailInfo emailInfo);}如果对电子邮件服务器的请求失败， EmailService的默认实现会引发异常。我们想扩展EmailService实现，以便在放弃之前重试失败的请求几次。我们还希望能够选择实现是否是线程安全的。我们可以通过向EmailService类本身添加可选的重试和线程安全特性来实现这一点。该类将接受构造函数中启用/禁用每个功能的参数。然而，这种解决方案违反了单一职责原则（因为EmailService将有额外的责任）和开闭原则（因为必须修改类本身以进行扩展）。此外，EmailService类可能是我们无法修改的第三方库的一部分。在不修改类的情况下扩展类的常用方法是使用继承。通过继承，派生类继承其父类的属性和行为，并且可以选择扩展或覆盖其某些功能。在EmailService示例中，我们可以创建三个子类，一个添加重试，一个添加线程安全，一个添加两个功能。类层次结构如下所示：请注意，ThreadSafeEmailServiceWithRetries也可以从EmailServiceWithRetries或ThreadSafeEmailService继承（如果支持多重继承，则两者都继承）。但是，类的数量和产生的功能将是相似的。除了重试和线程安全之外，我们还想扩展我们的电子邮件服务 API，以便可以选择启用日志记录。再次，我们使用继承来扩展类层次结构，如下所示：请注意，添加对日志记录的支持所需的附加类的数量等于现有层次结构中的类总数（在本例中为四个）。为了确认这种行为，让我们将缓存添加到层次结构并检查结果。新的层次结构如下所示：如您所见，类层次结构呈指数级增长，并且很快变得难以管理。这个问题被称为类层次结构爆炸。装饰者模式的解决方案装饰器模式使用组合而不是继承来扩展对象功能。它消除了类层次结构爆炸的问题，因为每个新特性只需要一个装饰器。为了说明，让我们为重试功能创建一个装饰器。为简单起见，使用了一个简单的 for 循环重试3 次。EmailServiceRetryDecorator如下：public class EmailServiceRetryDecorator implements IEmailService{ private final IEmailService emailService; public EmailServiceRetryDecorator(IEmailService emailService) { this.emailService = emailService; } @Override public void send(Email email) { executeWithRetries(() -&amp;gt; emailService.send(email)); } @Override public Collection&amp;lt;EmailInfo&amp;gt; listEmails(int indexBegin, int indexEnd) { final List&amp;lt;EmailInfo&amp;gt; emailInfos = new ArrayList&amp;lt;&amp;gt;(); executeWithRetries(() -&amp;gt; emailInfos.addAll(emailService.listEmails(indexBegin, indexEnd))); return emailInfos; } @Override public Email downloadEmail(EmailInfo emailInfo) { final Email[] email = new Email[1]; executeWithRetries(() -&amp;gt; email[0] = emailService.downloadEmail(emailInfo)); return email[0]; } private void executeWithRetries(Runnable runnable) { for(int i=0; i&amp;lt;3; ++i) { try { runnable.run(); } catch (EmailServiceTransientError e) { continue; } break; } }}请注意，EmailServiceRetryDecorator 的构造函数引用了IEmailService，它可以是IEmailService的任何实现（包括装饰器本身）。这将装饰器与IEmailService的特定实现完全分离，并增加了它的可重用性和可测试性。同样，我们可以为线程安全、日志记录和缓存创建装饰器。生成的类层次结构如下：如上面的类图所示，每个特性只需要一个类，并且生成的类层次结构简单且可扩展（线性增长）。装饰器队列乍一看，使用装饰器模式似乎只能将一个功能添加到给定的实现中。然而，因为装饰器可以堆叠在一起，所以可能性是无穷无尽的。例如，我们可以动态地创建一个等价于我们使用继承创建的EmailServiceWithRetriesAndCaching，如下所示：IEmailService emailServiceWithRetriesAndCaching = new EmailServiceCacheDecorator( new EmailServiceRetryDecorator(new EmailService()));此外，通过更改装饰器的顺序或在多个级别使用相同的装饰器，我们可以动态地创建新的实现，而这些实现是通过继承难以创建的。例如，我们可以在重试之前和之后添加日志记录，如下所示：IEmailService emailService = new EmailServiceLoggingDecorator(new EmailServiceRetryDecorator( new EmailServiceLoggingDecorator(new EmailService())));通过这种组合，将记录重试前后的请求状态。这提供了详细的日志记录，可用于调试目的或创建丰富的仪表板。另请参阅 -decorator -builder 模式，它使组合装饰器更简单、更易读。可测试性装饰器相对于继承的另一个主要好处是可测试性。为了说明，让我们考虑为重试功能编写一个单元测试。我们通过继承创建的EmailServiceWithRetries无法独立于其父类（EmailService）进行测试，因为没有机制可以用存根（也称为模拟）替换父类。此外，因为 EmailService 对后端服务器执行网络调用，它的所有子类都变得难以进行单元测试（因为网络调用通常很慢且不可靠）。在这种情况下，通常使用集成测试而不是单元测试。另一方面，因为EmailServiceRetryDecorator在其构造函数中引用了IEmailService，所以装饰对象可以很容易地替换为存根实现（即模拟）。这使得可以单独测试重试功能，而继承是不可能的。为了说明这一点，让我们编写一个单元测试来验证是否执行了至少一次重试（在下面的示例中，使用了 Mockito 框架）。IEmailService mock = mock(IEmailService.class);when(mock.downloadEmail(emailInfo)) .thenThrow(new EmailServiceTransientError()) .thenReturn(email);EmailServiceRetryDecorator decorator = new EmailServiceRetryDecorator(mock);Assert.assertEquals(email, decorator.downloadEmail(emailInfo));与依赖于EmailService实现和远程服务调用的集成测试相比，该测试更简单、快速且可靠。其他优点除了简化类层次结构和提高可测试性之外，装饰器模式还鼓励开发人员编写符合SOLID 设计原则的代码。事实上，使用装饰器模式，新特性被添加到新的焦点对象（单一责任原则）而不修改现有类（开闭原则）。此外，装饰器模式鼓励使用依赖倒置（这有很多好处，例如松耦合和可测试性），因为装饰器依赖于抽象而不是具体化。缺点装饰器模式与替代方案（继承或修改现有类）相比具有许多优点，但同样也有一定的缺点。一个缺点是装饰接口中的所有方法都必须在装饰器类中实现。事实上，不添加任何额外行为的方法必须实现为转发方法以保持现有行为。相反，继承只需要子类来实现改变或扩展基类行为的方法。为了说明转发方法的问题，让我们考虑下面的IProcess接口并为其创建一个装饰器。public interface IProcess { void start(String args); void kill(); ProcessInfo getInfo(); ProcessStatus getStatus(); ProcessStatistics getStatistics();}如果进程启动失败，start方法的默认实现会抛出FailedToStartProcessException 。我们想扩展默认实现，以便在放弃之前重试启动进程三遍。使用装饰器模式，实现将如下所示：public class RetryStartProcess implements IProcess{ private IProcess process; public RetryStartProcess(IProcess process) { this.process = process; } @Override public void start(String args) { for(int i=0; i&amp;lt;3; ++i) { try { process.start(args); } catch (FailedToStartProcessException e) { continue; } break; } } @Override public void kill() { process.kill(); } @Override public ProcessInfo getInfo() { return process.getInfo(); } @Override public ProcessStatus getStatus() { return process.getStatus(); } @Override public ProcessStatistics getStatistics() { return process.getStatistics(); }}请注意，此实现包含大量样板代码。事实上，唯一相关的实现部分是start方法的实现。对于具有许多方法的接口，这种样板可以被视为生产力和维护开销。装饰器模式的另一个缺点是它不受欢迎，尤其是在初级开发人员中。事实上，不太受欢迎通常意味着更难理解哪些会导致开发时间变慢。对装饰器模式的原生支持如果装饰器模式受益于面向对象编程语言的原生支持（类似于今天为继承提供的支持），则可以克服上一节中讨论的两个缺点。事实上，有了这样的原生支持，就不需要转发方法，装饰器模式也更容易使用。此外，对装饰器模式的原生支持肯定会增加其流行度和使用率。编程语言如何影响设计模式的采用的一个很好的例子是在 C# 中引入了对观察者模式（也称为事件）的本机支持。今天的 C# 开发人员（包括初级开发人员）自然而然地使用观察者模式在松散耦合的类之间进行事件通信。如果 C# 中不存在事件，许多开发人员会在类之间引入直接依赖关系来传递事件，这将导致代码的可重用性降低并且更难测试。类似地，对装饰器模式的原生支持将鼓励开发人员创建装饰器，而不是修改现有类或不恰当地使用继承，这将导致更好的代码质量。以下实现说明了 Java 中对装饰器模式的原生支持：public class RetryStartProcess decorates IProcess{ @Override public void start(String args) { for(int i=0; i&amp;lt;3; ++i) { try { decorated.start(args); } catch (FailedToStartProcessException e) { continue; } break; } }}请注意用于代替implements的**decorates关键字，以及使用修饰字段来访问修饰对象。为此，装饰器的默认构造函数需要一个IProcess参数（将在语言级别处理，就像今天处理无参数默认构造函数一样）。如您所见，这种原生支持将使装饰模式样板免费，并且与继承一样容易实现（如果不是更容易的话）。抽象装饰器如果像我一样，你经常使用装饰器模式并且经常为每个接口使用许多装饰器，那么你可以使用一种解决方法来减少转发方法的样板文件（与此同时，直到对装饰器的原生支持模式变为可用）。解决方法包括创建一个抽象装饰器，该装饰器将所有方法实现为转发方法，并从中派生（继承）所有装饰器。因为转发方法继承自抽象装饰器，所以只有装饰方法需要重新实现。此解决方法利用了对继承的本机支持并使用它来实现装饰器模式。以下代码说明了这种方法。public abstract class AbstractProcessDecorator implements IProcess{ protected final IProcess process; protected AbstractProcessDecorator(IProcess process) { this.process = process; } public void start(String args) { process.start(args); } public void kill() { process.kill(); } public ProcessInfo getInfo() { return process.getInfo(); } public ProcessStatus getStatus() { return process.getStatus(); } public ProcessStatistics getStatistics() { return process.getStatistics(); }}public class RetryStartProcess extends AbstractProcessDecorator{ public RetryStartProcess(IProcess process) { super(process); } @Override public void start(String args) { for(int i=0; i&amp;lt;3; ++i) { try { process.start(args); } catch (FailedToStartProcessException e) { continue; } break; } }}这种方法的一个缺点是装饰器将无法从其他类继承（对于不支持多重继承的语言，如Java）。何时使用继承尽管我认为应该尽可能选择装饰器模式而不是继承，但在某些情况下继承更合适。装饰器不适用的常见情况是：派生类需要访问父类中的非公共字段或方法。因为装饰器只知道公共接口，所以它们无法访问特定于一个实现或另一个实现的字段或方法。根据经验，如果您的子类仅仅依赖于其父类的公共接口，则暗示着可以改用装饰器模式。事实上，如果静态分析工具建议在这种情况下用装饰器替换继承，那就太好了。总结 在可能的情况下，装饰器模式应该优先于继承。 装饰器模式消除了继承遇到的类层次结构爆炸的问题。事实上，使用装饰器模式，生成的类层次结构很简单并且可以线性扩展。 装饰器可以独立于装饰对象进行测试，但子类不能独立于其父类进行测试。通过继承，如果父类难以进行单元测试（例如执行远程调用），则其派生类会继承此问题。但是，由于装饰器只依赖于装饰对象的接口（通过装饰器类的构造函数注入），所以装饰器可以独立进行单元测试。 装饰器模式的使用鼓励开发人员编写符合 SOLID 设计原则的代码。 面向对象编程语言中对装饰器模式的原生支持将使该模式更易于使用并增加其采用率。" }, { "title": "万字长文，结合电商支付业务一文搞懂DDD", "url": "/posts/%E4%B8%87%E5%AD%97%E9%95%BF%E6%96%87-%E7%BB%93%E5%90%88%E7%94%B5%E5%95%86%E6%94%AF%E4%BB%98%E4%B8%9A%E5%8A%A1%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82DDD/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-04-16 00:00:00 +0800", "snippet": "万字长文，结合电商支付业务一文搞懂DDD2004 年，软件大师 Eric Evans 的不朽著作《领域驱动设计：软件核心复杂性应对之道》面世，从书名可以看出，这是一本应对软件系统越来越复杂的方法论的图书。然而，在当时，中国的软件业才刚刚起步，软件系统还没有那么复杂，即使维护了几年，软件退化了，不好维护了，推倒重新开发就好了。因此，在过去的那么多年里，真正运用领域驱动设计开发（DDD）的团队并不多。一套优秀的方法论，因为现实阶段的原因而一直不温不火。不过，这些年随着中国软件业的快速发展，软件规模越来越大，生命周期也越来越长，推倒重新开发的成本和风险越来越大。这时，软件团队急切需要在较低成本的状态下持续维护一个系统很多年。然而，事与愿违。随着时间的推移，程序越来越乱，维护成本越来越高，软件退化成了无数软件团队的噩梦。这时，微服务架构成了规模化软件的解决之道。不过，微服务对设计提出了很高的要求，强调“小而专、高内聚”，否则就不能发挥出微服务的优势，甚至可能令问题更糟糕。因此，微服务的设计，微服务的拆分都需要领域驱动设计的指导。那么，领域驱动为什么能解决软件规模化的问题呢？我们先从问题的根源谈起，即软件退化。软件退化的根源最近 10 年的互联网发展，从电子商务到移动互联，再到“互联网+”与传统行业的互联网转型，是一个非常痛苦的转型过程。而近几年的人工智能与 5G 技术的发展，又会带动整个产业向着大数据与物联网发展，另一轮的技术转型已经拉开帷幕。那么，在这个过程中，一方面会给我们带来诸多的挑战，另一方面又会给我们带来无尽的机会，它会带来更多的新兴市场、新兴产业与全新业务，给我们带来全新的发展机遇。然而，在面对全新业务、全新增长点的时候，我们能不能把握住这样的机遇呢？我们期望能把握住，但每次回到现实，回到正在维护的系统时，却令人沮丧。我们的软件总是经历着这样的轮回，软件设计质量最高的时候是第一次设计的那个版本，当第一个版本设计上线以后就开始各种需求变更，这常常又会打乱原有的设计。因此，需求变更一次，版本迭代一次，软件就修改一次，软件修改一次，质量就下降一次。不论第一次的设计质量有多高，软件经历不了几次变更，就进入一种低质量、难以维护的状态。进而，团队就不得不在这样的状态下，以高成本的方式不断地维护下去，维护很多年。这时候，维护好原有的业务都非常不易，又如何再去期望未来更多的全新业务呢？比如，这是一段电商网站支付功能的设计，最初的版本设计质量还是不错的：当第一个版本上线以后，很快就迎来了第一次变更，变更的需求是增加商品折扣功能，并且这个折扣功能还要分为限时折扣、限量折扣、某类商品的折扣、某个商品的折扣。当我们拿到这个需求时怎么做呢？很简单，增加一个 if 语句，if 限时折扣就怎么怎么样，if 限量折扣就怎么怎么样……代码开始膨胀了。接着，第二次变更需要增加 VIP 会员，除了增加各种金卡、银卡的折扣，还要为会员发放各种福利，让会员享受各种特权。为了实现这些需求，我们又要在 payoff() 方法中加入更多的代码。第三次变更增加的是支付方式，除了支付宝支付，还要增加微信支付、各种银行卡支付、各种支付平台支付，此时又要塞入一大堆代码。经过这三次变更，你可以想象现在的 payoff() 方法是什么样子了吧，变更是不是就可以结束了呢？其实不能，接着还要增加更多的秒杀、预订、闪购、众筹，以及各种返券。程序变得越来越乱而难以阅读和维护，每次变更也变得越来越困难。问题来了：为什么软件会退化，会随着变更而设计质量下降呢？在这个问题上，我们必须寻找到问题的根源，才能对症下药、解决问题。要探寻软件退化的根源，先要从探寻软件的本质及其规律开始，软件的本质就是对真实世界的模拟，每个软件都能在真实世界中找到它的影子。因此，软件中业务逻辑正确与否的唯一标准就是是否与真实世界一致。如果一致，则软件是 OK 的；不一致，则用户会提 Bug、提新需求。在这里发现了一个非常重要的线索，那就是，软件要做成什么样，既不由我们来决定，也不由用户来决定，而是由客观世界决定。用户为什么总在改需求，是因为他们也不确定客观世界的规则，只有遇到问题了他们才能想得起来。因此，对于我们来说，与其唯唯诺诺地按照用户的要求去做软件，不如在充分理解业务的基础上去分析软件，这样会更有利于我们减少软件维护的成本。那么，真实世界是怎样的，我们就怎样开发软件，不就简单了吗？其实并非如此，因为真实世界是非常复杂的，要深刻理解真实世界中的这些业务逻辑是需要一个过程的。因此，我们最初只能认识真实世界中那些简单、清晰、易于理解的业务逻辑，把它们做到我们的软件里，即每个软件的第一个版本的需求总是那么清晰明了、易于设计。然而，当我们把第一个版本的软件交付用户使用的时候，用户却会发现，还有很多不简单、不明了、不易于理解的业务逻辑没做到软件里。这在使用软件的过程中很不方便，和真实业务不一致，因此用户就会提 Bug、提新需求。在我们不断地修复 Bug，实现新需求的过程中，软件的业务逻辑也会越来越接近真实世界，使得我们的软件越来越专业，让用户感觉越来越好用。但是，在软件越来越接近真实世界的过程中，业务逻辑就会变得越来越复杂，软件规模也越来越庞大。你一定有这样一个认识：简单软件有简单软件的设计，复杂软件有复杂软件的设计。比如，现在的需求就是将用户订单按照“单价 × 数量”公式来计算应付金额，那么在一个 PaymentBus 类中增加一个 payoff() 方法即可，这样的设计没有问题。不过，如果现在的需要在付款的过程中计算各种折扣、各种优惠、各种返券，那么我们必然会做成一个复杂的程序结构。但是，真实情况却不是这样的。真实情况是，起初我们拿到的需求是那个简单需求，然后在简单需求的基础上进行了设计开发。但随着软件的不断变更，软件业务逻辑变得越来越复杂，软件规模不断扩大，逐渐由一个简单软件转变成一个复杂软件。这时，如果要保持软件设计质量不退化，就应当逐步调整软件的程序结构，逐渐由简单的程序结构转变为复杂的程序结构。如果我们总是这样做，就能始终保持软件的设计质量，不过非常遗憾的是，我们以往在维护软件的过程中却不是这样做的，而是不断地在原有简单软件的程序结构下，往 payoff() 方法中塞代码，这样做必然会造成软件的退化。也就是说，软件退化的根源不是版本迭代和需求变更，版本迭代和需求变更只是一个诱因。如果每次软件变更时，适时地进行解耦，进行功能扩展，再实现新的功能，就能保持高质量的软件设计。但如果在每次软件变更时没有调整程序结构，而是在原有的程序结构上不断地塞代码，软件就会退化。这就是软件发展的规律，软件退化的根源。杜绝软件退化：两顶帽子前面谈到，要保持软件设计质量不退化，必须在每次需求变更的时候，对原有的程序结构适当地进行调整。那么应当怎样进行调整呢？还是回到前面电商网站付款功能的那个案例，看看每次需求变更应当怎样设计。在交付第一个版本的基础上，很快第一次需求变更就到来了。第一次需求变更的内容如下。增加商品折扣功能，该功能分为以下几种类型： 限时折扣 限量折扣 对某类商品进行折扣 对某个商品进行折扣 不折扣 以往我们拿到这个需求，就很不冷静地开始改代码，修改成了如下一段代码：这里增加了的 if else 语句，并不是一种好的变更方式。如果每次都这样变更，那么软件必然就会退化，进入难以维护的状态。这种变更为什么不好呢？因为它违反了“开放-封闭原则”。开闭原则（OCP） 分为开放原则与封闭原则两部分。 开放原则：我们开发的软件系统，对于功能扩展是开放的（Open for Extension），即当系统需求发生变更时，可以对软件功能进行扩展，使其满足用户新的需求。 封闭原则：对软件代码的修改应当是封闭的（Close for Modification），即在修改软件的同时，不要影响到系统原有的功能，所以应当在不修改原有代码的基础上实现新的功能。也就是说，在增加新功能的时候，新代码与老代码应当隔离，不能在同一个类、同一个方法中。 前面的设计，在实现新功能的同时，新代码与老代码在同一个类、同一个方法中了，违反了“开闭原则”。怎样才能既满足“开闭原则”，又能够实现新功能呢？在原有的代码上你发现什么都做不了！难道“开闭原则”错了吗？问题的关键就在于，当我们在实现新需求时，应当采用“两顶帽子”的方式进行设计，这种方式就要求在每次变更时，将变更分为两个步骤。两顶帽子： 在不添加新功能的前提下，重构代码，调整原有程序结构，以适应新功能； 实现新的功能。按以上案例为例，为了实现新的功能，我们在原有代码的基础上，在不添加新功能的前提下调整原有程序结构，我们抽取出了 Strategy 这样一个接口和“不折扣”这个实现类。这时，原有程序变了吗？没有。但是程序结构却变了，增加了这样一个接口，称之为“可扩展点”。在这个可扩展点的基础上再实现各种折扣，既能满足“开放-封闭原则”来保证程序质量，又能够满足新的需求。当日后发生新的变更时，什么类型的折扣有变化就修改哪个实现类，添加新的折扣类型就增加新的实现类，维护成本得到降低。“两顶帽子”的设计方式意义重大。过去，我们每次在设计软件时总是担心日后的变更，就很不冷静地设计了很多所谓的“灵活设计”。然而，每一种“灵活设计”只能应对一种需求变更，而我们又不是先知，不知道日后会发生什么样的变更。最后的结果就是，我们期望的变更并没有发生，所做的设计都变成了摆设，它既不起什么作用，还增加了程序复杂度；我们没有期望的变更发生了，原有的程序依然不能解决新的需求，程序又被打回了原形。因此，这样的设计不能真正解决未来变更的问题，被称为“过度设计”。有了“两顶帽子”，我们不再需要焦虑，不再需要过度设计，正确的思路应当是“活在今天的格子里做今天的事儿”，也就是为当前的需求进行设计，使其刚刚满足当前的需求。所谓的“高质量的软件设计”就是要掌握一个平衡，一方面要满足当前的需求，另一方面要让设计刚刚满足需求，从而使设计最简化、代码最少。这样做，不仅软件设计质量提高了，设计难点也得到了大幅度降低。简而言之，保持软件设计不退化的关键在于每次需求变更的设计，只有保证每次需求变更时做出正确的设计，才能保证软件以一种良性循环的方式不断维护下去。这种正确的设计方式就是“两顶帽子”。但是，在实践“两顶帽子”的过程中，比较困难的是第一步。在不添加新功能的前提下，如何重构代码，如何调整原有程序结构，以适应新功能，这是有难度的。很多时候，第一次变更、第二次变更、第三次变更，这些事情还能想清楚；但经历了第十次变更、第二十次变更、第三十次变更，这些事情就想不清楚了，设计开始迷失方向。那么，有没有一种方法，让我们在第十次变更、第二十次变更、第三十次变更时，依然能够找到正确的设计呢？有，那就是“领域驱动设计”。保持软件质量：领域驱动前面谈到，软件的本质就是对真实世界的模拟。因此，我们会有一种想法，能不能将软件设计与真实世界对应起来，真实世界是什么样子，那么软件世界就怎么设计。如果是这样的话，那么在每次需求变更时，将变更还原到真实世界中，看看真实世界是什么样子的，根据真实世界进行变更。这样，日后不论怎么变更，经过多少轮变更，都按照这样的方法进行设计，就不会迷失方向，设计质量就可以得到保证，这就是“领域驱动设计”的思想。那么，如何将真实世界与软件世界对应起来呢？这样的对应就包括以下三个方面的内容： 真实世界有什么事物，软件世界就有什么对象； 真实世界中这些事物都有哪些行为，软件世界中这些对象就有哪些方法； 真实世界中这些事物间都有哪些关系，软件世界中这些对象间就有什么关联。 在领域驱动设计中，就将以上三个对应，先做成一个领域模型，然后通过这个领域模型指导程序设计；在每次需求变更时，先将需求还原到领域模型中分析，根据领域模型背后的真实世界进行变更，然后根据领域模型的变更指导软件的变更，设计质量就可以得到提高。结合电商支付实际演练DDD现在，我们以电商网站的支付功能为例，来演练一下基于 DDD 的软件设计及其变更的过程。运用 DDD 进行软件设计开发人员在最开始收到的关于用户付款功能的需求描述是这样的： 在用户下单以后，经过下单流程进入付款功能； 通过用户档案获得用户名称、地址等信息； 记录商品及其数量，并汇总付款金额； 保存订单； 通过远程调用支付接口进行支付。 以往当拿到这个需求时，开发人员往往草草设计以后就开始编码，设计质量也就不高。而采用领域驱动的方式，在拿到新需求以后，应当先进行需求分析，设计领域模型。按照以上业务场景，可以分析出： 该场景中有“订单”，每个订单都对应一个用户； 一个用户可以有多个用户地址，但每个订单只能有一个用户地址； 此外，一个订单对应多个订单明细，每个订单明细对应一个商品，每个商品对应一个供应商。 最后，我们对订单可以进行“下单”“付款”“查看订单状态”等操作。因此形成了以下领域模型图：有了这样的领域模型，就可以通过该模型进行以下程序设计：通过领域模型的指导，将“订单”分为订单 Service 与值对象，将“用户”分为用户 Service 与值对象，将“商品”分为商品 Service 与值对象……然后，在此基础上实现各自的方法。商品折扣的需求变更当电商网站的付款功能按照领域模型完成了第一个版本的设计后，很快就迎来了第一次需求变更，即增加折扣功能，并且该折扣功能分为限时折扣、限量折扣、某类商品的折扣、某个商品的折扣与不折扣。当我们拿到这个需求时应当怎样设计呢？很显然，在 payoff() 方法中去插入 if else 语句是不 OK 的。这时，按照领域驱动设计的思想，应当将需求变更还原到领域模型中进行分析，进而根据领域模型背后的真实世界进行变更。这是上一个版本的领域模型，现在我们要在这个模型的基础上增加折扣功能，并且还要分为限时折扣、限量折扣、某类商品的折扣等不同类型。这时，我们应当怎么分析设计呢？首先要分析付款与折扣的关系。付款与折扣是什么关系呢？你可能会认为折扣是在付款的过程中进行的折扣，因此就应当将折扣写到付款中。这样思考对吗？我们应当基于什么样的思想与原则来设计呢？这时，另外一个重量级的设计原则应该出场了，那就是“单一职责原则”。单一职责原则：软件系统中的每个元素只完成自己职责范围内的事，而将其他的事交给别人去做，我只是去调用。单一职责原则是软件设计中一个非常重要的原则，但如何正确地理解它成为一个非常关键的问题。在这句话中，准确理解的关键就在于“职责”二字，即自己职责的范围到底在哪里。以往，我们错误地理解这个“职责”就是做某一个事，与这个事情相关的所有事情都是它的职责，正因为这个错误的理解，带来了许多错误的设计，而将折扣写到付款功能中。那么，怎样才是对“职责”正确的理解呢？“一个职责就是软件变化的一个原因”是著名的软件大师 Bob 大叔在他的《敏捷软件开发：原则、模式与实践》中的表述。但这个表述过于精简，很难深刻地理解其中的内涵。这里我好好解读一下这句话。先思考一下什么是高质量的代码？你可能立即会想到“低耦合、高内聚”，以及各种设计原则，但这些评价标准都太“虚”。最直接、最落地的评价标准就是，当用户提出一个需求变更时，为了实现这个变更而修改软件的成本越低，那么软件的设计质量就越高。当来了一个需求变更时，怎样才能让修改软件的成本降低呢？如果为了实现这个需求，需要修改 3 个模块的代码，完后这 3 个模块都需要测试，其维护成本必然是“高”。那么怎样才能降到最低呢？如果只需要修改 1 个模块就可以实现这个需求，维护成本就要低很多了。那么，怎样才能在每次变更的时候都只修改一个模块就能实现新需求呢？那就需要我们在平时就不断地整理代码，将那些因同一个原因而变更的代码都放在一起，而将因不同原因而变更的代码分开放，放在不同的模块、不同的类中。这样，当因为这个原因而需要修改代码时，需要修改的代码都在这个模块、这个类中，修改范围就缩小了，维护成本降低了，修改代码带来的风险自然也降低了，设计质量也就提高了。总之，单一职责原则要求我们在维护软件的过程中需要不断地进行整理，将软件变化同一个原因的代码放在一起，将软件变化不同原因的代码分开放。按照这样的设计原则，回到前面那个案例中，那么应当怎样去分析“付款”与“折扣”之间的关系呢？只需要回答两个问题： 当“付款”发生变更时，“折扣”是不是一定要变？ 当“折扣”发生变更时，“付款”是不是一定要变？ 当这两个问题的答案是否定时，就说明“付款”与“折扣”是软件变化的两个不同的原因，那么把它们放在一起，放在同一个类、同一个方法中，合适吗？不合适，就应当将“折扣”从“付款”中提取出来，单独放在一个类中。同样的道理： 当“限时折扣”发生变更的时候，“限量折扣”是不是一定要变？ 当“限量折扣”发生变更的时候，“某类商品的折扣”是不是一定要变？ …… 最后发现，不同类型的折扣也是软件变化不同的原因。将它们放在同一个类、同一个方法中，合适吗？通过以上分析，我们做出了如下设计：在该设计中，将折扣功能从付款功能中独立出去，做出了一个接口，然后以此为基础设计了各种类型的折扣实现类。这样的设计，当付款功能发生变更时不会影响折扣，而折扣发生变更的时候不会影响付款。同样，当“限时折扣”发生变更时只与“限时折扣”有关，“限量折扣”发生变更时也只与“限量折扣”有关，与其他折扣类型无关。变更的范围缩小了，维护成本就降低了，设计质量提高了。这样的设计就是“单一职责原则”的真谛。接着，在这个版本的领域模型的基础上进行程序设计，在设计时还可以加入一些设计模式的内容，因此我们进行了如下的设计：显然，在该设计中加入了“策略模式”的内容，将折扣功能做成了一个折扣策略接口与各种折扣策略的实现类。当哪个折扣类型发生变更时就修改哪个折扣策略实现类；当要增加新的类型的折扣时就再写一个折扣策略实现类，设计质量得到了提高。VIP 会员的需求变更在第一次变更的基础上，很快迎来了第二次变更，这次是要增加 VIP 会员，业务需求如下。增加 VIP 会员功能： 对不同类型的 VIP 会员（金卡会员、银卡会员）进行不同的折扣； 在支付时，为 VIP 会员发放福利（积分、返券等）； VIP 会员可以享受某些特权。 我们拿到这样的需求又应当怎样设计呢？同样，先回到领域模型，分析“用户”与“VIP 会员”的关系，“付款”与“VIP 会员”的关系。在分析的时候，还是回答那两个问题： “用户”发生变更时，“VIP 会员”是否要变； “VIP 会员”发生变更时，“用户”是否要变。 通过分析发现，“用户”与“VIP 会员”是两个完全不同的事物。 “用户”要做的是用户的注册、变更、注销等操作； “VIP 会员”要做的是会员折扣、会员福利与会员特权； 而“付款”与“VIP 会员”的关系是在付款的过程中去调用会员折扣、会员福利与会员特权。 通过以上的分析，我们做出了以下版本的领域模型：有了这些领域模型的变更，然后就可以以此作为基础，指导后面程序代码的变更了。支付方式的需求变更同样，第三次变更是增加更多的支付方式，我们在领域模型中分析“付款”与“支付方式”之间的关系，发现它们也是软件变化不同的原因。因此，我们果断做出了这样的设计：而在设计实现时，因为要与各个第三方的支付系统对接，也就是要与外部系统对接。为了使第三方的外部系统的变更对我们的影响最小化，在它们中间果断加入了“适配器模式”，设计如下：通过加入适配器模式，订单 Service 在进行支付时调用的不再是外部的支付接口，而是“支付方式”接口，与外部系统解耦。只要保证“支付方式”接口是稳定的，那么订单 Service 就是稳定的。比如： 当支付宝支付接口发生变更时，影响的只限于支付宝 Adapter； 当微信支付接口发生变更时，影响的只限于微信支付 Adapter； 当要增加一个新的支付方式时，只需要再写一个新的 Adapter。 日后不论哪种变更，要修改的代码范围缩小了，维护成本自然降低了，代码质量就提高了。写在最后软件发展的规律就是逐步由简单软件向复杂软件转变。简单软件有简单软件的设计，复杂软件有复杂软件的设计。因此，当软件由简单软件向复杂软件转变时，就需要通过两顶帽子适时地对程序结构进行调整，再实现新需求，只有这样才能保证软件不退化。然而，在变更的时候，如何调整代码以适应新的需求呢？DDD 给了我们思路：在每次变更的时候，先回到领域模型，基于业务进行领域模型的变更。然后，再基于领域模型的变更，指导程序的变更。这样，不论经历多少次需求变更，始终能够保持设计质量不退化。这样的设计，才能保障系统始终在低成本的状态下，可持续地不断维护下去。本文我们演练了如何运用 DDD 进行软件的设计与变更，以及在设计与变更的过程中如何分析思考、如何评估代码、如何实现高质量。后续文章，我们将结合具体案例分析如何将领域模型的设计进一步落实到软件系统的微服务设计与数据库设计。文章来源- 二马读书作者范钢，曾任航天信息首席架构师，《大话重构》一书的作者。本文结合电商支付场景详细描述了领域驱动模型的实际应用。" }, { "title": "继承关系的 3 种数据库设计", "url": "/posts/%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB%E5%88%B0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E7%9A%84%E8%BD%AC%E6%8D%A2/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-04-15 00:00:00 +0800", "snippet": "继承关系的 3 种数据库设计简介：继承关系是在领域模型设计中有，但在数据库设计中却没有。如何将领域模型中的继承关系转换成数据库设计呢？有 3 种方案可以选择。1. 继承关系的第一种方案首先，看看以上案例。“执法行为”通过继承分为“正确行为”和“过错行为”。如果这种继承关系的子类不多（一般就 2 ~ 3 个），并且每个子类的个性化字段也不多（3 个以内）的话，则可以使用一个表来记录整个继承关系。在这个表的中间有一个标识字段，标识表中的每条记录到底是哪个子类，这个字段的前面部分罗列的是父类的字段，后面依次罗列各个子类的个性化字段。采用这个方案的优点是简单，整个继承关系的数据全部都保存在这个表里。但是，它会造成“表稀疏”。在该案例中，如果是一条“正确行为”的记录，则字段“过错类型”与“扣分”永远为空；如果是一条“过错行为”的记录，则字段“加分”永远为空。假如这个继承关系中各子类的个性化字段很多，就会造成该表中出现大量字段为空，称为“表稀疏”。在关系型数据库中，为空的字段是要占用空间的。因此，这种“表稀疏”既会浪费大量存储空间，又会影响查询速度，是需要极力避免的。所以，当子类比较多，或者子类个性化字段多的情况是不适合该方案（第一种方案）的。2. 继承关系的第二种方案如果执法行为按照考核指标的类型进行继承，分为“考核指标1”“考核指标2”“考核指标3”……如下图所示：并且每个子类都有很多的个性化字段，则采用前面那个方案就不合适了。这时，用另外两个方案进行数据库设计。其中一个方案是将每个子类都对应到一个表，有几个子类就有几个表，这些表共用一个主键，即这几个表的主键生成器是一个，某个主键值只能存在于某一个表中，不能存在于多个表中。每个表的前面是父类的字段，后面罗列各个子类的字段，如下图所示：如果业务需求是在前端查询时，每次只能查询某一个指标，那么采用这种方案就能将每次查询落到某一个表中，方案就最合适。但如果业务需求是要查询某个过错责任人涉及的所有指标，则采用这种方案就必须要在所有的表中进行扫描，那么查询效率就比较低，并不适用。3. 继承关系的第三种方案如果业务需求是要查询某个过错责任人涉及的所有指标，则更适合采用以下方案，将父类做成一个表，各个子类分别对应各自的表（如图所示）。这样，当需要查询某个过错责任人涉及的所有指标时，只需要查询父类的表就可以了。如果要查看某条记录的详细信息，再根据主键与类型字段，查询相应子类的个性化字段。这样，这种方案就可以完美实现该业务需求。综上所述，将领域模型中的继承关系转换成数据库设计有 3 种方案，并且每个方案都有各自的优缺点。因此，需要根据业务场景的特点与需求去评估，选择哪个方案更适用。4. NoSQL 数据库的设计前面我们讲的数据库设计，还是基于传统的关系型数据库、基于第三范式的数据库设计。但是，随着互联网高并发与分布式技术的发展，另一种全新的数据库类型孕育而生，那就是NoSQL 数据库。正是由于互联网应用带来的高并发压力，采用关系型数据库进行集中式部署不能满足这种高并发的压力，才使得分布式 NoSQL 数据库得到快速发展。也正因为如此，NoSQL 数据库与关系型数据库的设计套路是完全不同的。关系型数据库的设计是遵循第三范式进行的，它使得数据库能够大幅度降低冗余，但又从另一个角度使得数据库查询需要频繁使用 join 操作，在高并发场景下性能低下。所以，NoSQL 数据库的设计思想就是尽量干掉 join 操作，即将需要 join 的查询在写入数据库表前先进行 join 操作，然后直接写到一张单表中进行分布式存储，这张表称为“宽表”。这样，在面对海量数据进行查询时，就不需要再进行 join 操作，直接在这个单表中查询。同时，因为 NoSQL 数据库自身的特点，使得它在存储为空的字段时不占用空间，不担心“表稀疏”，不影响查询性能。因此，NoSQL 数据库在设计时的套路就是，尽量在单表中存储更多的字段，只要避免数据查询中的 join 操作，即使出现大量为空的字段也无所谓了。增值税发票票样图正因为 NoSQL 数据库在设计上有以上特点，因此将领域模型转换成 NoSQL 数据库时，设计就完全不一样了。比如，这样一张增值税发票，如上图所示，在数据库设计时就需要分为发票信息表、发票明细表与纳税人表，而在查询时需要进行 4 次 join 才能完成查询。但在 NoSQL 数据库设计时，将其设计成这样一张表：{ _id: ObjectId(7df78ad8902c) fpdm: &#39;3700134140&#39;, fphm: &#39;02309723‘, kprq: &#39;2016-1-25 9:22:45&#39;, je: 70451.28, se: 11976.72, gfnsr: { nsrsbh: &#39;370112582247803&#39;, nsrmc:&#39;联通华盛通信有限公司济南分公司&#39;,… }, xfnsr: { nsrsbh: &#39;370112575587500&#39;, nsrmc:&#39;联通华盛通信有限公司济南分公司&#39;,… }, spmx: [ { qdbz:&#39;00&#39;, wp_mc:&#39;蓝牙耳机 车语者S1 蓝牙耳机&#39;, sl:2, dj:68.37,… }, { qdbz:&#39;00&#39;, wp_mc:&#39;车载充电器 新在线&#39;, sl:1, dj:11.11,… }, { qdbz:&#39;00&#39;, wp_mc:&#39;保护壳 非尼膜属 iPhone6 电镀壳&#39;, sl:1, dj:24,… } ]}在该案例中，对于“一对一”和“多对一”关系，在发票信息表中通过一个类型为“对象”的字段来存储，比如“购方纳税人（gfnsr）”与“销方纳税人（xfnsr）”字段。对于“一对多”和“多对多”关系，通过一个类型为“对象数组”的字段来存储，如“商品明细（spmx）”字段。在这样一个发票信息表中就可以完成对所有发票的查询，无须再进行任何 join 操作。同样，采用 NoSQL 数据库怎样实现继承关系的设计呢？由于 NoSQL 数据库自身的特点决定了不用担心“表稀疏”，同时要避免 join 操作，所以比较适合采用第一个方案，即将整个继承关系放到同一张表中进行设计。这时，NoSQL 数据库的每一条记录可以有不一定完全相同的字段，可以设计成这样：{ _id: ObjectId(79878ad8902c), name: ‘Jack’, type: ‘parent’, partner: ‘Elizabeth’, children: [ { name: ‘Tom’, gender: ‘male’ }, { name: ‘Mary’, gender: ‘female’} ]},{ _id: ObjectId(79878ad8903d), name: ‘Bob’, type: ‘kid’, mother: ‘Anna’, father: ‘David’}以上案例是一个用户档案表，有两条记录：Jack 与 Bob。但是，Jack 的类型是“家长”，因此其个性化字段是“伴侣”与“孩子”；而 Bob 的类型是“孩子”，因此他的个性化字段是“父亲”与“母亲”。显然，在 NoSQL 数据库设计时就会变得更加灵活。文章来源- 二马读书作者范钢，曾任航天信息首席架构师，《大话重构》一书的作者。本文结合电商支付场景详细描述了领域驱动模型的实际应用。" }, { "title": "白盒SPNBOX 论文阅读笔记", "url": "/posts/%E7%99%BD%E7%9B%92SPNBOX%E7%AC%94%E8%AE%B0/", "categories": "白盒密码学", "tags": "白盒密码学", "date": "2022-04-02 00:00:00 +0800", "snippet": "简评：本文是对Space-Hardness一文的补充，主要贡献在于完善了Sparc-Hardness攻击者的安全模型。同时提出了SPNBOX新方案，考虑并行化和硬件优化，效率相交于SPACE有了数倍提升。原文：Towards Practical Whitebox Cryptography: Optimizing Efficiency and Space Hardness1. Introduction白盒的应用场景 云服务DRM场景 NFC付款场景这些场景给出了在服务下的新的攻击模式 共享缓存攻击：cache attacks，云服务器上的内存资源是共享的，攻击者可以利用共享的内存数据来进行攻击 密钥恢复攻击：常规的黑盒场景，攻击者试图恢复密钥。 密钥提取攻击：常规的白盒场景，攻击者试图从白盒实现中提取密钥。 代码提升攻击（decomposition attacks or code lifting attack）：攻击者窃取一部分的白盒实现，试图加密或解密数据。贡献点 设计了新的白盒方法，SPNbox。效率相较于SPACE有数倍的提升，并且可以用到硬件加速。 对SPNbox在黑盒环境下做了安全性分析。 重新定义了代码分解攻击设定 给出了Space-Hard的可证明安全的上界。2 SPNbox: Efficient Space-Hard Block Ciphers设计决策 From Feistel to nested SPN：从Fristel结构到嵌套SPN结构（nested SPN structure）： 考虑到并行的要求，从Feistel结构切换到SPN结构。 考虑到要适用于不同内存大小的场景，使用nested SPN structure。 考虑到硬件加速的需求，AES不能有截断（SPACE方案对AES作了128-&amp;gt;120的截断） 考虑高效率的解密操作，用了MDS矩阵 Efficient Constant-Time Small Block Ciphers：使用 高效率的恒定时间的小规模加密算法设计实现总体思路SPNbox 的输入是n=128bit，分成t组，每组大小为n-in = n/t bit。n-in = 的取值有8，16，24，32。置换操作共有10轮，每轮的变换如下。 S 是一个小型加密器层，仿照AES实现（The Underlying Small Block Ciphers） θ是线性扩散层，就是乘以一个MDS矩阵 σr是仿射层，操作就是加上每轮的常量C，常量C有轮数决定。S层：The Underlying Small Block Ciphers S本身就是一个block cipher，内部需要做很多轮（R-in），基本思路就是参照AES每一轮做实现。 R-32 = 16, R-24 = 20, R-16 = 32 and R-8 = 64. S每一轮操作如下图，首先经过一个非线性S-box，然后做MC = MixCloumn，最后AK=加上每一轮的密钥Key。当n-in = 32时，这个加密器和AES每一轮变换是一样的。 3. Security in the Black Box: Analysis as a Block Cipher黑盒的安全性分析这一部分主要是依赖于AES的安全性。4. Security in the White Box: Analysis of Space Hardness白盒的安全性分析4.1Key Extraction and Table Decomposition Attacks安全性可以规约到内置的小型加密器的密钥提取问题上，也就是AES困难假设上。4.2 Existing Notions of Space Hardness把week-space-hard 和 strong space-hard 区分开4.3 Target Construction将白盒加密过程形式化归类为下图，，输入明文，输出密文。白盒加密分为R轮，每一轮有T个查找表。攻击者可以看到整个执行环境。4.4 Adversary Models of Space Hardness进一步定义了攻击者的能力，攻击模式有三种 know-space，chosen-space，adaptively-chosen space known-space attack：模拟的是攻击者被动的接收到一些信息。 chosen-space attack：模拟的是攻击者可以选择接收那些信息。 adaptively-chosen-space attack：模拟的是攻击者可以自适应的选择接收信息，对执行环境有完全控制权。和最开始的白盒环境定义相同。" }, { "title": "白盒Space-Hardness 论文阅读笔记", "url": "/posts/%E7%99%BD%E7%9B%92Space-hard%E7%AC%94%E8%AE%B0/", "categories": "白盒密码学", "tags": "白盒密码学", "date": "2022-03-28 00:00:00 +0800", "snippet": "原文：White-box Cryptography Revisited: Space-Hard Ciphers简评：白盒密码学的经典好文，开辟了白盒密码学的新方向。本文可以当作综述来看，对15年以前的白盒算法了非常好的总结。1. INTRODUCTION主要贡献点 White-box security is based on black-box security:提出了新的白盒算法，同时该算法的安全性依赖于内置的黑盒算法（AES），安全性有保障 Space hardness：开辟了白盒密码学的新方向，Space-Hard No external encoding：新白盒算法不再依赖于外部编码，因为外部编码在实践中几乎不可用 Variable white-box implementation size:可变长度的白盒实现，可以适用于不同场景2. ATTACK MODELS介绍了黑盒和白盒的攻击模型黑盒黑盒场景该模型假设攻击者能够使用已知或选择的明文或密文访问密码的输入和输出。 可以允许自适应查询。黑盒的两个安全性要求 Key recovery security：攻击者不能计算出密钥 Distinguishing security：攻击者不能区分随机字符串和加密字符串白盒白盒场景该模型假设攻击者可以通过任意跟踪执行、检查内存中的子结果和密钥、插入断点、修改内部代码以静态和动态方式完全控制密码的执行环境，变量等等。白盒的两个安全性要求 Key extraction security：不能被提取出密钥 Code lifting security：攻击者可以直接拿白盒表去做加解密。这要求白盒（代码）不能被剥离。3. KNOWN WHITE-BOX TECHNIQUES白盒AES实现代表作- Chow 白盒 2002（把chow的白盒AES讲解的很透彻和简洁）安全性问题目前（2015年）所有白盒实现几乎都被攻破了，还有一种针对SPN结构的通用攻击。或许是因为目前的白盒实现 都在考虑 黑盒安全，或许使得白盒安全难以达到 一种针对SPN结构的通用攻击，可以看看 【31】Wil Michiels, Paul Gorissen, and Henk D. L. Hollmann. Cryptanalysis of a Generic Class of White-Box Implementations. In Selected Areas in Cryptography - SAC 2008, LNCS, Vol. 7707, pages 414–428, 2008.外部编码的问题 影响互操作性，互通性（interoperability），对于一些企业非常强调互操作性，比如银行。 外部编码需要安全执行环境：这个要求对于DRM环境本身就是冲突的。既然有安全执行环境，为什么不所有解密操作都在安全环境中执行呢（笑）。但移除外部编码，又会导致不安全，因为第一轮和最后一轮查找表直接可以被攻击者获取破解。专用方法：ASASA不依赖于外部编码，2014年由Biryukov提出。弱白盒安全性定义Weak White-box Security不可压缩性: 如果在完全访问 F 的情况下很难获得大小小于 T 的等效密钥，则函数 F 是 EK 的 T 安全弱白盒实现。换句话说，攻击者在计算上应该很难找到任何小于 T 的紧凑等效函数。因此，攻击者需要大小为 T 的代码来完全复制密码的功能。 弱白盒安全性可以通过需要从白盒环境中提取的数据量来估计代码提升的难度。 这个可以看下, 这篇文章提出了弱白盒和强白盒定义 [4] Alex Biryukov, Charles Bouillaguet, and Dmitry Khovratovich. Cryptographic Schemes Based on the ASASA Structure: Black-Box, White-Box, and Public-Key (Extended Abstract). In Advances in Cryptology - ASIACRYPT 2014, LNCS, Vol. 8873, pages 63–84, 2014安全性问题已经提出了针对ASASA的攻击方法，并且发现了AS结构的通用攻击。4. OUR DESIGN GOALS安全性目标(M, Z)-space hardness新的安全性指标 (M, Z)-space hardness在攻击者只能获取小于M大小的代码时，其能够加解密随机的明文（密文）的可能性少于2^-z。功能性目标 不依赖于外部编码性能目标 能和现有白盒实现相当 能适用于不同的场景，由多个参数调节代码大小。5.SPACE: FIXED SPACE算法设计SPACE 是一个 l 行 的 Feistel 网络结构，输入是n-bit的明文，输出是n-bit的密文。总共有r轮的变换。对于每一轮的变换，又分成l行。变换操作看图很好理解。这里其实是把每一轮的密钥由F函数生成。生成的密钥和其他行去做异或。接下来看F函数的构成，F函数是一个扩展函数。在白盒环境下，F函数被做成一个可以复用的查找表。Feistel结构整体来看，SPACE是使用了target-heavy Feistel 结构（Feistel结构的一种变种） 这里提到了Feistel结构由许多变种，type-1, -2, -3 generalized Feistel ，target-heavy Feistel这里不使用SPN结构的原因是ASASA采用的SPN结构需要秘密的置换过程。而SPACE用到了标准的AES算法作为内置函数，自定义的置换过程不太行。白盒下安全性分析密钥提取攻击该攻击不可能做到，因为SPACE的查找表就一个F函数查找表，里面是标准AES-128加密。要从查找表提取key，就相当于要攻破AES-128，甚至更难，因为查找表还对AES的输出做了截断。代码提取攻击（ Code Lifting Attack）假设攻击者只偷到了一部分的查找表，假设是i 条记录。攻击者能够区分加密数据的可能性为，R为加密轮次。困难程度随着攻击者能偷取到的查找表而降低，如下图Strong Space Hardness在攻击者只能获取小于M大小的代码时，其能够区分随机明文的加解密结果的可能性少于2^-z。6.N-SPACE: VARIABLE SPACE结合前面的SPACE方案，混合应用不同大小的F函数来实现可变的SPACE白盒方案。就是拿前面的SPACE盒子组合使用。" }, { "title": "网络安全——DDoS拒绝访问攻击与防御", "url": "/posts/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8-DDoS%E6%8B%92%E7%BB%9D%E8%AE%BF%E9%97%AE%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1/", "categories": "课程", "tags": "课程, 网络安全", "date": "2022-03-27 00:00:00 +0800", "snippet": "网络安全——拒绝访问攻击与防御[TOC]前置工作实验系统设计实验系统架构设计如下图所示。攻击集群由两台本地机器和⼀个云服务器节点组成。其中⽤于攻击的服务器节点配置为1C2G，带宽4Mbps，流量上限500GB/⽉。具体攻击⽅式和过程将在后续章节描述。攻击流量将经由互联⽹到达⼀台位于阿里云的服务器节点。该服务器配置为1C2G，带宽5Mbps，流量上限1000GB/月。 在被攻击⼀侧，设置有基于Oracle Apex的简单Web服务程序。该程序基于Tomcat服务器，通过ORDS与Apex Engine访问Oracle数据库。应用提供⼀些简单的RESTful接⼝。该程序默认监听8080端⼝，当8080端口并不对外网开放。同时有Nginx监听80端⼝反向代理该Web服务器。此外，配置有 CentOS默认的firewall防⽕墙。 为实现对被攻击节点和攻击流量的实时监控和可视化，被攻击节点同时配置有 Prometheus的Node Exporter，在9100端⼝提供当前服务器节点的负载和⽹络连接数 据。Web服务程序通过引⼊Prometheus的micrometer提供服务层⾯的流量和负载数 据。监控平台节点是⼀台位于阿里云的1C2G服务器，带宽上限1Mbps，设置有 Prometheus实时采集和存储上述数据，配置Grafana作为监控数据可视化的⼯作平台。被攻击应用介绍为实现分布式拒绝服务攻击的真实模拟，我们选用了一款在企业中实际使用的Web应用——进销存系统。该应用基于Oracle Apex等框架和⼯具实现。为模拟攻击的真实性，服务器程序初始运⾏时将数千条销售单数据员写⼊数据库。在校园实验室内的⼀般网络条件下，耗时低的接口响应时间约50ms，耗时高接口响应时间在500ms以上。应用页面如下图所示。节点监控为便捷监控节点负载，实时显示攻击流量和效果，我们额外使⽤⼀台服务器节点部署 Prometheus实时收集被攻击节点的负载以及Tomcat内存使用和GC频率等数据，并部署Grafana作为数据可视化的平台。部署启动后，我们使用社区设计提供的 Node Exporter for Prometheus Dashboard、 和 SLS JVM监控⼤盘分别展示服务器节点、⽹络和应用程序负载数据。部分界⾯效果如下。网络攻击HTTP泛洪攻击（HTTP Flood）HTTP泛洪攻击简介 定义 HTTP 洪水攻击是一种大规模分布式拒绝服务 (DDoS)攻击，旨在利用HTTP 请求使目标服务器不堪重负。目标因请求而达到饱和，且无法响应正常流量后，将出现拒绝服务，拒绝来自实际用户的其他请求。 原理 HTTP 洪水攻击是“第 7 层”DDoS 攻击的一种。第 7 层是 OSI 模型的应用程序层，指的是 HTTP 等互联网协议。HTTP 是基于浏览器的互联网请求的基础，通常用于加载网页或通过互联网发送表单内容。缓解应用程序层攻击特别复杂，因为恶意流量和正常流量很难区分。 为了获得最大效率，恶意行为者通常会利用或创建僵尸网络，以最大程度地扩大攻击的影响。通过利用感染了恶意软件的多台设备，攻击者可以发起大量攻击流量来进行攻击。 分类 HTTP GET 攻击 在这种攻击形式下，多台计算机或其他设备相互协调，向目标服务器发送对图像、文件或其他资产的多个请求。当目标被传入的请求和响应所淹没时，来自正常流量源的其他请求将被拒绝服务。 HTTP POST 攻击 一般而言，在网站上提交表单时，服务器必须处理传入的请求并将数据推送到持久层（通常是数据库）。与发送 POST 请求所需的处理能力和带宽相比，处理表单数据和运行必要数据库命令的过程相对密集。这种攻击利用相对资源消耗的差异，直接向目标服务器发送许多 POST 请求，直到目标服务器的容量饱和并拒绝服务为止。 HOIC工具介绍 简介 HOIC（The high orbit ion cannon）是一款基于HTTP 协议开源的DDOS 攻击工具，可用于内部网络或者外部服务器的安全性和稳定性的测试等等。 特点 高速多线程的HTTP 洪水攻击。 一次可同时洪水攻击高达256 个网站。 内置脚本系统，允许自行修改设置脚本，用来阻挠DDoS 攻 击的防御措施，并增加DOS 输出。 简单且易于使用的界面。 可移植到Linux/ Mac。 能够选择攻击的线程数。 可设置三种攻击强度：低，中，高。 用REALbasic 这种极其书面的语言写成，简单易修改。 关键功能界面 实验内容我们从HOIC的维基百科主页提供的下载地址中将HOIC下载到本地windows电脑当中。这个软件提供win、mac和linux不同版本。HOIC的使用对于普通用户来说也十分容易上手，打开下载压缩包中的exe文件，我们就能看到软件的用户界面。在这个用户界面当中，我们点击“+”按钮就可以添加攻击的目标，在弹出的窗口当中，我们将攻击目标的url填入输入框当中，下面两个选项允许用户选择攻击的强度以及模式。另外，在软件界面上，用户还可以选择使用到的攻击线程的数量。值得一提的是，HOIC会自动适配用户本地的机器配置，当所选线程超过机器的支持时，显示数量将变为红色。在本次实验当中，我们分别使用 ”Low强度 + 2线程“ 和 “High强度 + 4线程”对目标进行攻击。从实验当中我们观察到，哪怕只是采用一台普通的家用笔记本电脑，也可以在几分钟的时间里向攻击目标发送上百兆字节，这对于小型的服务器而言已经是很大的负担。在初次实验当中，目标服务器没有配置防御措施，高强度的HTTP洪泛攻击直接导致了目标网页的后台宕机，服务器的CPU占用率和内存使用率都达到了90%以上，此时其他的普通用户已经完全无法访问该网页。HTTP慢速攻击(low and slow attack)HTTP慢速攻击简介定义：HTTP慢速度攻击是DoS或DDoS攻击的一种。攻击者会向目标服务器非常缓慢地发送一小段数据流来占用服务器的连接会话，从而使服务器停止或无法快速地对真实用户进行响应。原理：服务器与客户端建立的每一个会话都依赖于一个线程。而线程资源是有限的，如果其被大量的慢速会话占用，会使得正常的HTTP被阻塞。类比：想象一座四车道的桥，每条车道都有一个收费站。司机把车停在收费亭前，交上一张钞票或一把硬币，然后开车过桥，为下一个司机腾出车道。现在想象一下，四名司机同时出现，占据了每一条开放的车道，他们每个人都慢慢地把硬币交给收费站操作员，一次一枚硬币，把所有可用的车道堵上几个小时，阻止其他司机通过。这种令人难以置信的令人沮丧的场景与慢速攻击的工作原理非常相似。特点：1. 与传统的蛮力攻击（例如泛洪攻击）相比，慢速度攻击占用的带宽更少，且其产生的流量与正常访问产生的流量相当，故很难被发现。2. 不需要很多资源（例如僵尸网络）来完成，使用一台计算机就可以成功地发起慢攻击。注意：服务器一般会配置会话的最长时间，故攻击者在缓慢发送数据时要确保不要让会话超时。HTTP慢速攻击分类攻击者可以通过使用HTTP头部、HTTP POST请求、TCP请求来实施攻击，具体如下： HTTP头部：攻击者缓慢地发送HTTP请求的头部信息，这会使得服务器一直保持连接以接受剩余头部。 HTTP POST请求：攻击者制造这样一种POST请求，该请求表示在body中会有很长的数据要发送，然后缓慢地发送body中的数据。 TCP请求：利用TCP三次握手的漏洞，创建一个不确定的连接，使该连接既不会断开也不可使用。slowhttptest工具简介slowhttptest是⼀个灵活易用的HTTP慢速攻击模拟⼯具，其支持客户线程数量、攻击时长、间隔时间等诸多配置，可选择基于HTTP头部或BODY部分的慢速攻击。该工具可通过Linux命令行启动并运行，命令参数如下所示： -g 在测试完成后，以时间戳为名生成一个CVS和HTML文件的统计数据 -H SlowLoris模式 -B Slow POST模式 -R Range Header模式 -X Slow Read模式 -c number of connections 测试时建立的连接数 -d HTTP proxy host:port 为所有连接指定代理 -e HTTP proxy host:port 为探测连接指定代理 -i seconds 在slowrois和Slow POST模式中，指定发送数据间的间隔。 -l seconds 测试维持时间 -n seconds 在Slow Read模式下，指定每次操作的时间间隔。 -o file name 使用-g参数时，可以使用此参数指定输出文件名 -p seconds 指定等待时间来确认DoS攻击已经成功 -r connections per second 每秒连接个数 -s bytes 声明Content-Length header的值 -t HTTP verb 在请求时使用什么操作，默认GET -u URL 指定目标url -v level 日志等级（详细度） -w bytes slow read模式中指定tcp窗口范围下限 -x bytes 在slowloris and Slow POST tests模式中，指定发送的最大数据长度 -y bytes slow read模式中指定tcp窗口范围上限 -z bytes 在每次的read()中，从buffer中读取数据量实验内容我们从https://github.com/shekyan/slowhttptest中下载slowhttptest工具至linux服务器上并根据官方教程进行安装。并运行如下命令：slowhttptest -c 10000 -l 3000 -u &amp;lt;url&amp;gt;。其中-c 10000表示攻击者将尝试建立10000个缓慢的连接，-l 3000表示攻击时间最多持续3000秒，-u &amp;lt;url&amp;gt;表示被攻击者的地址。控制台展示如下：从上图可以看出，同一时刻下平均有一千个正在工作的慢速连接。这时被攻击者开启防御模式，以限制单一客户端能够建立的连接个数，这时运行结果展示如下：可以看到同一时刻下平均只有一百个正在工作的慢速连接，说明被攻击者的防御方式已经生效。网络防御总体分析要防御DDOS分布式拒绝服务攻击，首先要从网络请求的处理过程进行分析。针对各个处理环节做出针对性防御措施。网络流量从客户端出发到请求结果返回，将依次通过服务器中多个节点。各个节点对流量进行分工处理，具体如下： 防火墙：根据防火墙规则过滤流量 Nginx：负责处理静态资源（例如图片、视频、CSS、JavaScript文件等），将动态请求交给Tomcat处理。 Tomcat：接收来自Nginx的流量，将请求包装为Http调用交给后端应用，并接收后端应用返回的结果。 Ords + Apex engin：接收Tomcat请求，负责后端逻辑处理，查询修改数据库并返回结果。 Oracle数据库：接收后端数据查询请求，查询磁盘数据并返回结果。在不同节点可以做出的针对性防御不同防火墙端 将恶意IP加入黑名单 只允许信任用户IP加入白名单 自动识别异常流量，并将异常IP加入黑名单。Web服务器端（Nginx和Tomcat） 限制接口的访问频率，文件下载速率。 限制TCP连接数，连接时长。 配置定制化的网络参数限制恶意攻击。应用后端（ORDS和APEX） 在应用层面对可疑用户进行限制，对重要接口访问做限流。 使用缓存来提高吞吐率Nginx是⼀个非常高效的网页服务器，通过配置可使⽤反向代理、负载均衡等多种功能。我们使⽤Nginx在80端口代理8080端口的全部请求，静态资源访问直接由Nginx处理，动态请求将交由Tomcat处理。本次实验主要使用Nginx来防御恶意攻击。泛洪攻击防御HTTP Flood 攻击模拟正常⽤户的浏览器浏览过程，⼤量向服务器发送请求以耗尽带 宽、CPU等资源。为减少泛洪攻击的影响，Nginx配置的核心思路是控制每个用户使用的资源大小。具体包括限制每个IP的速率，TCP连接数量，带宽大小。控制速率在 nginx.conf http 中添加限流配置： 格式：limit_req_zone key zone rate http { limit_req_zone $binary_remote_addr zone=myRateLimit:10m rate=10r/s; }配置 server，使用 limit_req 指令应用限流。 server { location / { limit_req zone=myRateLimit; proxy_pass http://my_upstream; } } key ：定义限流对象，binary_remote_addr 是一种key，表示基于 remote_addr(客户端IP) 来做限流，binary_ 的目的是压缩内存占用量。 zone：定义共享内存区来存储访问信息， myRateLimit:10m 表示一个大小为10M，名字为myRateLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。 rate 用于设置最大访问速率，rate=10r/s 表示每秒最多处理10个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 10r/s 实际上是限制：每100毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求。处理突发流量在正常业务使用中，业务也可能某一段时间中操作较为频繁。为避免速率控制对正常业务造成影响，需要配置Nginx处理突发流量。burst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数。当 rate=10r/s 时，将1s拆成10份，即每100ms可处理1个请求。此处，burst=20 ，若同时有21个请求到达，Nginx 会处理第一个请求，剩余20个请求将放入队列，然后每隔100ms从队列中获取一个请求进行处理。若请求数大于21，将拒绝处理多余的请求，直接返回503.不过，单独使用 burst 参数并不实用。假设 burst=50 ，rate依然为10r/s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待 50 * 100ms即 5s，这么长的处理时间自然难以接受。因此，burst 往往结合 nodelay 一起使用。 server { location / { limit_req zone=myRateLimit burst=20 nodelay; proxy_pass http://my_upstream; } }nodelay 针对的是 burst 参数，burst=20 nodelay 表示这20个请求立马处理，不能延迟，相当于特事特办。不过，即使这20个突发请求立马处理结束，后续来了请求也不会立马处理。burst=20 相当于缓存队列中占了20个坑，即使请求被处理了，这20个位置这只能按 100ms一个来释放。这就达到了速率稳定，但突然流量也能正常处理的效果限制连接数ngx_http_limit_conn_module 提供了限制连接数的能力，利用 limit_conn_zone 和 limit_conn 两个指令即可。下面是 Nginx 官方例子： limit_conn_zone $binary_remote_addr zone=perip:10m; limit_conn_zone $server_name zone=perserver:10m; server { ... limit_conn perip 10; limit_conn perserver 100; }limit_conn perip 10 作用的key 是 $binary_remote_addr，表示限制单个IP同时最多能持有10个连接。limit_conn perserver 100 作用的key是 $server_name，表示虚拟主机(server) 同时能处理并发连接的总数。需要注意的是：只有当 request header 被后端server处理后，这个连接才进行计数。限制带宽以下示例显示了用于限制连接数和带宽的组合配置。允许的最大连接数设置为5 每个客户端地址的连接数，这适合大多数情况，因为现代浏览器通常一次最多打开3个连接。同时，提供下载的位置仅允许一个连接：location /download/ { limit_conn addr 1; limit_rate_after 1m; limit_rate 50k;}防御效果系统负载和磁盘读写对比网络情况对比实测发现：防御前系统服务完全不可用，tomcat处于down机状态。防御后系统服务虽会出现些许卡顿，但仍能够正常使用。可以发现防御对防洪攻击有不错的抵抗效果，慢速攻击防御慢速攻击者会向目标服务器非常缓慢地发送一小段数据流来占用服务器的连接会话，从而使服务器停止或无法快速地对真实用户进行响应。为抵御慢速攻击，Nginx可以从三个方面入手进行配置。首先是Nginx的超时设置，请求时长限制在合理范围内。其次是禁用静态文件的访问日志，避免大量慢速攻击对磁盘IO的影响。最后是修改内核参数，对处于Time Wait的TCP连接进行限制，并允许系统服用已有连接Nginx的超时设置减少Nginx的最长请求时长，将请求时长限制在合理范围内，断开长期的死连接。 keepalive_timeout用于设置keepalive连接的超时时间，默认为65秒。若将它设置为0，那么就禁止了keepalive连接。它在http, server 和 location模块中定义。 send_timeout指定了向客户端传输数据的超时时间。默认值为60秒，可以在http, server 和 location模块中定义。 client_body_timeout设定客户端与服务器建立连接后发送request body的超时时间。如果客户端在此时间内没有发送任何内容，那么Nginx返回HTTP 408错误（Request Timed Out）。它的默认值是60秒，在http, server 和 location模块中定义。 client_header_timeout设定客户端向服务器发送一个完整的request header的超时时间。如果客户端在此时间内没有发送一个完整的request header，那么Nginx返回HTTP 408错误（Request Timed Out）。它的默认值是60秒，在http 和 server模块中定义。keepalive_timeout 30s;send_timeout 10s;client_body_timeout 5s;client_header_timeout 3s;Nginx禁用静态文件日志禁用Nginx静态文件的访问日志，避免大量慢速攻击对磁盘IO的影响，具体如下 提升 IO 性能 避免无意义的写入磁盘操作 便于实时查看 nginx 日志在Nginx 的配置文件中加入log_format compression &#39;$remote_addr - $remote_user [$time_local] &#39; &#39;&quot;$request&quot; $status $body_bytes_sent &#39; &#39;&quot;$http_referer&quot; &quot;$http_user_agent&quot; &quot;$gzip_ratio&quot;&#39;;map $uri $loggable { default 1; ~*\\.(ico|css|js|gif|jpg|jpeg|png|svg|woff|ttf|eot)$ 0;}access_log /var/log/nginx/access.log compression if=$loggable;修改Linux内核参数慢速攻击的原理是产生大量请求，占用服务器连接资源。并且这个TCP连接不能被正常用户使用。执行以下命令，编辑系统内核配置。vi /etc/sysctl.conf net.ipv4.tcp_syncookies=1：开启SYN的cookies，当出现SYN等待队列溢出时，启用cookies进行处理。 net.ipv4.tcp_tw_reuse=1：允许将TIME-WAIT的socket重新用于新的TCP连接。如果新请求的时间戳，比存储的时间戳更大，则系统将会从TIME_WAIT状态的存活连接中选取一个，重新分配给新的请求连接。 net.ipv4.tcp_fin_timeout=30：如果socket由服务端要求关闭，则该参数决定了保持在FIN-WAIT-2状态的时间。修改或加入以下内容。net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_fin_timeout = 30防御效果系统负责和磁盘读写网络情况防御前后TCP连接对比实测发现：可以发现，防御后磁盘io有所下降。网络情况虽然处于等待的TCP请求增多，但实际占用的TCP连接却显著减少。可以发现防御对慢速攻击有不错的抵抗效果。" }, { "title": "软件建模——分享我的业务建模经历", "url": "/posts/%E8%BD%AF%E4%BB%B6%E5%BB%BA%E6%A8%A1%E8%AF%BE%E7%A8%8B-%E5%88%86%E4%BA%AB%E6%88%91%E7%9A%84%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1%E7%BB%8F%E5%8E%86/", "categories": "课程", "tags": "课程, 软件建模", "date": "2022-03-25 00:00:00 +0800", "snippet": "分享我的业务建模经历我感触最多的是为公司开发一款进销存系统。项目介绍公司概况东莞市久源建材贸易有限公司是一家小微企业，成立于2017年，注册资本100万元。主营业务为五金建材、建筑材料、五金配件、螺杆的加工制作。目前公司在职员工达30人，客户群体规模达300人。系统功能简介进销存系统功能是对公司产品采购、商品销售、产品生产、库存管理、财务汇总、数据分析等工作进行信息化管理，目的是提高进、销、存三方面工作的效率。主要功能包括客户跟踪、销售记录、成本核算、库存盘点、财务分析、员工薪酬等功能。参与人员 开发者只有我一人，负责系统分析-设计-实现-维护整个流程。 利益相关方有3~4人，同时也是系统最终用户。按用户建模可分为两类角色，员工和老板。 员工：主要职能是录入系统数据并核对数据，包括销售单，进货单，付款记录等等。最开始员工只有一人，随着业务扩展员工变成三人。 老板：不关心系统如何运行的，只关心数据大盘，包括客户欠款报表，月销售报表，利润表。 时间进度 2019年12月底项目启动，和老板讨论可行性，和员工讨论业务需求。 2020年3月初，系统上线第一版并开始试运行，只有销售模块。 2020年5月底，系统上线财务、员工、进货模块，销售模块相对成熟。 2021年2月底，系统上线库存模块，进货模块大改动。 2021年7月初，系统更新财务模块，新增银行卡概念，细分开销费用类别。 2022年1月底，系统上线生产模块和操作记录，记录每日生产活动，员工系统操作记录。业务流程建模经历以销售模块的业务流程建模为例，站在开发者视角，谈谈我对公司销售流程建模的发展过程。第一版最开始的我设计的销售模块功能很简单，可以在系统上录入销售单，然后可以打印纸质单据送货。这是当时最主要的需求当时的业务流程建模如下图所示（但实际上公司的需求远不止这莫简单，他们的销售流程比这复杂的多，只是当时的我不理解）第二版随着系统使用，我开始意识到销售单是需要做状态管理的，销售单有新建，已送货，未付款，已付款，归档等状态。因此在系统开发的第二版中，增加了财务模块，支持针对每一张订单进行付款，生成一条付款记录（付款证明）。付款确认后订单会变成归档状态。下图是当时的业务流程模型第三版很快用户就反映，订单许多时候并不是按单付款的。对于许多批发客户，是每一季度或者某一个周期进行付款，一次性结清多张订单。系统目前是认为一张订单一条付款记录，这样与实际情况不同，而且会导致公司对账的时候弄不清楚。总结而言，就是客户有两大类付款方式，一类是下单立即付款，每单结清，针对零售客户。另一类是按期付款，针对批发客户，针对这类客户往往还需要开对账单和开发票。因此新的业务建模如下，客户付款的时候将根据实际业务发生情况生成收款记录。第四版第三版业务建模满足公司需求，稳定了大概一年左右。随着公司客户的增多，业务量变大，公司开始反映会出现纸质送货单与系统记录对不上的情况。系统上存在某些销售单，但实际却找不到纸质单据。公司送货主要有两类情况，一类是客户自己到公司仓库提货，当场确认签字。第二类是物流送货，需要委托货运司机将货物送到客户工地，然后客户签字后，由货运司机将签字的送货单带回。经过排查发现，单据缺失的情况主要出现在第二类情况因此公司要求对每一张销售单，都要增加确认收到回执的过程。回执是指经过客户签字确认的送货单，代表客户确认收货。新的业务建模如下前后系统对比第一版：仅支持录入客户和产品名，可以开出销售单。系统目前版本销售单支持状态管理，利润计算，销售报表，检测异常订单，记录操作日志，对接库存模块和财务模块。" }, { "title": "白盒AES-tutorial 论文阅读笔记", "url": "/posts/%E7%99%BD%E7%9B%92tutorial%E7%AC%94%E8%AE%B0/", "categories": "白盒密码学", "tags": "白盒密码学", "date": "2022-03-21 00:00:00 +0800", "snippet": "论文原文链接 A Tutorial on White-box AES总体思路Chow2002年提出的白盒AES 基本思路是将原来AES的每一轮变换替换为查找表。AES过程白盒AES过程查找表理解什么是查找表，查找表（Lookup Table）另一个理解就是真值表，比如下图。查找表的构建 枚举所有输入 计算出输入对应的输出 将（输入，输出）项插入查找表查找表的使用 遍历查找表，找到输入相同的项 读取（输入，输出）项，返回输出实际使用的时候可以优化速度，以数组index作为索引，查询复杂度为O(1)具体看AES每个过程ShiftRowsshiftRows操作是将AES输入态矩阵进行移位操作。第一行不移动，第二行移动一列，第三行移动两列，第3行移动3列。这一步不需要转为查找表，只需要生成查找表取数的时候，从对应移位后的矩阵中取就可以了。shiftRows 本质就是每一轮加密的输入从不同地方取值。AddRoundkey与SubBytesAddRoundKey是对每一轮的输入与RoundKey做异或⊕操作。SubBytes操作本身就是一个8进8出的查找表，将一个字节替换为另一个字节。这两步可以合并到一个查找表中完成，称为T-boxes。T-box也是一个8进8出的查找表。AES每一轮的Key有16字节，每个字节与S-box合并做一个查找表，一轮16个。AES总共10轮，所以总共需要160个T-boxes。MixColumnsT_yi tablesAES的MixColumn作用是扩散混淆，具体操作就是把输入态16字节矩阵左乘MC矩阵。MC矩阵是固定的。矩阵乘法可以分开按每列单独计算，因为只有同一列的数据才会互相影响。输入态的16字节矩阵可以拆开为4列，每列是4字节。对于一列的矩阵乘法，进一步可以拆开成矩阵乘以常数，再异或相加，如下图：因此，可以把MixCloumn操作，变为一个8进32出的一个查找表，称为Tyi表，计算公式如下：之后将得到的4个Tyi表的结果做异或相加，可以得到最后的结果，完成一列4个字节的MixCloumn操作。这样对于每一轮16个字节的输入态，就有16个Tyi查找表，总共9轮，也就是144个查找表。每个查找表是8进32出。 这里有个疑问了，为什么不直接把整个MixCloumn操作做成查找表呢，为什么还要拆开按每行每列做一个查找表呢。 原因：整个MixCloumn作为查找表，这个查找表体积会非常大。具体而言，是一个128进128出的查找表，查找表有2^128行，每一行需要128位（16字节）空间。总占用空间为2^128 * 16 byte，要知道2^32byte约为1GB，所以查找表体积完全不可接受。换一个方面来想，AES加密本身就是一个128进128出的查找表呢。 采用拆开的方式，查找表是8进32出，每个查找表需要2^8 行，每行32位（4字节），体积为1KB。 共144个查找表，也就144KB。XOR Table每一列4个字节得到的4个Tyi表的需要进行异或后得到最终结果。异或操作也需要转换为查找表。这里的异或操作也需要拆开，原本不拆开是8个字节做异或，输出4字节。查找表是32进16出，体积也会过大。拆开为8个异或表，8进4出。异或操作理论上只需要一个查找表就够了，但因为后续章节的保护操作需要分开。每一列4字节需要做3次异或，也就需要3*8个查找表，一轮有16字节就需要3 * 8 * 4 = 96个查找表。9轮共864个查找表。每个查找表是2^8 * 0.5 = 128byte。表格合并对于T-box 8进8出，Tyi 表 8进32出，这两个表可以合并变为一个表，也是8进32出，空间减少，速度更快。这个表称为TboxTyiTable 那为什么异或表不能也一起合并呢？ 因为异或表的输入来自两方，如果合并，就是一个16进32出的表，大小为256kb一轮查找表的整体流程如下保护实现首先在上述查找表实现中，查找表的生成由具体的RoundKey决定，查找表的目的就是保护RoundKey。但看具体来看每个TboxTyiTable，其保护的是RoundKey的其中一个字节（RoundKey共16字节）。对于攻击者而言，由于一个查找表的随机性只由一字节决定，攻击者可以遍历所有可能情况（2^8次方，256种）来生成查找表，最后比对查找表就可以判断出RoundKey了。因此需要对查找表做保护实现，具体方法如下Encodings保护思想Encoding意识是编码，是对所有生成的查找表在输入输出都增加一个双射，以此把整个查找表打乱。假设R是T的下一个查找表（即T的输出作为R的输入），那么T的输出编码和R的输入编码是互逆的（互相抵消）。这样保证了计算结果不会因为编码而出错，同时又将查找表给编码保护了。由于攻击者而言，查找表的生成由随机的双射和key决定。因为对于同一个查找表（假设8进），可以由256种（双射，key）对生成，攻击者因此无法确定是哪一个（双射，key）对。这样达到了信息论安全。编码链接考虑到XOR表是从两个表的输出中各取4bit作为输入，因此要求编码可连接（concatenated encoding），具体如下具体实现的时候，是随机选取长度为4bit的双射，然后将这些双射进行拼接形成长度更长的双射（如8bit，32bit）。这样形成的双射天然满足编码可连接的要求。所有双射都是由长度为4bit的双射拼接而成，除了一个例外，外部编码（External encodings），这个编码可以自由选择，无需拼接，在后续第三节将介绍外部编码。Mixing Bijections为了扩散混淆，在所有Key相关的表中，都加上一层随机的双射变换（Mixing bijection）。以此使得只要key稍微变化，所有生成的查找表将完全不同。具体操作是，在TyiTboxTable前后都加上一个双射变换。TyiTboxTable是8进32出，所以在输入端加上一个8bit的逆双射-L，在输出端加上一个32bit的双射MB。类似于Encoding思想，逆双射-L用于抵消上一层L双射的影响，而逆双射-MB用于抵消本层MB的影响。逆双射的构造由矩阵进行拆分得到，具体如下加上Mixing Bijection后，一轮的操作如下 疑问：为什么需要Mixing Bijections 文章中给出的理由是：扩散混淆External encodings外部编码的目的就是使得在整个加解密运行的过程中，不会出现明文。思想很简单，就是对在输入前做一次编码，输出后再做一次解码。举例在一个知识版权软件音乐播放器，用户需要从服务器上下载付费音乐。传统做法是：服务器下载的音乐数据是加密的，然后用户客户端上运行的音乐播放器运行解密程序，对音乐进行播放。白盒做法是：服务器下载的音乐是经过加密以及外部编码的，用户客户端上运行的音乐播放器运行白盒解密，得到经过编码的音乐，硬件绑定的播放器对音乐解码并播放。在整个过程中，没有出现真正的明文（即音乐）。" }, { "title": "Github pages提升加载速度—配置cdn", "url": "/posts/GitHub-Pages%E6%8F%90%E5%8D%87%E5%8A%A0%E8%BD%BD%E9%80%9F%E5%BA%A6-%E9%85%8D%E7%BD%AECDN/", "categories": "小技巧", "tags": "github, 小技巧", "date": "2022-03-21 00:00:00 +0800", "snippet": "前置条件 绑定域名 网上说得很多，可以参考这一条知乎配置。 域名备案 使用CDN要求域名必须备案，这一步可能要3天左右。备案在阿里云备案。 下文的你的域名均指你申请的域名，而不是xxx.github.io，这个称为gitpages域名。配置阿里云CDN加速首先开通阿里云CDN，链接在此 阿里云CDN创建加速域名然后进入CDN控制台，点击创建我的CDN，具体配置如下图。加速域名根据你实际情况填写，如果你博客绑定在某个子域名上，就填你的子域名(如blog.tjfish.top)，像我域名下只有一个博客，所以我这里填写根域名tjfish.top点击新增源站源站信息 里选 源站域名域名设置成你对应的 XXXXXX.github.io（如我的tjfish.github.io)端口根据GitHub PagesEnforce HTTPS状态，如果勾中填 443端口，未勾中填 80端口。如下图：等待审核创建成功后会需要阿里云审核，大概5min配置DNS解析过一会儿你会获得一个CNAME。复制这个CNAME，去控制台-&amp;gt;域名下修改域名解析。记录类型填写CNAME，注意分清楚你是根域名还是子域名加速。可以按照阿里云给出的指导进行操作配置后，可以查询CANME记录已经生效回源配置 回源HOST 选择 加速域名 之后保存，可能需要几分钟时间同步，等一下就好了到这里就配置结束了，DNS生效大概需要10min才能看到效果。之后访问你的Github Page可以明显发现变快啦。可以ping一下自己的域名，我的延迟就10ms左右费用问题阿里云CDN按流量计费费用如下，每GB 0.24元，如果Github Page访问量不大，基本等于免费。" }, { "title": "Maven 提升下载速度——配置阿里源", "url": "/posts/maven%E6%8F%90%E5%8D%87%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6-%E6%9B%B4%E6%8D%A2%E5%9B%BD%E5%86%85%E6%BA%90/", "categories": "小技巧", "tags": "Maven, 小技巧", "date": "2022-03-19 00:00:00 +0800", "snippet": "Maven 配置打开 Maven 的配置文件(windows机器一般在maven安装目录的conf/settings.xml)，在&amp;lt;mirrors&amp;gt;&amp;lt;/mirrors&amp;gt;标签中添加 mirror 子节点:&amp;lt;mirror&amp;gt; &amp;lt;id&amp;gt;aliyunmaven&amp;lt;/id&amp;gt; &amp;lt;mirrorOf&amp;gt;*&amp;lt;/mirrorOf&amp;gt; &amp;lt;name&amp;gt;阿里云公共仓库&amp;lt;/name&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/public&amp;lt;/url&amp;gt;&amp;lt;/mirror&amp;gt;如果想使用其它代理仓库,可在&amp;lt;repositories&amp;gt;&amp;lt;/repositories&amp;gt;节点中加入对应的仓库使用地址。以使用spring代理仓为例：&amp;lt;repository&amp;gt; &amp;lt;id&amp;gt;spring&amp;lt;/id&amp;gt; &amp;lt;url&amp;gt;https://maven.aliyun.com/repository/spring&amp;lt;/url&amp;gt; &amp;lt;releases&amp;gt; &amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt; &amp;lt;/releases&amp;gt; &amp;lt;snapshots&amp;gt; &amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt; &amp;lt;/snapshots&amp;gt;&amp;lt;/repository&amp;gt;Gradle 配置在 build.gradle 文件中加入以下代码:allprojects { repositories { maven { url &#39;https://maven.aliyun.com/repository/public/&#39; } mavenLocal() mavenCentral() }}如果想使用 maven.aliyun.com 提供的其它代理仓，以使用 spring 仓为例，代码如下:allprojects { repositories { maven { url &#39;https://maven.aliyun.com/repository/public/&#39; } maven { url &#39;https://maven.aliyun.com/repository/spring/&#39;} mavenLocal() mavenCentral() }}" }, { "title": "ubuntu 提升下载速度—更换阿里源", "url": "/posts/ubuntu%E6%8F%90%E5%8D%87%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6-%E6%9B%B4%E6%8D%A2%E9%98%BF%E9%87%8C%E6%BA%90/", "categories": "小技巧", "tags": "ubuntu, 小技巧", "date": "2022-03-17 00:00:00 +0800", "snippet": "经常发现ubuntu 安装软件（apt install）贼慢，非常痛苦。本文介绍更换阿里源提升下载速度。ubuntu 更换国内源的方法查看系统版本lsb_release -c&amp;gt; Codename: focal #focal 说明是ubuntu20.04ubuntu版本名与系统名对应关系，使用错误的版本源会导致依赖缺失和版本不对。 系统名 版本名 ubuntu20.04 focal ubuntu18.04 bionic ubuntu16.04 xenial ubuntu14.04 trusty 首先备份原来的源sudo mv /etc/apt/sources.list /etc/apt/sources.list.bak再输入以下命令打开sources.list配置文件更换源，以Ubuntu20.04为例。sudo vim /etc/apt/sources.list//配置内容如下-阿里源 Ubuntu20.04deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse更新源sudo apt-get update再重新下载安装速度就很快啦。其他版本的源ubuntu 14.04 配置deb https://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty-security main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty-updates main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiversedeb-src https://mirrors.aliyun.com/ubuntu/ trusty-backports main restricted universe multiverseubuntu 16.04 配置deb http://mirrors.aliyun.com/ubuntu/ xenial maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial maindeb http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates maindeb http://mirrors.aliyun.com/ubuntu/ xenial universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial universedeb http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universedeb http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security maindeb http://mirrors.aliyun.com/ubuntu/ xenial-security universedeb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universeubuntu 18.04(bionic) 配置deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverseubuntu 20.04(focal) 配置deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse原理部分ubuntu默认的下载源是国外地址，国内下载非常慢。ubuntu下载源定义在 /etc/apt/sources.list 文件中，通过将下载源替换为阿里源，随后运行sudo apt-get update重新加载下载源，之后下载都默认从阿里云下载了，速度就非常快。" }, { "title": "Linux安装普罗米修斯监控系统", "url": "/posts/%E6%99%AE%E7%BD%97%E7%B1%B3%E4%BF%AE%E6%96%AF%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%E5%AE%89%E8%A3%85/", "categories": "小技巧", "tags": "工具, linux", "date": "2022-03-16 00:00:00 +0800", "snippet": "普罗米修斯监控系统简介普罗米修斯(Prometheus)是一个SoundCloud公司开源的监控系统，拥有强大的监控能力。普罗米修斯监控系统分两部分，客户端和服务端。客户端部署在被监控的服务器上，负责收集本机数据。服务端部署在其他机器上，记录客户端发送的监控数据，提供页面供管理员查看。可以部署多个客户端同时监控多台机器。如下图所示部署后最终效果客户端部署部署环境 centos 7.3 阿里云服务器 1核2G轻量应用服务器 部署node_exporter，用来监控机器数据，包括CPU，内存，TCP连接等数据下载安装包官方下载地址。 https://prometheus.io/download/wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz解压压缩包tar -zxvf node_exporter-1.3.1.linux-amd64.tar.gzcp -R node_exporter-1.3.1.linux-amd64 /usr/local/node_exporter注册node_exporter服务cd /usr/local/node_exportertouch /usr/lib/systemd/system/node_exporter.service vim /usr/lib/systemd/system/node_exporter.serv在node_exporter.service中加入如下代码:[Unit]Description=node_exporterAfter=network.target[Service]Type=simpleUser=rootExecStart=/usr/local/node_exporter/node_exporterRestart=on-failure[Install]WantedBy=multi-user.target启动 node_exporter 服务并设置开机启动systemctl daemon-reloadsystemctl enable node_exporter.servicesystemctl start node_exporter.service# 查看状态systemctl status node_exporter.service注意：记得防火墙打开9100的端口。检测端口9100是否监听，监听则客户端node_exporter启动成功，node_exporter启动成功后, 你就可以通过如下api看到你的监控数据了 http://localhost:9100/metrics。服务端部署下载并解压wget https://github.com/prometheus/prometheus/releases/download/v2.33.5/prometheus-2.33.5.linux-amd64.tar.gz由于Prometheus是go语言写的，所以不需要编译，安装的过程非常简单，仅需要解压然后运行。tar -zxvf prometheus-2.33.5.linux-amd64.tar.gz cp -R prometheus-2.33.5.linux-amd64 /usr/local/prometheus修改配置来监控客户端 这里需要特别注意，一定要严格注意间距和格式，建议参考范例所示的格式，我在第一次部署的时候，曾因为少了一个空格导致启动失败。cd /usr/local/prometheusvim prometheus.yml#在最后面增加如下配置- job_name: &#39;nodes&#39; static_configs: - targets: [&#39;客户端ip:9100&#39;]设置prometheus系统服务,并配置开机启动touch /usr/lib/systemd/system/prometheus.servicevim /usr/lib/systemd/system/prometheus.service将如下配置写入prometheus.servie[Unit]Description=PrometheusDocumentation=https://prometheus.io/After=network.target[Service]Type=simpleUser=root# --storage.tsdb.path是可选项，默认数据目录在运行目录的./dada目录中ExecStart=/usr/local/prometheus/prometheus --config.file=/usr/local/prometheus/prometheus.yml --web.enable-lifecycle --storage.tsdb.path=/usr/local/prometheus/data --storage.tsdb.retention=60dRestart=on-failure[Install]WantedBy=multi-user.targetPrometheus启动参数说明 config.file – 指明prometheus的配置文件路径 web.enable-lifecycle – 指明prometheus配置更改后可以进行热加载 storage.tsdb.path – 指明监控数据存储路径 storage.tsdb.retention –指明数据保留时间启动Prometheussystemctl daemon-reload# 设置开机启动systemctl enable prometheus.service# 启动服务systemctl start prometheus.servicesystemctl status prometheus.serviceGrafana安装配置Grafana安装安装Grafana用于可视化普罗米修斯的监控数据下载地址: https://grafana.com/grafana/downloadwget https://dl.grafana.com/enterprise/release/grafana-enterprise-8.4.3-1.x86_64.rpmsudo yum install grafana-enterprise-8.4.3-1.x86_64.rpmsystemctl start grafana-serversystemctl status grafana-server配置监控Grafana默认端口为3000，输入http://localhost:3000/访问，记得打开3000端口的防火墙设置。granafa首次登录账户名和密码admin/admin，第一次登录需要修改密码配置数据源Data sources-&amp;gt;Add data source -&amp;gt; Prometheus，输入prometheus数据源的信息，主要是输入name和url。由于我是Grafana 和Prometheus安装在同一个机器上，所以我的url如下http://localhost:9090配置Dashboard模板Grafana官方提供模板地址： https://grafana.com/grafana/dashboards本次要导入的模板： https://grafana.com/grafana/dashboards/11074输入ID：11074监控tomcat数据客户端安装jmx_exporterPrometheus监控Tomcat需要用到jmx_exporter。jmx_exporter项目地址：https://github.com/prometheus/jmx_exporterwget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.16.1/jmx_prometheus_javaagent-0.16.1.jarmkdir -p /usr/local/jmx/cp jmx_prometheus_javaagent-0.16.1.jar /usr/local/jmx/修改配置，在config.yaml下添加如下内容#修改配置touch /usr/local/jmx/config.yamlvim /usr/local/jmx/config.yaml#添加如下内容： lowercaseOutputLabelNames: truelowercaseOutputName: truerules:- pattern: &#39;Catalina&amp;lt;type=GlobalRequestProcessor, name=\\&quot;(\\w+-\\w+)-(\\d+)\\&quot;&amp;gt;&amp;lt;&amp;gt;(\\w+):&#39; name: tomcat_$3_total labels: port: &quot;$2&quot; protocol: &quot;$1&quot; help: Tomcat global $3 type: COUNTER- pattern: &#39;Catalina&amp;lt;j2eeType=Servlet, WebModule=//([-a-zA-Z0-9+&amp;amp;@#/%?=~_|!:.,;]*[-a-zA-Z0-9+&amp;amp;@#/%=~_|]), name=([-a-zA-Z0-9+/$%~_-|!.]*), J2EEApplication=none, J2EEServer=none&amp;gt;&amp;lt;&amp;gt;(requestCount|maxTime|processingTime|errorCount):&#39; name: tomcat_servlet_$3_total labels: module: &quot;$1&quot; servlet: &quot;$2&quot; help: Tomcat servlet $3 total type: COUNTER- pattern: &#39;Catalina&amp;lt;type=ThreadPool, name=&quot;(\\w+-\\w+)-(\\d+)&quot;&amp;gt;&amp;lt;&amp;gt;(currentThreadCount|currentThreadsBusy|keepAliveCount|pollerThreadCount|connectionCount):&#39; name: tomcat_threadpool_$3 labels: port: &quot;$2&quot; protocol: &quot;$1&quot; help: Tomcat threadpool $3 type: GAUGE- pattern: &#39;Catalina&amp;lt;type=Manager, host=([-a-zA-Z0-9+&amp;amp;@#/%?=~_|!:.,;]*[-a-zA-Z0-9+&amp;amp;@#/%=~_|]), context=([-a-zA-Z0-9+/$%~_-|!.]*)&amp;gt;&amp;lt;&amp;gt;(processingTime|sessionCounter|rejectedSessions|expiredSessions):&#39; name: tomcat_session_$3_total labels: context: &quot;$2&quot; host: &quot;$1&quot; help: Tomcat session $3 total type: COUNTER- pattern: &quot;.*&quot; #让所有的jmx metrics全部暴露出来修改tomcat配置，让监控和tomcat程序一起启动 # 注意路径改为你tomcat所在位置 vim /usr/local/tomcat/bin/catalina.sh # 添加以下内容，注意jar包的版本改为你下载的版本 JAVA_OPTS=&quot;$JAVA_OPTS -javaagent:/usr/local/jmx/jmx_prometheus_javaagent-0.16.1.jar=30013:/usr/local/jmx/config.yaml&quot;重启tomcatsystemctl restart tomcat访问地址，记得开启防火墙端口30013，查看监控数据：http://localhost:30013/服务端集成Prometheusvim /usr/local/prometheus/prometheus.yml#添加如下内容: - job_name: &#39;tomcat&#39; static_configs: - targets: [&#39;客户端ip:30013&#39;]重启Prometheussystemctl restart prometheusJMX Exporter 在 Grafana 上为我们提供好了 Dashboard 模板：https://grafana.com/dashboards/导入8563模板，job填写tomcat就可以使用了。" }, { "title": "殷浩详解DDD系列 第五讲 - 聊聊如何避免写流水账代码", "url": "/posts/%E6%AE%B7%E6%B5%A9%E8%AF%A6%E8%A7%A3DDD%E7%B3%BB%E5%88%97-%E7%AC%AC%E4%BA%94%E8%AE%B2-%E8%81%8A%E8%81%8A%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%86%99%E6%B5%81%E6%B0%B4%E8%B4%A6%E4%BB%A3%E7%A0%81/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-03-15 00:00:00 +0800", "snippet": "简介： 在过去一年里我们团队做了大量的老系统重构和迁移，其中有大量的代码属于流水账代码，通常能看到是开发在对外的API接口里直接写业务逻辑代码，或者在一个服务里大量的堆接口，导致业务逻辑实际无法收敛，接口复用性比较差。所以这讲主要想系统性的解释一下如何通过DDD的重构，将原有的流水账代码改造为逻辑清晰、职责分明的模块。1. 案例简介这里举一个简单的常见案例：下单链路。假设我们在做一个checkout接口，需要做各种校验、查询商品信息、调用库存服务扣库存、然后生成订单：一个比较典型的代码如下：@RestController@RequestMapping(&quot;/&quot;)public class CheckoutController { @Resource private ItemService itemService; @Resource private InventoryService inventoryService; @Resource private OrderRepository orderRepository; @PostMapping(&quot;checkout&quot;) public Result&amp;lt;OrderDO&amp;gt; checkout(Long itemId, Integer quantity) { // 1) Session管理 Long userId = SessionUtils.getLoggedInUserId(); if (userId &amp;lt;= 0) { return Result.fail(&quot;Not Logged In&quot;); } // 2）参数校验 if (itemId &amp;lt;= 0 || quantity &amp;lt;= 0 || quantity &amp;gt;= 1000) { return Result.fail(&quot;Invalid Args&quot;); } // 3）外部数据补全 ItemDO item = itemService.getItem(itemId); if (item == null) { return Result.fail(&quot;Item Not Found&quot;); } // 4）调用外部服务 boolean withholdSuccess = inventoryService.withhold(itemId, quantity); if (!withholdSuccess) { return Result.fail(&quot;Inventory not enough&quot;); } // 5）领域计算 Long cost = item.getPriceInCents() * quantity; // 6）领域对象操作 OrderDO order = new OrderDO(); order.setItemId(itemId); order.setBuyerId(userId); order.setSellerId(item.getSellerId()); order.setCount(quantity); order.setTotalCost(cost); // 7）数据持久化 orderRepository.createOrder(order); // 8）返回 return Result.success(order); }}为什么这种典型的流水账代码在实际应用中会有问题呢？其本质问题是违背了SRP（Single Responsbility Principle）单一职责原则。这段代码里混杂了业务计算、校验逻辑、基础设施、和通信协议等，在未来无论哪一部分的逻辑变更都会直接影响到这段代码，长期当后人不断的在上面叠加新的逻辑时，会造成代码复杂度增加、逻辑分支越来越多，最终造成bug或者没人敢重构的历史包袱。所以我们才需要用DDD的分层思想去重构一下以上的代码，通过不同的代码分层和规范，拆分出逻辑清晰，职责明确的分层和模块，也便于一些通用能力的沉淀。主要的几个步骤分为： 分离出独立的Interface接口层，负责处理网络协议相关的逻辑 从真实业务场景中，找出具体用例（Use Cases），然后将具体用例通过专用的Command指令、Query查询、和Event事件对象来承接 分离出独立的Application应用层，负责业务流程的编排，响应Command、Query和Event。每个应用层的方法应该代表整个业务流程中的一个节点 处理一些跨层的横切关注点，如鉴权、异常处理、校验、缓存、日志等下面会针对每个点做详细的解释。2. Interface接口层随着REST和MVC架构的普及，经常能看到开发同学直接在Controller中写业务逻辑，如上面的典型案例，但实际上MVC Controller不是唯一的重灾区。以下的几种常见的代码写法通常都可能包含了同样的问题： HTTP 框架：如Spring MVC框架，Spring Cloud等 RPC 框架：如Dubbo、HSF、gRPC等 消息队列MQ的“消费者”：比如JMS的 onMessage，RocketMQ的MessageListener等 Socket通信：Socket通信的receive、WebSocket的onMessage等 文件系统：WatcherService等 分布式任务调度：SchedulerX等这些的方法都有一个共同的点就是都有自己的网络协议，而如果我们的业务代码和网络协议混杂在一起，则会直接导致代码跟网络协议绑定，无法被复用。所以，在DDD的分层架构中，我们单独会抽取出来Interface接口层，作为所有对外的门户，将网络协议和业务逻辑解耦。2.1 接口层的组成接口层主要由以下几个功能组成： 网络协议的转化：通常这个已经由各种框架给封装掉了，我们需要构建的类要么是被注解的bean，要么是继承了某个接口的bean。 统一鉴权：比如在一些需要AppKey+Secret的场景，需要针对某个租户做鉴权的，包括一些加密串的校验 Session管理：一般在面向用户的接口或者有登陆态的，通过Session或者RPC上下文可以拿到当前调用的用户，以便传递给下游服务。 限流配置：对接口做限流避免大流量打到下游服务 前置缓存：针对变更不是很频繁的只读场景，可以前置结果缓存到接口层 异常处理：通常在接口层要避免将异常直接暴露给调用端，所以需要在接口层做统一的异常捕获，转化为调用端可以理解的数据格式 日志：在接口层打调用日志，用来做统计和debug等。一般微服务框架可能都直接包含了这些功能。当然，如果有一个独立的网关设施/应用，则可以抽离出鉴权、Session、限流、日志等逻辑，但是目前来看API网关也只能解决一部分的功能，即使在有API网关的场景下，应用里独立的接口层还是有必要的。在interface层，鉴权、Session、限流、缓存、日志等都比较直接，只有一个异常处理的点需要重点说下。2.2 返回值和异常处理规范，Result vs Exception注：这部分主要还是面向REST和RPC接口，其他的协议需要根据协议的规范产生返回值。在我见过的一些代码里，接口的返回值比较多样化，有些直接返回DTO甚至DO，另一些返回Result。接口层的核心价值是对外，所以如果只是返回DTO或DO会不可避免的面临异常和错误栈泄漏到使用方的情况，包括错误栈被序列化反序列化的消耗。所以，这里提出一个规范： 规范：Interface层的HTTP和RPC接口，返回值为Result，捕捉所有异常 规范：Application层的所有接口返回值为DTO，不负责处理异常Application层的具体规范等下再讲，在这里先展示Interface层的逻辑。举个例子：@PostMapping(&quot;checkout&quot;)public Result&amp;lt;OrderDTO&amp;gt; checkout(Long itemId, Integer quantity) { try { CheckoutCommand cmd = new CheckoutCommand(); OrderDTO orderDTO = checkoutService.checkout(cmd); return Result.success(orderDTO); } catch (ConstraintViolationException cve) { // 捕捉一些特殊异常，比如Validation异常 return Result.fail(cve.getMessage()); } catch (Exception e) { // 兜底异常捕获 return Result.fail(e.getMessage()); }}当然，每个接口都要写异常处理逻辑会比较烦，所以可以用AOP做个注解@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface ResultHandler {}@Aspect@Componentpublic class ResultAspect { @Around(&quot;@annotation(ResultHandler)&quot;) public Object logExecutionTime(ProceedingJoinPoint joinPoint) throws Throwable { Object proceed = null; try { proceed = joinPoint.proceed(); } catch (ConstraintViolationException cve) { return Result.fail(cve.getMessage()); } catch (Exception e) { return Result.fail(e.getMessage()); } return proceed; }}然后最终代码则简化为：@PostMapping(&quot;checkout&quot;)@ResultHandlerpublic Result&amp;lt;OrderDTO&amp;gt; checkout(Long itemId, Integer quantity) { CheckoutCommand cmd = new CheckoutCommand(); OrderDTO orderDTO = checkoutService.checkout(cmd); return Result.success(orderDTO);}2.3 接口层的接口的数量和业务间的隔离在传统REST和RPC的接口规范中，通常一个领域的接口，无论是REST的Resource资源的GET/POST/DELETE，还是RPC的方法，是追求相对固定的，统一的，而且会追求统一个领域的方法放在一个领域的服务或Controller中。但是我发现在实际做业务的过程中，特别是当支撑的上游业务比较多时，刻意去追求接口的统一通常会导致方法中的参数膨胀，或者导致方法的膨胀。举个例子：假设有一个宠物卡和一个亲子卡的业务公用一个开卡服务，但是宠物需要传入宠物类型，亲子的需要传入宝宝年龄。// 可以是RPC Provider 或者 Controllerpublic interface CardService { // 1）统一接口，参数膨胀 Result openCard(int petType, int babyAge); // 2）统一泛化接口，参数语意丢失 Result openCardV2(Map&amp;lt;String, Object&amp;gt; params); // 3）不泛化，同一个类里的接口膨胀 Result openPetCard(int petType); Result openBabyCard(int babyAge);}可以看出来，无论是怎么操作，都有可能导致CardService这个服务未来越来越难以维护，方法越来越多，一个业务的变更有可能会导致整个服务/Controller的变更，最终变得无法维护。我曾经参与过的一个服务，提供了几十个方法，上万行代码，可想而知无论是使用方对接口的理解成本还是对代码的维护成本都是极高的。所以，这里提出另一个规范： 规范：一个Interface层的类应该是“小而美”的，应该是面向“一个单一的业务”或“一类同样需求的业务”，需要尽量避免用同一个类承接不同类型业务的需求。基于上面的这个规范，可以发现宠物卡和亲子卡虽然看起来像是类似的需求，但并非是“同样需求”的，可以预见到在未来的某个时刻，这两个业务的需求和需要提供的接口会越走越远，所以需要将这两个接口类拆分开：public interface PetCardService { Result openPetCard(int petType);}public interface BabyCardService { Result openBabyCard(int babyAge);}这个的好处是符合了Single Responsibility Principle单一职责原则，也就是说一个接口类仅仅会因为一个（或一类）业务的变化而变化。一个建议是当一个现有的接口类过度膨胀时，可以考虑对接口类做拆分，拆分原则和SRP一致。也许会有人问，如果按照这种做法，会不会产生大量的接口类，导致代码逻辑重复？答案是不会，因为在DDD分层架构里，接口类的核心作用仅仅是协议层，每类业务的协议可以是不同的，而真实的业务逻辑会沉淀到应用层。也就是说Interface和Application的关系是多对多的：因为业务需求是快速变化的，所以接口层也要跟着快速变化，通过独立的接口层可以避免业务间相互影响，但我们希望相对稳定的是Application层的逻辑。所以我们接下来看一下Application层的一些规范。3. Application层3.1 Application层的组成部分Application层的几个核心类： ApplicationService应用服务：最核心的类，负责业务流程的编排，但本身不负责任何业务逻辑 DTO Assembler：负责将内部领域模型转化为可对外的DTO Command、Query、Event对象：作为ApplicationService的入参 返回的DTO：作为ApplicationService的出参Application层最核心的对象是ApplicationService，它的核心功能是承接“业务流程“。但是在讲ApplicationService的规范之前，必须要先重点的讲几个特殊类型的对象，即Command、Query和Event。3.2 Command、Query、Event对象从本质上来看，这几种对象都是Value Object，但是从语义上来看有比较大的差异： Command指令：指调用方明确想让系统操作的指令，其预期是对一个系统有影响，也就是写操作。通常来讲指令需要有一个明确的返回值（如同步的操作结果，或异步的指令已经被接受）。 Query查询：指调用方明确想查询的东西，包括查询参数、过滤、分页等条件，其预期是对一个系统的数据完全不影响的，也就是只读操作。 Event事件：指一件已经发生过的既有事实，需要系统根据这个事实作出改变或者响应的，通常事件处理都会有一定的写操作。事件处理器不会有返回值。这里需要注意一下的是，Application层的Event概念和Domain层的DomainEvent是类似的概念，但不一定是同一回事，这里的Event更多是外部一种通知机制而已。简单总结下：   Command Query Event 语意 ”希望“能触发的操作 各种条件的查询 已经发生过的事情 读/写 写 只读 通常是写 返回值 DTO 或 Boolean DTO 或 Collection Void 为什么要用CQE对象？通常在很多代码里，能看到接口上有多个参数，比如上文中的案例：Result&amp;lt;OrderDO&amp;gt; checkout(Long itemId, Integer quantity);如果需要在接口上增加参数，考虑到向前兼容，则需要增加一个方法：Result&amp;lt;OrderDO&amp;gt; checkout(Long itemId, Integer quantity);Result&amp;lt;OrderDO&amp;gt; checkout(Long itemId, Integer quantity, Integer channel);或者常见的查询方法，由于条件的不同导致多个方法：List&amp;lt;OrderDO&amp;gt; queryByItemId(Long itemId);List&amp;lt;OrderDO&amp;gt; queryBySellerId(Long sellerId);List&amp;lt;OrderDO&amp;gt; queryBySellerIdWithPage(Long sellerId, int currentPage, int pageSize);可以看出来，传统的接口写法有几个问题： 接口膨胀：一个查询条件一个方法 难以扩展：每新增一个参数都有可能需要调用方升级 难以测试：接口一多，职责随之变得繁杂，业务场景各异，测试用例难以维护但是另外一个最重要的问题是：这种类型的参数罗列，本身没有任何业务上的”语意“，只是一堆参数而已，无法明确的表达出来意图。CQE的规范：所以在Application层的接口里，强力建议的一个规范是： 规范：ApplicationService的接口入参只能是一个Command、Query或Event对象，CQE对象需要能代表当前方法的语意。唯一可以的例外是根据单一ID查询的情况，可以省略掉一个Query对象的创建按照上面的规范，实现案例是：public interface CheckoutService { OrderDTO checkout(@Valid CheckoutCommand cmd); List&amp;lt;OrderDTO&amp;gt; query(OrderQuery query); OrderDTO getOrder(Long orderId); // 注意单一ID查询可以不用Query}@Datapublic class CheckoutCommand { private Long userId; private Long itemId; private Integer quantity;}@Datapublic class OrderQuery { private Long sellerId; private Long itemId; private int currentPage; private int pageSize;}这个规范的好处是：提升了接口的稳定性、降低低级的重复，并且让接口入参更加语意化。CQE vs DTO从上面的代码能看出来，ApplicationService的入参是CQE对象，但是出参却是一个DTO，从代码格式上来看都是简单的POJO对象，那么他们之间有什么区别呢？ CQE：CQE对象是ApplicationService的输入，是有明确的”意图“的，所以这个对象必须保证其”正确性“。 DTO：DTO对象只是数据容器，只是为了和外部交互，所以本身不包含任何逻辑，只是贫血对象。但可能最重要的一点：因为CQE是”意图“，所以CQE对象在理论上可以有”无限“个，每个代表不同的意图；但是DTO作为模型数据容器，和模型一一对应，所以是有限的。CQE的校验CQE作为ApplicationService的输入，必须保证其正确性，那么这个校验是放在哪里呢？在最早的代码里，曾经有这样的校验逻辑，当时写在了服务里：if (itemId &amp;lt;= 0 || quantity &amp;lt;= 0 || quantity &amp;gt;= 1000) { return Result.fail(&quot;Invalid Args&quot;);}这种代码在日常非常常见，但其最大的问题就是大量的非业务代码混杂在业务代码中，很明显的违背了单一职责原则。但因为当时入参仅仅是简单的int，所以这个逻辑只能出现在服务里。现在当入参改为了CQE之后，我们可以利用java标准JSR303或JSR380的Bean Validation来前置这个校验逻辑。 规范：CQE对象的校验应该前置，避免在ApplicationService里做参数的校验。可以通过JSR303/380和Spring Validation来实现前面的例子可以改造为：@Validated // Spring的注解public class CheckoutServiceImpl implements CheckoutService { OrderDTO checkout(@Valid CheckoutCommand cmd) { // 这里@Valid是JSR-303/380的注解 // 如果校验失败会抛异常，在interface层被捕捉 }}@Datapublic class CheckoutCommand { @NotNull(message = &quot;用户未登陆&quot;) private Long userId; @NotNull @Positive(message = &quot;需要是合法的itemId&quot;) private Long itemId; @NotNull @Min(value = 1, message = &quot;最少1件&quot;) @Max(value = 1000, message = &quot;最多不能超过1000件&quot;) private Integer quantity;}这种做法的好处是，让ApplicationService更加清爽，同时各种错误信息可以通过Bean Validation的API做各种个性化定制。避免复用CQE因为CQE是有“意图”和“语意”的，我们需要尽量避免CQE对象的复用，哪怕所有的参数都一样，只要他们的语意不同，尽量还是要用不同的对象。 规范：针对于不同语意的指令，要避免CQE对象的复用❌ 反例：一个常见的场景是“Create创建”和“Update更新”，一般来说这两种类型的对象唯一的区别是一个ID，创建没有ID，而更新则有。所以经常能看见有的同学用同一个对象来作为两个方法的入参，唯一区别是ID是否赋值。这个是错误的用法，因为这两个操作的语意完全不一样，他们的校验条件可能也完全不一样，所以不应该复用同一个对象。正确的做法是产出两个对象：public interface CheckoutService { OrderDTO checkout(@Valid CheckoutCommand cmd); OrderDTO updateOrder(@Valid UpdateOrderCommand cmd);}@Datapublic class UpdateOrderCommand { @NotNull(message = &quot;用户未登陆&quot;) private Long userId; @NotNull(message = &quot;必须要有OrderID&quot;) private Long orderId; @NotNull @Positive(message = &quot;需要是合法的itemId&quot;) private Long itemId; @NotNull @Min(value = 1, message = &quot;最少1件&quot;) @Max(value = 1000, message = &quot;最多不能超过1000件&quot;) private Integer quantity;}3.3 ApplicationServiceApplicationService负责了业务流程的编排，是将原有业务流水账代码剥离了校验逻辑、领域计算、持久化等逻辑之后剩余的流程，是“胶水层”代码。参考一个简易的交易流程：在这个案例里可以看出来，交易这个领域一共有5个用例：下单、支付成功、支付失败关单、物流信息更新、关闭订单。这5个用例可以用5个Command/Event对象代替，也就是对应了5个方法。我见过3种ApplicationService的组织形态：1、一个ApplicationService类是一个完整的业务流程，其中每个方法负责处理一个Use Case。这种的好处是可以完整的收敛整个业务逻辑，从接口类即可对业务逻辑有一定的掌握，适合相对简单的业务流程。坏处就是对于复杂的业务流程会导致一个类的方法过多，有可能代码量过大。这种类型的具体案例如：public interface CheckoutService { // 下单 OrderDTO checkout(@Valid CheckoutCommand cmd); // 支付成功 OrderDTO payReceived(@Valid PaymentReceivedEvent event); // 支付取消 OrderDTO payCanceled(@Valid PaymentCanceledEvent event); // 发货 OrderDTO packageSent(@Valid PackageSentEvent event); // 收货 OrderDTO delivered(@Valid DeliveredEvent event); // 批量查询 List&amp;lt;OrderDTO&amp;gt; query(OrderQuery query); // 单个查询 OrderDTO getOrder(Long orderId);}2、针对于比较复杂的业务流程，可以通过增加独立的CommandHandler、EventHandler来降低一个类中的代码量：@Componentpublic class CheckoutCommandHandler implements CommandHandler&amp;lt;CheckoutCommand, OrderDTO&amp;gt; { @Override public OrderDTO handle(CheckoutCommand cmd) { // }}public class CheckoutServiceImpl implements CheckoutService { @Resource private CheckoutCommandHandler checkoutCommandHandler; @Override public OrderDTO checkout(@Valid CheckoutCommand cmd) { return checkoutCommandHandler.handle(cmd); }}3、比较激进一点，通过CommandBus、EventBus，直接将指令或事件抛给对应的Handler，EventBus比较常见。具体案例代码如下，通过消息队列收到MQ消息后，生成Event，然后由EventBus做路由到对应的Handler：// Application层// 在这里框架通常可以根据接口识别到这个负责处理PaymentReceivedEvent// 也可以通过增加注解识别@Componentpublic class PaymentReceivedHandler implements EventHandler&amp;lt;PaymentReceivedEvent&amp;gt; { @Override public void process(PaymentReceivedEvent event) { // }}// Interface层，这个是RocketMQ的Listenerpublic class OrderMessageListener implements MessageListenerOrderly { @Resource private EventBus eventBus; @Override public ConsumeOrderlyStatus consumeMessage(List&amp;lt;MessageExt&amp;gt; msgs, ConsumeOrderlyContext context) { PaymentReceivedEvent event = new PaymentReceivedEvent(); eventBus.dispatch(event); // 不需要指定消费者 return ConsumeOrderlyStatus.SUCCESS; }}⚠️ 不建议：这种做法可以实现Interface层和某个具体的ApplicationService或Handler的完全静态解藕，在运行时动态dispatch，做的比较好的框架如AxonFramework。虽然看起来很便利，但是根据我们自己业务的实践和踩坑发现，当代码中的CQE对象越来越多，handler越来越复杂时，运行时的dispatch缺乏了静态代码间的关联关系，导致代码很难读懂，特别是当你需要trace一个复杂调用链路时，因为dispatch是运行时的，很难摸清楚具体调用到的对象。所以我们虽然曾经有过这种尝试，但现在已经不建议这么做了。Application Service 是业务流程的封装，不处理业务逻辑虽然之前曾经无数次重复ApplicationService只负责业务流程串联，不负责业务逻辑，但如何判断一段代码到底是业务流程还是逻辑呢？举个之前的例子，最初的代码重构后：@Service@Validatedpublic class CheckoutServiceImpl implements CheckoutService { private final OrderDtoAssembler orderDtoAssembler = OrderDtoAssembler.INSTANCE; @Resource private ItemService itemService; @Resource private InventoryService inventoryService; @Resource private OrderRepository orderRepository; @Override public OrderDTO checkout(@Valid CheckoutCommand cmd) { ItemDO item = itemService.getItem(cmd.getItemId()); if (item == null) { throw new IllegalArgumentException(&quot;Item not found&quot;); } boolean withholdSuccess = inventoryService.withhold(cmd.getItemId(), cmd.getQuantity()); if (!withholdSuccess) { throw new IllegalArgumentException(&quot;Inventory not enough&quot;); } Order order = new Order(); order.setBuyerId(cmd.getUserId()); order.setSellerId(item.getSellerId()); order.setItemId(item.getItemId()); order.setItemTitle(item.getTitle()); order.setItemUnitPrice(item.getPriceInCents()); order.setCount(cmd.getQuantity()); Order savedOrder = orderRepository.save(order); return orderDtoAssembler.orderToDTO(savedOrder); }}判断是否业务流程的几个点：1、不要有if/else分支逻辑：也就是说代码的Cyclomatic Complexity（循环复杂度）应该尽量等于1通常有分支逻辑的，都代表一些业务判断，应该将逻辑封装到DomainService或者Entity里。但这不代表完全不能有if逻辑，比如，在这段代码里：boolean withholdSuccess = inventoryService.withhold(cmd.getItemId(), cmd.getQuantity());if (!withholdSuccess) { throw new IllegalArgumentException(&quot;Inventory not enough&quot;);}虽然CC &amp;gt; 1，但是仅仅代表了中断条件，具体的业务逻辑处理并没有受影响。可以把它看作为Precondition。2、不要有任何计算：在最早的代码里有这个计算：// 5）领域计算Long cost = item.getPriceInCents() * quantity;order.setTotalCost(cost);通过将这个计算逻辑封装到实体里，避免在ApplicationService里做计算@Datapublic class Order { private Long itemUnitPrice; private Integer count; // 把原来一个在ApplicationService的计算迁移到Entity里 public Long getTotalCost() { return itemUnitPrice * count; }}order.setItemUnitPrice(item.getPriceInCents());order.setCount(cmd.getQuantity());3、一些数据的转化可以交给其他对象来做：比如DTO Assembler，将对象间转化的逻辑沉淀在单独的类中，降低ApplicationService的复杂度OrderDTO dto = orderDtoAssembler.orderToDTO(savedOrder);常用的ApplicationService“套路”我们可以看出来，ApplicationService的代码通常有类似的结构：AppService通常不做任何决策（Precondition除外），仅仅是把所有决策交给DomainService或Entity，把跟外部交互的交给Infrastructure接口，如Repository或防腐层。一般的“套路”如下： 准备数据：包括从外部服务或持久化源取出相对应的Entity、VO以及外部服务返回的DTO。 执行操作：包括新对象的创建、赋值，以及调用领域对象的方法对其进行操作。需要注意的是这个时候通常都是纯内存操作，非持久化。 持久化：将操作结果持久化，或操作外部系统产生相应的影响，包括发消息等异步操作。如果涉及到对多个外部系统（包括自身的DB）都有变更的情况，这个时候通常处在“分布式事务”的场景里，无论是用分布式TX、TCC、还是Saga模式，取决于具体场景的设计，在此处暂时略过。3.4 DTO Assembler一个经常被忽视的问题是 ApplicationService应该返回 Entity 还是 DTO？这里提出一个规范，在DDD分层架构中： ApplicationService应该永远返回DTO而不是Entity为什么呢？ 构建领域边界：ApplicationService的入参是CQE对象，出参是DTO，这些基本上都属于简单的POJO，来确保Application层的内外互相不影响。 降低规则依赖：Entity里面通常会包含业务规则，如果ApplicationService返回Entity，则会导致调用方直接依赖业务规则。如果内部规则变更可能直接影响到外部。 通过DTO组合降低成本：Entity是有限的，DTO可以是多个Entity、VO的自由组合，一次性封装成复杂DTO，或者有选择的抽取部分参数封装成DTO可以降低对外的成本。因为我们操作的对象是Entity，但是输出的对象是DTO，这里就需要一个专属类型的对象叫DTO Assembler。DTO Assembler的唯一职责是将一个或多个Entity/VO，转化为DTO。注意：DTO Assembler通常不建议有反操作，也就是不会从DTO到Entity，因为通常一个DTO转化为Entity时是无法保证Entity的准确性的。通常，Entity转DTO是有成本的，无论是代码量还是运行时的操作。手写转换代码容易出错，为了节省代码量用Reflection会造成极大的性能损耗。所以这里我还是不遗余力的推荐MapStruct这个库。MapStruct通过静态编译时代码生成，通过写接口和配置注解就可以生成对应的代码，且因为生成的代码是直接赋值，其性能损耗基本可以忽略不计。通过MapStruct，代码即可简化为：import org.mapstruct.Mapper;@Mapperpublic interface OrderDtoAssembler { OrderDtoAssembler INSTANCE = Mappers.getMapper(OrderDtoAssembler.class); OrderDTO orderToDTO(Order order);}public class CheckoutServiceImpl implements CheckoutService { private final OrderDtoAssembler orderDtoAssembler = OrderDtoAssembler.INSTANCE; @Override public OrderDTO checkout(@Valid CheckoutCommand cmd) { // ... Order order = new Order(); // ... Order savedOrder = orderRepository.save(order); return orderDtoAssembler.orderToDTO(savedOrder); }}结合之前的Data Mapper，DTO、Entity和DataObject之间的关系如下图：3.5 Result vs Exception最后，上文曾经提及在Interface层应该返回Result，在Application层应该返回DTO，在这里再次重复提出规范：Application层只返回DTO，可以直接抛异常，不用统一处理。所有调用到的服务也都可以直接抛异常，除非需要特殊处理，否则不需要刻意捕捉异常异常的好处是能明确的知道错误的来源，堆栈等，在Interface层统一捕捉异常是为了避免异常堆栈信息泄漏到API之外，但是在Application层，异常机制仍然是信息量最大，代码结构最清晰的方法，避免了Result的一些常见且繁杂的Result.isSuccess判断。所以在Application层、Domain层，以及Infrastructure层，遇到错误直接抛异常是最合理的方法。3.6 简单讲一下Anti-Corruption Layer防腐层本文仅仅简单描述一下ACL的原理和作用，具体的实施规范可能要等到另外一篇文章。在ApplicationService中，经常会依赖外部服务，从代码层面对外部系统产生了依赖。比如上文中的：ItemDO item = itemService.getItem(cmd.getItemId());boolean withholdSuccess = inventoryService.withhold(cmd.getItemId(), cmd.getQuantity());会发现我们的ApplicationService会强依赖ItemService、InventoryService以及ItemDO这个对象。如果任何一个服务的方法变更，或者ItemDO字段变更，都会有可能影响到ApplicationService的代码。也就是说，我们自己的代码会因为强依赖了外部系统的变化而变更，这个在复杂系统中应该是尽量避免的。那么如何做到对外部系统的隔离呢？需要加入ACL防腐层。ACL防腐层的简单原理如下： 对于依赖的外部对象，我们抽取出所需要的字段，生成一个内部所需的VO或DTO类 构建一个新的Facade，在Facade中封装调用链路，将外部类转化为内部类 针对外部系统调用，同样的用Facade方法封装外部调用链路无防腐层的情况：有防腐层的情况：具体简单实现，假设所有外部依赖都命名为ExternalXXXService：// 自定义的内部值类@Datapublic class ItemDTO { private Long itemId; private Long sellerId; private String title; private Long priceInCents;}// 商品Facade接口public interface ItemFacade { ItemDTO getItem(Long itemId);}// 商品facade实现@Servicepublic class ItemFacadeImpl implements ItemFacade { @Resource private ExternalItemService externalItemService; @Override public ItemDTO getItem(Long itemId) { ItemDO itemDO = externalItemService.getItem(itemId); if (itemDO != null) { ItemDTO dto = new ItemDTO(); dto.setItemId(itemDO.getItemId()); dto.setTitle(itemDO.getTitle()); dto.setPriceInCents(itemDO.getPriceInCents()); dto.setSellerId(itemDO.getSellerId()); return dto; } return null; }}// 库存Facadepublic interface InventoryFacade { boolean withhold(Long itemId, Integer quantity);}@Servicepublic class InventoryFacadeImpl implements InventoryFacade { @Resource private ExternalInventoryService externalInventoryService; @Override public boolean withhold(Long itemId, Integer quantity) { return externalInventoryService.withhold(itemId, quantity); }}通过ACL改造之后，我们ApplicationService的代码改为：@Servicepublic class CheckoutServiceImpl implements CheckoutService { @Resource private ItemFacade itemFacade; @Resource private InventoryFacade inventoryFacade; @Override public OrderDTO checkout(@Valid CheckoutCommand cmd) { ItemDTO item = itemFacade.getItem(cmd.getItemId()); if (item == null) { throw new IllegalArgumentException(&quot;Item not found&quot;); } boolean withholdSuccess = inventoryFacade.withhold(cmd.getItemId(), cmd.getQuantity()); if (!withholdSuccess) { throw new IllegalArgumentException(&quot;Inventory not enough&quot;); } // ... }}很显然，这么做的好处是ApplicationService的代码已经完全不再直接依赖外部的类和方法，而是依赖了我们自己内部定义的值类和接口。如果未来外部服务有任何的变更，需要修改的是Facade类和数据转化逻辑，而不需要修改ApplicationService的逻辑。Repository可以认为是一种特殊的ACL，屏蔽了具体数据操作的细节，即使底层数据库结构变更，数据库类型变更，或者加入其他的持久化方式，Repository的接口保持稳定，ApplicationService就能保持不变。在一些理论框架里ACL Facade也被叫做Gateway，含义是一样的。4. Orchestration vs Choreography在本文最后想聊一下复杂业务流程的设计规范。在复杂的业务流程里，我们通常面临两种模式：Orchestration 和 Choreography。很无奈，这两个英文单词的百度翻译/谷歌翻译，都是“编排”，但实际上这两种模式是完全不一样的设计模式。Orchestration的编排（比如SOA/微服务的服务编排Service Orchestration）是我们通常熟悉的用法，Choreography是最近出现了事件驱动架构EDA才慢慢流行起来。网上可能会有其他的翻译，比如编制、编舞、协作等，但感觉都没有真正的把英文单词的意思表达出来，所以为了避免误解，在下文我尽量还是用英文原词。如果谁有更好的翻译方法欢迎联系我。4.1 模式简介Orchestration：通常出现在脑海里的是一个交响乐团（Orchestra，注意这两个词的相似性），如下图。交响乐团的核心是一个唯一的指挥家Conductor，在一个交响乐中，所有的音乐家必须听从Conductor的指挥做操作，不可以独自发挥。所以在Orchestration模式中，所有的流程都是由一个节点或服务触发的。我们常见的业务流程代码，包括调用外部服务，就是Orchestration，由我们的服务统一触发。Choreography：通常会出现在脑海的场景是一个舞剧（来自于希腊文的舞蹈，Choros），如下图。其中每个不同的舞蹈家都在做自己的事，但是没有一个中心化的指挥。通过协作配合，每个人做好自己的事，整个舞蹈可以展现出一个完整的、和谐的画面。所以在Choreography模式中，每个服务都是独立的个体，可能会响应外部的一些事件，但整个系统是一个整体。4.2 案例用一个常见的例子：下单后支付并发货如果这个案例是Orchestration，则业务逻辑为：下单时从一个预存的账户里扣取资金，并且生成物流单发货，从图上看是这样的：如果这个案例是Choreography，则业务逻辑为：下单，然后等支付成功事件，然后再发货，类似这样：4.3 模式的区别和选择虽然看起来这两种模式都能达到一样的业务目的，但是在实际开发中他们有巨大的差异：从代码依赖关系来看： Orchestration：涉及到一个服务调用到另外的服务，对于调用方来说，是强依赖的服务提供方。 Choreography：每一个服务只是做好自己的事，然后通过事件触发其他的服务，服务之间没有直接调用上的依赖。但要注意的是下游还是会依赖上游的代码（比如事件类），所以可以认为是下游对上游有依赖。从代码灵活性来看： Orchestration：因为服务间的依赖关系是写死的，增加新的业务流程必然需要修改代码。 Choreography：因为服务间没有直接调用关系，可以增加或替换服务，而不需要改上游代码。从调用链路来看： Orchestration：是从一个服务主动调用另一个服务，所以是Command-Driven指令驱动的。 Choreography：是每个服务被动的被外部事件触发，所以是Event-Driven事件驱动的。从业务职责来看： Orchestration：有主动的调用方（比如：下单服务）。无论下游的依赖是谁，主动的调用方都需要为整个业务流程和结果负责。 Choreography：没有主动调用方，每个服务只关心自己的触发条件和结果，没有任何一个服务会为整个业务链路负责总结下来一个比较：   Orchestration Choreography 驱动力 指令驱动Command-Driven 事件驱动Event-Driven 调用依赖 上游强依赖下游 无直接调用依赖但是有代码依赖可以认为是下游依赖上游 灵活性 较差 较高 业务职责 上游为业务负责 无全局责任人 另外需要重点明确的：“指令驱动”和“事件驱动”的区别不是“同步”和“异步”。指令可以是同步调用，也可以是异步消息触发（但异步指令不是事件）；反过来事件可以是异步消息，但也完全可以是进程内的同步调用。所以指令驱动和事件驱动差异的本质不在于调用方式，而是一件事情是否“已经”发生。所以在日常业务中当你碰到一个需求时，该如何选择是用Orchestration还是Choreography？这里给出两个判断方法： 明确依赖的方向： 在代码中的依赖是比较明确的：如果你是下游，上游对你无感知，则只能走事件驱动；如果上游必须要对你有感知，则可以走指令驱动。反过来，如果你是上游，需要对下游强依赖，则是指令驱动；如果下游是谁无所谓，则可以走事件驱动。 找出业务中的“负责人”： 第二种方法是根据业务场景找出其中的“负责人”。比如，如果业务需要通知卖家，下单系统的单一职责不应该为消息通知负责，但订单管理系统需要根据订单状态的推进主动触发消息，所以是这个功能的负责人。 在一个复杂业务流程里，通常两个模式都要有，但也很容易设计错误。如果出现依赖关系很奇怪，或者代码里调用链路/负责人梳理不清楚的情况，可以尝试转换一下模式，可能会好很多。哪个模式更好？很显然，没有最好的模式，只有最合适自己业务场景的模式。❌ 反例：最近几年比较流行的Event-Driven Architecture（EDA）事件驱动架构，以及Reactive-Programming响应式编程（比如RxJava），虽然有很多创新，但在一定程度上是“当你有把锤子，所有问题都是钉子”的典型案例。他们对一些基于事件的、流处理的问题有奇效，但如果拿这些框架硬套指令驱动的业务，就会感到代码极其“不协调”，认知成本提高。所以在日常选型中，还是要先根据业务场景梳理出来是哪些流程中的部分是Orchestration，哪些是Choreography，然后再选择相对应的框架。4.4 跟DDD分层架构的关系最后，讲了这么多O vs C，跟DDD有啥关系？很简单： O&amp;amp;C其实是Interface层的关注点，Orchestration = 对外的API，而Choreography = 消息或事件。当你决策了O还是C之后，需要在interface层承接这些“驱动力”。 无论O&amp;amp;C如何设计，Application层都“无感知”，因为ApplicationService天生就可以处理Command、Query和Event，至于这些对象怎么来，是Interface层的决策。所以，虽然Orchestration 和 Choreography是两种完全不同的业务设计模式，但最终落到Application层的代码应该是一致的，这也是为什么Application层是“用例”而不是“接口”，是相对稳定的存在。5. 总结只要是做业务的，一定会需要写业务流程和服务编排，但不代表这种代码一定质量差。通过DDD的分层架构里的Interface层和Application层的合理拆分，代码可以变得优雅、灵活，能更快的响应业务但同时又能更好的沉淀。本文主要介绍了一些代码的设计规范，帮助大家掌握一定的技巧。Interface层： 职责：主要负责承接网络协议的转化、Session管理等 接口数量：避免所谓的统一API，不必人为限制接口类的数量，每个/每类业务对应一套接口即可，接口参数应该符合业务需求，避免大而全的入参 接口出参：统一返回Result 异常处理：应该捕捉所有异常，避免异常信息的泄漏。可以通过AOP统一处理，避免代码里有大量重复代码。Application层： 入参：具像化Command、Query、Event对象作为ApplicationService的入参，唯一可以的例外是单ID查询的场景。 CQE的语意化：CQE对象有语意，不同用例之间语意不同，即使参数一样也要避免复用。 入参校验：基础校验通过Bean Validation api解决。Spring Validation自带Validation的AOP，也可以自己写AOP。 出参：统一返回DTO，而不是Entity或DO。 DTO转化：用DTO Assembler负责Entity/VO到DTO的转化。 异常处理：不统一捕捉异常，可以随意抛异常。部分Infra层： 用ACL防腐层将外部依赖转化为内部代码，隔离外部的影响业务流程设计模式： 没有最好的模式，取决于业务场景、依赖关系、以及是否有业务“负责人”。避免拿着锤子找钉子。5.1 前瞻预告 CQRS是Application层的一种设计模式，是基于Command和Query分离的一种设计理念，从最简单的对象分离，到目前最复杂的Event-Sourcing。这个topic有很多需要深入的点，也经常可以被用到，特别是结合复杂的Aggregate。后面单独会拉出来讲，标题暂定为《CQRS的7层境界》 在当今复杂的微服务开发环境下，依赖外部团队开发的服务是不可避免的，但强耦合带来的成本（无论是变更、代码依赖、甚至Maven Jar包间接依赖）是一个复杂系统长期不可忽视的点。ACL防腐层是一种隔离理念，将外部耦合去除，让内部代码更加纯粹。ACL防腐层可以有很多种，Repository是一种特殊的面相数据持久化的ACL，K8S-sidecar-istio 可以说是一种网络层的ACL，但在Java/Spring里可以有比Istio更高效、更通用的方法，待后文介绍。 当你开始用起来DDD时，会发现很多代码模式都非常类似，比如主子订单就是总分模式、类目体系的CPV模式也可以用到一些活动上，ECS模式可以在互动业务上发挥作用等等。后面会尝试总结出一些通用的领域设计模式，他们的设计思路、可以解决的问题类型、以及实践落地的方法。 6. 欢迎联系，持续求简历欢迎看到这里的同学给我提任何关于DDD的问题，我会尽可能的回答。文章中的代码案例会稍后申请发布到github上，供大家参考。我的邮箱：guangmiao.lgm@alibaba-inc.com，也可以加我的钉钉号：luangm（殷浩）同时，我们团队也在持续招聘。我团队负责淘系的行业和导购业务，包括天猫和淘宝的四大行业（服饰、快消、消电、家装）以及淘宝的几个大横向业务（企业服务、全球购、有好货等）的日常业务需求和创新业务（3D/AR、360全景视频、搭配、定制、尺码导购、SPU导购等）、前台场（iFashion、全球购、有好货等），以及一些复杂的金融、交易、履约链路（IP撮合、金融服务、交易定制、分销、CPS分佣、服务供应链对接等），总DAU（日均访问用户数）大概3000W左右。我们团队对接了大量的业务形态，从前台导购到后台履约，有极其丰富的应用场景。新的财年我们希望能深入行业，挖掘新的商业模式和履约链路，覆盖一些传统B2C模式无法覆盖到的商业模式，帮助商家在新的赛道成长。欢迎感兴趣的同学加盟。" }, { "title": "殷浩详解DDD系列 第四讲 - 领域层设计规范", "url": "/posts/%E6%AE%B7%E6%B5%A9%E8%AF%A6%E8%A7%A3DDD%E7%B3%BB%E5%88%97-%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E9%A2%86%E5%9F%9F%E5%B1%82%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-03-14 00:00:00 +0800", "snippet": "简介： 在一个DDD架构设计中，领域层的设计合理性会直接影响整个架构的代码结构以及应用层、基础设施层的设计。但是领域层设计又是有挑战的任务，特别是在一个业务逻辑相对复杂应用中，每一个业务规则是应该放在Entity、ValueObject 还是 DomainService是值得用心思考的，既要避免未来的扩展性差，又要确保不会过度设计导致复杂性。今天我用一个相对轻松易懂的领域做一个案例演示，但在实际业务应用中，无论是交易、营销还是互动，都可以用类似的逻辑来实现。在一个DDD架构设计中，领域层的设计合理性会直接影响整个架构的代码结构以及应用层、基础设施层的设计。但是领域层设计又是有挑战的任务，特别是在一个业务逻辑相对复杂应用中，每一个业务规则是应该放在Entity、ValueObject 还是 DomainService是值得用心思考的，既要避免未来的扩展性差，又要确保不会过度设计导致复杂性。今天我用一个相对轻松易懂的领域做一个案例演示，但在实际业务应用中，无论是交易、营销还是互动，都可以用类似的逻辑来实现。一 初探龙与魔法的世界架构1 背景和规则平日里看了好多严肃的业务代码，今天找一个轻松的话题，如何用代码实现一个龙与魔法的游戏世界的（极简）规则？基础配置如下： 玩家（Player）可以是战士（Fighter）、法师（Mage）、龙骑（Dragoon） 怪物（Monster）可以是兽人（Orc）、精灵（Elf）、龙（Dragon），怪物有血量 武器（Weapon）可以是剑（Sword）、法杖（Staff），武器有攻击力玩家可以装备一个武器，武器攻击可以是物理类型（0），火（1），冰（2）等，武器类型决定伤害类型。攻击规则如下： 兽人对物理攻击伤害减半 精灵对魔法攻击伤害减半 龙对物理和魔法攻击免疫，除非玩家是龙骑，则伤害加倍2 OOP实现对于熟悉Object-Oriented Programming的同学，一个比较简单的实现是通过类的继承关系（此处省略部分非核心代码）：public abstract class Player { Weapon weapon}public class Fighter extends Player {}public class Mage extends Player {}public class Dragoon extends Player {}public abstract class Monster { Long health;}public Orc extends Monster {}public Elf extends Monster {}public Dragoon extends Monster {}public abstract class Weapon { int damage; int damageType; // 0 - physical, 1 - fire, 2 - ice etc.}public Sword extends Weapon {}public Staff extends Weapon {}而实现规则代码如下：public class Player { public void attack(Monster monster) { monster.receiveDamageBy(weapon, this); }}public class Monster { public void receiveDamageBy(Weapon weapon, Player player) { this.health -= weapon.getDamage(); // 基础规则 }}public class Orc extends Monster { @Override public void receiveDamageBy(Weapon weapon, Player player) { if (weapon.getDamageType() == 0) { this.setHealth(this.getHealth() - weapon.getDamage() / 2); // Orc的物理防御规则 } else { super.receiveDamageBy(weapon, player); } }}public class Dragon extends Monster { @Override public void receiveDamageBy(Weapon weapon, Player player) { if (player instanceof Dragoon) { this.setHealth(this.getHealth() - weapon.getDamage() * 2); // 龙骑伤害规则 } // else no damage, 龙免疫力规则 }}然后跑几个单测：public class BattleTest { @Test @DisplayName(&quot;Dragon is immune to attacks&quot;) public void testDragonImmunity() { // Given Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Excalibur&quot;, 10); fighter.setWeapon(sword); Dragon dragon = new Dragon(&quot;Dragon&quot;, 100L); // When fighter.attack(dragon); // Then assertThat(dragon.getHealth()).isEqualTo(100); } @Test @DisplayName(&quot;Dragoon attack dragon doubles damage&quot;) public void testDragoonSpecial() { // Given Dragoon dragoon = new Dragoon(&quot;Dragoon&quot;); Sword sword = new Sword(&quot;Excalibur&quot;, 10); dragoon.setWeapon(sword); Dragon dragon = new Dragon(&quot;Dragon&quot;, 100L); // When dragoon.attack(dragon); // Then assertThat(dragon.getHealth()).isEqualTo(100 - 10 * 2); } @Test @DisplayName(&quot;Orc should receive half damage from physical weapons&quot;) public void testFighterOrc() { // Given Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Excalibur&quot;, 10); fighter.setWeapon(sword); Orc orc = new Orc(&quot;Orc&quot;, 100L); // When fighter.attack(orc); // Then assertThat(orc.getHealth()).isEqualTo(100 - 10 / 2); } @Test @DisplayName(&quot;Orc receive full damage from magic attacks&quot;) public void testMageOrc() { // Given Mage mage = new Mage(&quot;Mage&quot;); Staff staff = new Staff(&quot;Fire Staff&quot;, 10); mage.setWeapon(staff); Orc orc = new Orc(&quot;Orc&quot;, 100L); // When mage.attack(orc); // Then assertThat(orc.getHealth()).isEqualTo(100 - 10); }}以上代码和单测都比较简单，不做多余的解释了。3 分析OOP代码的设计缺陷编程语言的强类型无法承载业务规则以上的OOP代码可以跑得通，直到我们加一个限制条件： 战士只能装备剑 法师只能装备法杖这个规则在Java语言里无法通过强类型来实现，虽然Java有Variable Hiding（或者C#的new class variable），但实际上只是在子类上加了一个新变量，所以会导致以下的问题：@Datapublic class Fighter extends Player { private Sword weapon;}@Testpublic void testEquip() { Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Sword&quot;, 10); fighter.setWeapon(sword); Staff staff = new Staff(&quot;Staff&quot;, 10); fighter.setWeapon(staff); assertThat(fighter.getWeapon()).isInstanceOf(Staff.class); // 错误了}在最后，虽然代码感觉是setWeapon(Staff)，但实际上只修改了父类的变量，并没有修改子类的变量，所以实际不生效，也不抛异常，但结果是错的。当然，可以在父类限制setter为protected，但这样就限制了父类的API，极大的降低了灵活性，同时也违背了Liskov substitution principle，即一个父类必须要cast成子类才能使用：@Datapublic abstract class Player { @Setter(AccessLevel.PROTECTED) private Weapon weapon;}@Testpublic void testCastEquip() { Fighter fighter = new Fighter(&quot;Hero&quot;); Sword sword = new Sword(&quot;Sword&quot;, 10); fighter.setWeapon(sword); Player player = fighter; Staff staff = new Staff(&quot;Staff&quot;, 10); player.setWeapon(staff); // 编译不过，但从API层面上应该开放可用}最后，如果规则增加一条： 战士和法师都能装备匕首（dagger）BOOM，之前写的强类型代码都废了，需要重构。对象继承导致代码强依赖父类逻辑，违反开闭原则Open-Closed Principle（OCP）开闭原则（OCP）规定“对象应该对于扩展开放，对于修改封闭“，继承虽然可以通过子类扩展新的行为，但因为子类可能直接依赖父类的实现，导致一个变更可能会影响所有对象。在这个例子里，如果增加任意一种类型的玩家、怪物或武器，或增加一种规则，都有可能需要修改从父类到子类的所有方法。比如，如果要增加一个武器类型：狙击枪，能够无视所有防御一击必杀，需要修改的代码包括： Weapon Player和所有的子类（是否能装备某个武器的判断） Monster和所有的子类（伤害计算逻辑）public void receiveDamageBy(Weapon weapon, Player player) { this.health -= weapon.getDamage(); // 老的基础规则 if (Weapon instanceof Gun) { // 新的逻辑 this.setHealth(0); } }}public class Dragon extends Monster { public void receiveDamageBy(Weapon weapon, Player player) { if (Weapon instanceof Gun) { // 新的逻辑 super.receiveDamageBy(weapon, player); } // 老的逻辑省略 }}在一个复杂的软件中为什么会建议“尽量”不要违背OCP？最核心的原因就是一个现有逻辑的变更可能会影响一些原有的代码，导致一些无法预见的影响。这个风险只能通过完整的单元测试覆盖来保障，但在实际开发中很难保障单测的覆盖率。OCP的原则能尽可能的规避这种风险，当新的行为只能通过新的字段/方法来实现时，老代码的行为自然不会变。继承虽然能Open for extension，但很难做到Closed for modification。所以今天解决OCP的主要方法是通过Composition-over-inheritance，即通过组合来做到扩展性，而不是通过继承。Player.attack(monster) 还是 Monster.receiveDamage(Weapon, Player)？在这个例子里，其实业务规则的逻辑到底应该写在哪里是有异议的：当我们去看一个对象和另一个对象之间的交互时，到底是Player去攻击Monster，还是Monster被Player攻击？目前的代码主要将逻辑写在Monster的类中，主要考虑是Monster会受伤降低Health，但如果是Player拿着一把双刃剑会同时伤害自己呢？是不是发现写在Monster类里也有问题？代码写在哪里的原则是什么？多对象行为类似，导致代码重复当我们有不同的对象，但又有相同或类似的行为时，OOP会不可避免的导致代码的重复。在这个例子里，如果我们去增加一个“可移动”的行为，需要在Player和Monster类中都增加类似的逻辑：public abstract class Player { int x; int y; void move(int targetX, int targetY) { // logic }}public abstract class Monster { int x; int y; void move(int targetX, int targetY) { // logic }}一个可能的解法是有个通用的父类：public abstract class Movable { int x; int y; void move(int targetX, int targetY) { // logic }}public abstract class Player extends Movable;public abstract class Monster extends Movable;但如果再增加一个跳跃能力Jumpable呢？一个跑步能力Runnable呢？如果Player可以Move和Jump，Monster可以Move和Run，怎么处理继承关系？要知道Java（以及绝大部分语言）是不支持多父类继承的，所以只能通过重复代码来实现。问题总结在这个案例里虽然从直觉来看OOP的逻辑很简单，但如果你的业务比较复杂，未来会有大量的业务规则变更时，简单的OOP代码会在后期变成复杂的一团浆糊，逻辑分散在各地，缺少全局视角，各种规则的叠加会触发bug。有没有感觉似曾相识？对的，电商体系里的优惠、交易等链路经常会碰到类似的坑。而这类问题的核心本质在于： 业务规则的归属到底是对象的“行为”还是独立的”规则对象“？ 业务规则之间的关系如何处理？ 通用“行为”应该如何复用和维护？在讲DDD的解法前，我们先去看看一套游戏里最近比较火的架构设计，Entity-Component-System（ECS）是如何实现的。二 Entity-Component-System（ECS）架构简介1 ECS介绍ECS架构模式是其实是一个很老的游戏架构设计，最早应该能追溯到《地牢围攻》的组件化设计，但最近因为Unity的加入而开始变得流行（比如《守望先锋》就是用的ECS）。要很快的理解ECS架构的价值，我们需要理解一个游戏代码的核心问题： 性能：游戏必须要实现一个高的渲染率（60FPS），也就是说整个游戏世界需要在1/60s（大概16ms）内完整更新一次（包括物理引擎、游戏状态、渲染、AI等）。而在一个游戏中，通常有大量的（万级、十万级）游戏对象需要更新状态，除了渲染可以依赖GPU之外，其他的逻辑都需要由CPU完成，甚至绝大部分只能由单线程完成，导致绝大部分时间复杂场景下CPU（主要是内存到CPU的带宽）会成为瓶颈。在CPU单核速度几乎不再增加的时代，如何能让CPU处理的效率提升，是提升游戏性能的核心。 代码组织：如同第一章讲的案例一样，当我们用传统OOP的模式进行游戏开发时，很容易就会陷入代码组织上的问题，最终导致代码难以阅读，维护和优化。 可扩展性：这个跟上一条类似，但更多的是游戏的特性导致：需要快速更新，加入新的元素。一个游戏的架构需要能通过低代码、甚至0代码的方式增加游戏元素，从而通过快速更新而留住用户。如果每次变更都需要开发新的代码，测试，然后让用户重新下载客户端，可想而知这种游戏很难在现在的竞争环境下活下来。而ECS架构能很好的解决上面的几个问题，ECS架构主要分为： Entity：用来代表任何一个游戏对象，但是在ECS里一个Entity最重要的仅仅是他的EntityID，一个Entity里包含多个Component Component：是真正的数据，ECS架构把一个个的实体对象拆分为更加细化的组件，比如位置、素材、状态等，也就是说一个Entity实际上只是一个Bag of Components。 System（或者ComponentSystem，组件系统）：是真正的行为，一个游戏里可以有很多个不同的组件系统，每个组件系统都只负责一件事，可以依次处理大量的相同组件，而不需要去理解具体的Entity。所以一个ComponentSystem理论上可以有更加高效的组件处理效率，甚至可以实现并行处理，从而提升CPU利用率。ECS的一些核心性能优化包括将同类型组件放在同一个Array中，然后Entity仅保留到各自组件的pointer，这样能更好的利用CPU的缓存，减少数据的加载成本，以及SIMD的优化等。一个ECS案例的伪代码如下：public class Entity { public Vector position; // 此处Vector是一个Component, 指向的是MovementSystem.list里的一个}public class MovementSystem { List&amp;lt; Vector&amp;gt; list; // System的行为 public void update(float delta) { for(Vector pos : list) { // 这个loop直接走了CPU缓存，性能很高，同时可以用SIMD优化 pos.x = pos.x + delta; pos.y = pos.y + delta; } }}@Testpublic void test() { MovementSystem system = new MovementSystem(); system.list = new List&amp;lt;&amp;gt;() { new Vector(0, 0) }; Entity entity = new Entity(list.get(0)); system.update(0.1); assertTrue(entity.position.x == 0.1);}由于本文不是讲解ECS架构的，感兴趣的同学可以搜索Entity-Component-System或者看看Unity的ECS文档等。2 ECS架构分析重新回来分析ECS，其实它的本源还是几个很老的概念：组件化在软件系统里，我们通常将复杂的大系统拆分为独立的组件，来降低复杂度。比如网页里通过前端组件化降低重复开发成本，微服务架构通过服务和数据库的拆分降低服务复杂度和系统影响面等。但是ECS架构把这个走到了极致，即每个对象内部都实现了组件化。通过将一个游戏对象的数据和行为拆分为多个组件和组件系统，能实现组件的高度复用性，降低重复开发成本。行为抽离这个在游戏系统里有个比较明显的优势。如果按照OOP的方式，一个游戏对象里可能会包括移动代码、战斗代码、渲染代码、AI代码等，如果都放在一个类里会很长，且很难去维护。通过将通用逻辑抽离出来为单独的System类，可以明显提升代码的可读性。另一个好处则是抽离了一些和对象代码无关的依赖，比如上文的delta，这个delta如果是放在Entity的update方法，则需要作为入参注入，而放在System里则可以统一管理。在第一章的有个问题，到底是应该Player.attack(monster) 还是 Monster.receiveDamage(Weapon, Player)。在ECS里这个问题就变的很简单，放在CombatSystem里就可以了。数据驱动即一个对象的行为不是写死的而是通过其参数决定，通过参数的动态修改，就可以快速改变一个对象的具体行为。在ECS的游戏架构里，通过给Entity注册相应的Component，以及改变Component的具体参数的组合，就可以改变一个对象的行为和玩法，比如创建一个水壶+爆炸属性就变成了“爆炸水壶”、给一个自行车加上风魔法就变成了飞车等。在有些Rougelike游戏中，可能有超过1万件不同类型、不同功能的物品，如果这些不同功能的物品都去单独写代码，可能永远都写不完，但是通过数据驱动+组件化架构，所有物品的配置最终就是一张表，修改也极其简单。这个也是组合胜于继承原则的一次体现。3 ECS的缺陷虽然ECS在游戏界已经开始崭露头角，我发现ECS架构目前还没有在哪个大型商业应用中被使用过。原因可能很多，包括ECS比较新大家还不了解、缺少商业成熟可用的框架、程序员们还不够能适应从写逻辑脚本到写组件的思维转变等，但我认为其最大的一个问题是ECS为了提升性能，强调了数据/状态（State）和行为（Behaivor）分离，并且为了降低GC成本，直接操作数据，走到了一个极端。而在商业应用中，数据的正确性、一致性和健壮性应该是最高的优先级，而性能只是锦上添花的东西，所以ECS很难在商业场景里带来特别大的好处。但这不代表我们不能借鉴一些ECS的突破性思维，包括组件化、跨对象行为的抽离、以及数据驱动模式，而这些在DDD里也能很好的用起来。三 基于DDD架构的一种解法1 领域对象回到我们原来的问题域上面，我们从领域层拆分一下各种对象：实体类在DDD里，实体类包含ID和内部状态，在这个案例里实体类包含Player、Monster和Weapon。Weapon被设计成实体类是因为两把同名的Weapon应该可以同时存在，所以必须要有ID来区分，同时未来也可以预期Weapon会包含一些状态，比如升级、临时的buff、耐久等。public class Player implements Movable { private PlayerId id; private String name; private PlayerClass playerClass; // enum private WeaponId weaponId; // （Note 1） private Transform position = Transform.ORIGIN; private Vector velocity = Vector.ZERO;}public class Monster implements Movable { private MonsterId id; private MonsterClass monsterClass; // enum private Health health; private Transform position = Transform.ORIGIN; private Vector velocity = Vector.ZERO;}public class Weapon { private WeaponId id; private String name; private WeaponType weaponType; // enum private int damage; private int damageType; // 0 - physical, 1 - fire, 2 - ice}在这个简单的案例里，我们可以利用enum的PlayerClass、MonsterClass来代替继承关系，后续也可以利用Type Object设计模式来做到数据驱动。 Note 1: 因为 Weapon 是实体类，但是Weapon能独立存在，Player不是聚合根，所以Player只能保存WeaponId，而不能直接指向Weapon。值对象的组件化在前面的ECS架构里，有个MovementSystem的概念是可以复用的，虽然不应该直接去操作Component或者继承通用的父类，但是可以通过接口的方式对领域对象做组件化处理：public interface Movable { // 相当于组件 Transform getPosition(); Vector getVelocity(); // 行为 void moveTo(long x, long y); void startMove(long velX, long velY); void stopMove(); boolean isMoving();}// 具体实现public class Player implements Movable { public void moveTo(long x, long y) { this.position = new Transform(x, y); } public void startMove(long velocityX, long velocityY) { this.velocity = new Vector(velocityX, velocityY); } public void stopMove() { this.velocity = Vector.ZERO; } @Override public boolean isMoving() { return this.velocity.getX() != 0 || this.velocity.getY() != 0; }}@Valuepublic class Transform { public static final Transform ORIGIN = new Transform(0, 0); long x; long y;}@Valuepublic class Vector { public static final Vector ZERO = new Vector(0, 0); long x; long y;}注意两点： Moveable的接口没有Setter。一个Entity的规则是不能直接变更其属性，必须通过Entity的方法去对内部状态做变更。这样能保证数据的一致性。 抽象Movable的好处是如同ECS一样，一些特别通用的行为（如在大地图里移动）可以通过统一的System代码去处理，避免了重复劳动。2 装备行为因为我们已经不会用Player的子类来决定什么样的Weapon可以装备，所以这段逻辑应该被拆分到一个单独的类里。这种类在DDD里被叫做领域服务（Domain Service）。public interface EquipmentService { boolean canEquip(Player player, Weapon weapon);}在DDD里，一个Entity不应该直接参考另一个Entity或服务，也就是说以下的代码是错误的：public class Player { @Autowired EquipmentService equipmentService; // BAD: 不可以直接依赖 public void equip(Weapon weapon) { // ... }}这里的问题是Entity只能保留自己的状态（或非聚合根的对象）。任何其他的对象，无论是否通过依赖注入的方式弄进来，都会破坏Entity的Invariance，并且还难以单测。正确的引用方式是通过方法参数引入（Double Dispatch）：public class Player { public void equip(Weapon weapon, EquipmentService equipmentService) { if (equipmentService.canEquip(this, weapon)) { this.weaponId = weapon.getId(); } else { throw new IllegalArgumentException(&quot;Cannot Equip: &quot; + weapon); } }}在这里，无论是Weapon还是EquipmentService都是通过方法参数传入，确保不会污染Player的自有状态。Double Dispatch是一个使用Domain Service经常会用到的方法，类似于调用反转。然后在EquipmentService里实现相关的逻辑判断，这里我们用了另一个常用的Strategy（或者叫Policy）设计模式：public class EquipmentServiceImpl implements EquipmentService { private EquipmentManager equipmentManager; @Override public boolean canEquip(Player player, Weapon weapon) { return equipmentManager.canEquip(player, weapon); }}// 策略优先级管理public class EquipmentManager { private static final List&amp;lt; EquipmentPolicy&amp;gt; POLICIES = new ArrayList&amp;lt;&amp;gt;(); static { POLICIES.add(new FighterEquipmentPolicy()); POLICIES.add(new MageEquipmentPolicy()); POLICIES.add(new DragoonEquipmentPolicy()); POLICIES.add(new DefaultEquipmentPolicy()); } public boolean canEquip(Player player, Weapon weapon) { for (EquipmentPolicy policy : POLICIES) { if (!policy.canApply(player, weapon)) { continue; } return policy.canEquip(player, weapon); } return false; }}// 策略案例public class FighterEquipmentPolicy implements EquipmentPolicy { @Override public boolean canApply(Player player, Weapon weapon) { return player.getPlayerClass() == PlayerClass.Fighter; } /** * Fighter能装备Sword和Dagger */ @Override public boolean canEquip(Player player, Weapon weapon) { return weapon.getWeaponType() == WeaponType.Sword || weapon.getWeaponType() == WeaponType.Dagger; }}// 其他策略省略，见源码这样设计的最大好处是未来的规则增加只需要添加新的Policy类，而不需要去改变原有的类。3 攻击行为在上文中曾经有提起过，到底应该是Player.attack(Monster)还是Monster.receiveDamage(Weapon, Player)？在DDD里，因为这个行为可能会影响到Player、Monster和Weapon，所以属于跨实体的业务逻辑。在这种情况下需要通过一个第三方的领域服务（Domain Service）来完成。public interface CombatService { void performAttack(Player player, Monster monster);}public class CombatServiceImpl implements CombatService { private WeaponRepository weaponRepository; private DamageManager damageManager; @Override public void performAttack(Player player, Monster monster) { Weapon weapon = weaponRepository.find(player.getWeaponId()); int damage = damageManager.calculateDamage(player, weapon, monster); if (damage &amp;gt; 0) { monster.takeDamage(damage); // （Note 1）在领域服务里变更Monster } // 省略掉Player和Weapon可能受到的影响 }}同样的在这个案例里，可以通过Strategy设计模式来解决damage的计算问题：// 策略优先级管理public class DamageManager { private static final List&amp;lt; DamagePolicy&amp;gt; POLICIES = new ArrayList&amp;lt;&amp;gt;(); static { POLICIES.add(new DragoonPolicy()); POLICIES.add(new DragonImmunityPolicy()); POLICIES.add(new OrcResistancePolicy()); POLICIES.add(new ElfResistancePolicy()); POLICIES.add(new PhysicalDamagePolicy()); POLICIES.add(new DefaultDamagePolicy()); } public int calculateDamage(Player player, Weapon weapon, Monster monster) { for (DamagePolicy policy : POLICIES) { if (!policy.canApply(player, weapon, monster)) { continue; } return policy.calculateDamage(player, weapon, monster); } return 0; }}// 策略案例public class DragoonPolicy implements DamagePolicy { public int calculateDamage(Player player, Weapon weapon, Monster monster) { return weapon.getDamage() * 2; } @Override public boolean canApply(Player player, Weapon weapon, Monster monster) { return player.getPlayerClass() == PlayerClass.Dragoon &amp;amp;&amp;amp; monster.getMonsterClass() == MonsterClass.Dragon; }}特别需要注意的是这里的CombatService领域服务和3.2的EquipmentService领域服务，虽然都是领域服务，但实质上有很大的差异。上文的EquipmentService更多的是提供只读策略，且只会影响单个对象，所以可以在Player.equip方法上通过参数注入。但是CombatService有可能会影响多个对象，所以不能直接通过参数注入的方式调用。4 单元测试@Test@DisplayName(&quot;Dragoon attack dragon doubles damage&quot;)public void testDragoonSpecial() { // Given Player dragoon = playerFactory.createPlayer(PlayerClass.Dragoon, &quot;Dart&quot;); Weapon sword = weaponFactory.createWeaponFromPrototype(swordProto, &quot;Soul Eater&quot;, 60); ((WeaponRepositoryMock)weaponRepository).cache(sword); dragoon.equip(sword, equipmentService); Monster dragon = monsterFactory.createMonster(MonsterClass.Dragon, 100); // When combatService.performAttack(dragoon, dragon); // Then assertThat(dragon.getHealth()).isEqualTo(Health.ZERO); assertThat(dragon.isAlive()).isFalse();}@Test@DisplayName(&quot;Orc should receive half damage from physical weapons&quot;)public void testFighterOrc() { // Given Player fighter = playerFactory.createPlayer(PlayerClass.Fighter, &quot;MyFighter&quot;); Weapon sword = weaponFactory.createWeaponFromPrototype(swordProto, &quot;My Sword&quot;); ((WeaponRepositoryMock)weaponRepository).cache(sword); fighter.equip(sword, equipmentService); Monster orc = monsterFactory.createMonster(MonsterClass.Orc, 100); // When combatService.performAttack(fighter, orc); // Then assertThat(orc.getHealth()).isEqualTo(Health.of(100 - 10 / 2));}具体的代码比较简单，解释省略。5 移动系统最后还有一种Domain Service，通过组件化，我们其实可以实现ECS一样的System，来降低一些重复性的代码：public class MovementSystem { private static final long X_FENCE_MIN = -100; private static final long X_FENCE_MAX = 100; private static final long Y_FENCE_MIN = -100; private static final long Y_FENCE_MAX = 100; private List&amp;lt; Movable&amp;gt; entities = new ArrayList&amp;lt;&amp;gt;(); public void register(Movable movable) { entities.add(movable); } public void update() { for (Movable entity : entities) { if (!entity.isMoving()) { continue; } Transform old = entity.getPosition(); Vector vel = entity.getVelocity(); long newX = Math.max(Math.min(old.getX() + vel.getX(), X_FENCE_MAX), X_FENCE_MIN); long newY = Math.max(Math.min(old.getY() + vel.getY(), Y_FENCE_MAX), Y_FENCE_MIN); entity.moveTo(newX, newY); } }}单测：@Test@DisplayName(&quot;Moving player and monster at the same time&quot;)public void testMovement() { // Given Player fighter = playerFactory.createPlayer(PlayerClass.Fighter, &quot;MyFighter&quot;); fighter.moveTo(2, 5); fighter.startMove(1, 0); Monster orc = monsterFactory.createMonster(MonsterClass.Orc, 100); orc.moveTo(10, 5); orc.startMove(-1, 0); movementSystem.register(fighter); movementSystem.register(orc); // When movementSystem.update(); // Then assertThat(fighter.getPosition().getX()).isEqualTo(2 + 1); assertThat(orc.getPosition().getX()).isEqualTo(10 - 1);}在这里MovementSystem就是一个相对独立的Domain Service，通过对Movable的组件化，实现了类似代码的集中化、以及一些通用依赖/配置的中心化（如X、Y边界等）。四 DDD领域层的一些设计规范上面我主要针对同一个例子对比了OOP、ECS和DDD的3种实现，比较如下： 基于继承关系的OOP代码：OOP的代码最好写，也最容易理解，所有的规则代码都写在对象里，但是当领域规则变得越来越复杂时，其结构会限制它的发展。新的规则有可能会导致代码的整体重构。 基于组件化的ECS代码：ECS代码有最高的灵活性、可复用性、及性能，但极具弱化了实体类的内聚，所有的业务逻辑都写在了服务里，会导致业务的一致性无法保障，对商业系统会有较大的影响。 基于领域对象 + 领域服务的DDD架构：DDD的规则其实最复杂，同时要考虑到实体类的内聚和保证不变性（Invariants），也要考虑跨对象规则代码的归属，甚至要考虑到具体领域服务的调用方式，理解成本比较高。所以下面，我会尽量通过一些设计规范，来降低DDD领域层的设计成本。1 实体类（Entity）大多数DDD架构的核心都是实体类，实体类包含了一个领域里的状态、以及对状态的直接操作。Entity最重要的设计原则是保证实体的不变性（Invariants），也就是说要确保无论外部怎么操作，一个实体内部的属性都不能出现相互冲突，状态不一致的情况。所以几个设计原则如下：创建即一致在贫血模型里，通常见到的代码是一个模型通过手动new出来之后，由调用方一个参数一个参数的赋值，这就很容易产生遗漏，导致实体状态不一致。所以DDD里实体创建的方法有两种：1）constructor参数要包含所有必要属性，或者在constructor里有合理的默认值比如，账号的创建：public class Account { private String accountNumber; private Long amount;}@Testpublic void test() { Account account = new Account(); account.setAmount(100L); TransferService.transfer(account); // 报错了，因为Account缺少必要的AccountNumber}如果缺少一个强校验的constructor，就无法保障创建的实体的一致性。所以需要增加一个强校验的constructor：public Account(String accountNumber, Long amount) { assert StringUtils.isNotBlank(accountNumber); assert amount &amp;gt;= 0; this.accountNumber = accountNumber; this.amount = amount; }}@Testpublic void test() { Account account = new Account(&quot;123&quot;, 100L); // 确保对象的有效性}2）使用Factory模式来降低调用方复杂度另一种方法是通过Factory模式来创建对象，降低一些重复性的入参。比如：public class WeaponFactory { public Weapon createWeaponFromPrototype(WeaponPrototype proto, String newName) { Weapon weapon = new Weapon(null, newName, proto.getWeaponType(), proto.getDamage(), proto.getDamageType()); return weapon; }}通过传入一个已经存在的Prototype，可以快速的创建新的实体。还有一些其他的如Builder等设计模式就不一一指出了。尽量避免public setter一个最容易导致不一致性的原因是实体暴露了public的setter方法，特别是set单一参数会导致状态不一致的情况。比如，一个订单可能包含订单状态（下单、已支付、已发货、已收货）、支付单、物流单等子实体，如果一个调用方能随意去set订单状态，就有可能导致订单状态和子实体匹配不上，导致业务流程走不通的情况。所以在实体里，需要通过行为方法来修改内部状态：@Data @Setter(AccessLevel.PRIVATE) // 确保不生成public setterpublic class Order { private int status; // 0 - 创建，1 - 支付，2 - 发货，3 - 收货 private Payment payment; // 支付单 private Shipping shipping; // 物流单 public void pay(Long userId, Long amount) { if (status != 0) { throw new IllegalStateException(); } this.status = 1; this.payment = new Payment(userId, amount); } public void ship(String trackingNumber) { if (status != 1) { throw new IllegalStateException(); } this.status = 2; this.shipping = new Shipping(trackingNumber); }} 【建议】在有些简单场景里，有时候确实可以比较随意的设置一个值而不会导致不一致性，也建议将方法名重新写为比较“行为化”的命名，会增强其语意。比如setPosition(x, y)可以叫做moveTo(x, y)，setAddress可以叫做assignAddress等。通过聚合根保证主子实体的一致性在稍微复杂一点的领域里，通常主实体会包含子实体，这时候主实体就需要起到聚合根的作用，即： 子实体不能单独存在，只能通过聚合根的方法获取到。任何外部的对象都不能直接保留子实体的引用 子实体没有独立的Repository，不可以单独保存和取出，必须要通过聚合根的Repository实例化 子实体可以单独修改自身状态，但是多个子实体之间的状态一致性需要聚合根来保障常见的电商域中聚合的案例如主子订单模型、商品/SKU模型、跨子订单优惠、跨店优惠模型等。很多聚合根和Repository的设计规范在我前面一篇关于Repository的文章中已经详细解释过，可以拿来参考。不可以强依赖其他聚合根实体或领域服务一个实体的原则是高内聚、低耦合，即一个实体类不能直接在内部直接依赖一个外部的实体或服务。这个原则和绝大多数ORM框架都有比较严重的冲突，所以是一个在开发过程中需要特别注意的。这个原则的必要原因包括：对外部对象的依赖性会直接导致实体无法被单测；以及一个实体无法保证外部实体变更后不会影响本实体的一致性和正确性。所以，正确的对外部依赖的方法有两种： 只保存外部实体的ID：这里我再次强烈建议使用强类型的ID对象，而不是Long型ID。强类型的ID对象不单单能自我包含验证代码，保证ID值的正确性，同时还能确保各种入参不会因为参数顺序变化而出bug。具体可以参考我的Domain Primitive文章。 针对于“无副作用”的外部依赖，通过方法入参的方式传入。比如上文中的equip(Weapon，EquipmentService）方法。如果方法对外部依赖有副作用，不能通过方法入参的方式，只能通过Domain Service解决，见下文。任何实体的行为只能直接影响到本实体（和其子实体）这个原则更多是一个确保代码可读性、可理解的原则，即任何实体的行为不能有“直接”的”副作用“，即直接修改其他的实体类。这么做的好处是代码读下来不会产生意外。另一个遵守的原因是可以降低未知的变更的风险。在一个系统里一个实体对象的所有变更操作应该都是预期内的，如果一个实体能随意被外部直接修改的话，会增加代码bug的风险。2 领域服务（Domain Service）在上文讲到，领域服务其实也分很多种，在这里根据上文总结出来三种常见的：单对象策略型这种领域对象主要面向的是单个实体对象的变更，但涉及到多个领域对象或外部依赖的一些规则。在上文中，EquipmentService即为此类： 变更的对象是Player的参数 读取的是Player和Weapon的数据，可能还包括从外部读取一些数据在这种类型下，实体应该通过方法入参的方式传入这种领域服务，然后通过Double Dispatch来反转调用领域服务的方法，比如：Player.equip(Weapon, EquipmentService) { EquipmentService.canEquip(this, Weapon);}为什么这种情况下不能先调用领域服务，再调用实体对象的方法，从而减少实体对领域服务的入参型依赖呢？比如，下面这个方法是错误的：boolean canEquip = EquipmentService.canEquip(Player, Weapon);if (canEquip) { Player.equip(Weapon); // ❌，这种方法不可行，因为这个方法有不一致的可能性}其错误的主要原因是缺少了领域服务入参会导致方法有可能产生不一致的情况。跨对象事务型当一个行为会直接修改多个实体时，不能再通过单一实体的方法作处理，而必须直接使用领域服务的方法来做操作。在这里，领域服务更多的起到了跨对象事务的作用，确保多个实体的变更之间是有一致性的。在上文里，虽然以下的代码虽然可以跑到通，但是是不建议的：public class Player { void attack(Monster, CombatService) { CombatService.performAttack(this, Monster); // ❌，不要这么写，会导致副作用 }}而我们真实调用应该直接调用CombatService的方法：public void test() { //... combatService.performAttack(mage, orc);}这个原则也映射了“任何实体的行为只能直接影响到本实体（和其子实体）”的原则，即Player.attack会直接影响到Monster，但这个调用Monster又没有感知。通用组件型这种类型的领域服务更像ECS里的System，提供了组件化的行为，但本身又不直接绑死在一种实体类上。具体案例可以参考上文中的MovementSystem实现。3 策略对象（Domain Policy）Policy或者Strategy设计模式是一个通用的设计模式，但是在DDD架构中会经常出现，其核心就是封装领域规则。一个Policy是一个无状态的单例对象，通常需要至少2个方法：canApply 和 一个业务方法。其中，canApply方法用来判断一个Policy是否适用于当前的上下文，如果适用则调用方会去触发业务方法。通常，为了降低一个Policy的可测试性和复杂度，Policy不应该直接操作对象，而是通过返回计算后的值，在Domain Service里对对象进行操作。在上文案例里，DamagePolicy只负责计算应该受到的伤害，而不是直接对Monster造成伤害。这样除了可测试外，还为未来的多Policy叠加计算做了准备。除了本文里静态注入多个Policy以及手动排优先级之外，在日常开发中经常能见到通过Java的SPI机制或类SPI机制注册Policy，以及通过不同的Priority方案对Policy进行排序，在这里就不作太多的展开了。五 副作用的处理方法 - 领域事件在上文中，有一种类型的领域规则被我刻意忽略了，那就是”副作用“。一般的副作用发生在核心领域模型状态变更后，同步或者异步对另一个对象的影响或行为。在这个案例里，我们可以增加一个副作用规则： 当Monster的生命值降为0后，给Player奖励经验值这种问题有很多种解法，比如直接把副作用写在CombatService里：public class CombatService { public void performAttack(Player player, Monster monster) { // ... monster.takeDamage(damage); if (!monster.isAlive()) { player.receiveExp(10); // 收到经验 } }}但是这样写的问题是：很快CombatService的代码就会变得很复杂，比如我们再加一个副作用： 当Player的exp达到100时，升一级这时我们的代码就会变成：public class CombatService { public void performAttack(Player player, Monster monster) { // ... monster.takeDamage(damage); if (!monster.isAlive()) { player.receiveExp(10); // 收到经验 if (player.canLevelUp()) { player.levelUp(); // 升级 } } }}如果再加上“升级后奖励XXX”呢？“更新XXX排行”呢？依此类推，后续这种代码将无法维护。所以我们需要介绍一下领域层最后一个概念：领域事件（Domain Event）。1 领域事件介绍领域事件是一个在领域里发生了某些事后，希望领域里其他对象能够感知到的通知机制。在上面的案例里，代码之所以会越来越复杂，其根本的原因是反应代码（比如升级）直接和上面的事件触发条件（比如收到经验）直接耦合，而且这种耦合性是隐性的。领域事件的好处就是将这种隐性的副作用“显性化”，通过一个显性的事件，将事件触发和事件处理解耦，最终起到代码更清晰、扩展性更好的目的。所以，领域事件是在DDD里，比较推荐使用的跨实体“副作用”传播机制。2 领域事件实现和消息队列中间件不同的是，领域事件通常是立即执行的、在同一个进程内、可能是同步或异步。我们可以通过一个EventBus来实现进程内的通知机制，简单实现如下：// 实现者：瑜进 2019/11/28public class EventBus { // 注册器 @Getter private final EventRegistry invokerRegistry = new EventRegistry(this); // 事件分发器 private final EventDispatcher dispatcher = new EventDispatcher(ExecutorFactory.getDirectExecutor()); // 异步事件分发器 private final EventDispatcher asyncDispatcher = new EventDispatcher(ExecutorFactory.getThreadPoolExecutor()); // 事件分发 public boolean dispatch(Event event) { return dispatch(event, dispatcher); } // 异步事件分发 public boolean dispatchAsync(Event event) { return dispatch(event, asyncDispatcher); } // 内部事件分发 private boolean dispatch(Event event, EventDispatcher dispatcher) { checkEvent(event); // 1.获取事件数组 Set&amp;lt; Invoker&amp;gt; invokers = invokerRegistry.getInvokers(event); // 2.一个事件可以被监听N次，不关心调用结果 dispatcher.dispatch(event, invokers); return true; } // 事件总线注册 public void register(Object listener) { if (listener == null) { throw new IllegalArgumentException(&quot;listener can not be null!&quot;); } invokerRegistry.register(listener); } private void checkEvent(Event event) { if (event == null) { throw new IllegalArgumentException(&quot;event&quot;); } if (!(event instanceof Event)) { throw new IllegalArgumentException(&quot;Event type must by &quot; + Event.class); } }}调用方式：public class LevelUpEvent implements Event { private Player player;}public class LevelUpHandler { public void handle(Player player);}public class Player { public void receiveExp(int value) { this.exp += value; if (this.exp &amp;gt;= 100) { LevelUpEvent event = new LevelUpEvent(this); EventBus.dispatch(event); this.exp = 0; } }}@Testpublic void test() { EventBus.register(new LevelUpHandler()); player.setLevel(1); player.receiveExp(100); assertThat(player.getLevel()).equals(2);}3 目前领域事件的缺陷和展望从上面代码可以看出来，领域事件的很好的实施依赖EventBus、Dispatcher、Invoker这些属于框架级别的支持。同时另一个问题是因为Entity不能直接依赖外部对象，所以EventBus目前只能是一个全局的Singleton，而大家都应该知道全局Singleton对象很难被单测。这就容易导致Entity对象无法被很容易的被完整单测覆盖全。另一种解法是侵入Entity，对每个Entity增加一个List：public class Player { List&amp;lt; Event&amp;gt; events; public void receiveExp(int value) { this.exp += value; if (this.exp &amp;gt;= 100) { LevelUpEvent event = new LevelUpEvent(this); events.add(event); // 把event加进去 this.exp = 0; } }}@Testpublic void test() { EventBus.register(new LevelUpHandler()); player.setLevel(1); player.receiveExp(100); for(Event event: player.getEvents()) { // 在这里显性的dispatch事件 EventBus.dispatch(event); } assertThat(player.getLevel()).equals(2);}但是能看出来这种解法不但会侵入实体本身，同时也需要比较啰嗦的显性在调用方dispatch事件，也不是一个好的解决方案。也许未来会有一个框架能让我们既不依赖全局Singleton，也不需要显性去处理事件，但目前的方案基本都有或多或少的缺陷，大家在使用中可以注意。六 总结在真实的业务逻辑里，我们的领域模型或多或少的都有一定的“特殊性”，如果100%的要符合DDD规范可能会比较累，所以最主要的是梳理一个对象行为的影响面，然后作出设计决策，即： 是仅影响单一对象还是多个对象 规则未来的拓展性、灵活性 性能要求 副作用的处理，等等当然，很多时候一个好的设计是多种因素的取舍，需要大家有一定的积累，真正理解每个架构背后的逻辑和优缺点。一个好的架构师不是有一个正确答案，而是能从多个方案中选出一个最平衡的方案。" }, { "title": "殷浩详解DDD系列 第三讲 - Repository模式", "url": "/posts/%E6%AE%B7%E6%B5%A9%E8%AF%A6%E8%A7%A3DDD%E7%B3%BB%E5%88%97-%E7%AC%AC%E4%B8%89%E8%AE%B2-Repository%E6%A8%A1%E5%BC%8F/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-03-13 00:00:00 +0800", "snippet": "简介： 写在前面 这篇文章和上一篇隔了比较久，一方面是工作比较忙，另一方面是在讲Repository之前其实应该先讲Entity（实体）、Aggregate Root（聚合根）、Bounded Context（限界上下文）等概念。但在实际写的过程中，发现单纯讲Entity相关的东西会比较抽象，很难落地。所以本文被推倒重来，从Repository第三讲 - Repository模式写在前面这篇文章和上一篇隔了比较久，一方面是工作比较忙，另一方面是在讲Repository之前其实应该先讲Entity（实体）、Aggregate Root（聚合根）、Bounded Context（限界上下文）等概念。但在实际写的过程中，发现单纯讲Entity相关的东西会比较抽象，很难落地。所以本文被推倒重来，从Repository开始入手，先把可以落地的、能形成规范的东西先确定下来，最后再尝试落地Entity。这个当然也是我们可以在日常按照DDD重构时尝试的路径。提前预告，接下来的一篇文章将覆盖Anti-Corruption Layer（防腐层）的逻辑，但是你会发现跟Repository模式的理念非常接近。等所有周边的东西都覆盖之后，再详细讲Entity也许会变得不那么抽象。DDD的宏观理念其实并不难懂，但是如同REST一样，DDD也只是一个设计思想，缺少一套完整的规范，导致DDD新手落地困难。我之前的架构篇主要从顶层设计往下看，从这一篇开始我希望能填补上一些DDD的代码落地规范，帮助同学在日常工作中落地DDD思想，并且希望能通过一整套规范，让不同的业务之间的同学能够更快的看懂、掌握对方的代码。但是规则是死的、人是活的，各位同学需要根据自己业务的实际情况去有选择的去落地规范，DDD的规范不可能覆盖所有场景，但我希望能通过解释，让同学们了解DDD背后的一些思考和取舍。1. 为什么要用Repository1.1 - 实体模型 vs. 贫血模型Entity（实体）这个词在计算机领域的最初应用可能是来自于Peter Chen在1976年的“The Entity-Relationship Model - Toward a Unified View of Data”（ER模型），用来描述实体之间的关系，而ER模型后来逐渐的演变成为一个数据模型，在关系型数据库中代表了数据的储存方式。而2006年的JPA标准，通过@Entity等注解，以及Hibernate等ORM框架的实现，让很多Java开发对Entity的理解停留在了数据映射层面，忽略了Entity实体的本身行为，造成今天很多的模型仅包含了实体的数据和属性，而所有的业务逻辑都被分散在多个服务、Controller、Utils工具类中，这个就是Martin Fowler所说的的Anemic Domain Model（贫血领域模型）。如何知道你的模型是贫血的呢？可以看一下你代码中是否有以下的几个特征： 有大量的XxxDO对象：这里DO虽然有时候代表了Domain Object，但实际上仅仅是数据库表结构的映射，里面没有包含（或包含了很少的）业务逻辑； 服务和Controller里有大量的业务逻辑：比如校验逻辑、计算逻辑、格式转化逻辑、对象关系逻辑、数据存储逻辑等； 大量的Utils工具类等。而贫血模型的缺陷是非常明显的： 无法保护模型对象的完整性和一致性：因为对象的所有属性都是公开的，只能由调用方来维护模型的一致性，而这个是没有保障的；之前曾经出现的案例就是调用方没有能维护模型数据的一致性，导致脏数据使用时出现bug，这一类的bug还特别隐蔽，很难排查到。 对象操作的可发现性极差：单纯从对象的属性上很难看出来都有哪些业务逻辑，什么时候可以被调用，以及可以赋值的边界是什么；比如说，Long类型的值是否可以是0或者负数？ 代码逻辑重复：比如校验逻辑、计算逻辑，都很容易出现在多个服务、多个代码块里，提升维护成本和bug出现的概率；一类常见的bug就是当贫血模型变更后，校验逻辑由于出现在多个地方，没有能跟着变，导致校验失败或失效。 代码的健壮性差：比如一个数据模型的变化可能导致从上到下的所有代码的变更。 强依赖底层实现：业务代码里强依赖了底层数据库、网络/中间件协议、第三方服务等，造成核心逻辑代码的僵化且维护成本高。虽然贫血模型有很大的缺陷，但是在我们日常的代码中，我见过的99%的代码都是基于贫血模型，为什么呢？我总结了以下几点： 数据库思维：从有了数据库的那一天起，开发人员的思考方式就逐渐从“写业务逻辑“转变为了”写数据库逻辑”，也就是我们经常说的在写CRUD代码。 贫血模型“简单”：贫血模型的优势在于“简单”，仅仅是对数据库表的字段映射，所以可以从前到后用统一格式串通。这里简单打了引号，是因为它只是表面上的简单，实际上当未来有模型变更时，你会发现其实并不简单，每次变更都是非常复杂的事情 脚本思维：很多常见的代码都属于“脚本”或“胶水代码”，也就是流程式代码。脚本代码的好处就是比较容易理解，但长久来看缺乏健壮性，维护成本会越来越高。但是可能最核心的原因在于，实际上我们在日常开发中，混淆了两个概念： 数据模型（Data Model）：指业务数据该如何持久化，以及数据之间的关系，也就是传统的ER模型； 业务模型/领域模型（Domain Model）：指业务逻辑中，相关联的数据该如何联动。所以，解决这个问题的根本方案，就是要在代码里严格区分Data Model和Domain Model，具体的规范会在后文详细描述。在真实代码结构中，Data Model和 Domain Model实际上会分别在不同的层里，Data Model只存在于数据层，而Domain Model在领域层，而链接了这两层的关键对象，就是Repository。1.2 - Repository的价值在传统的数据库驱动开发中，我们会对数据库操作做一个封装，一般叫做Data Access Object（DAO）。DAO的核心价值是封装了拼接SQL、维护数据库连接、事务等琐碎的底层逻辑，让业务开发可以专注于写代码。但是在本质上，DAO的操作还是数据库操作，DAO的某个方法还是在直接操作数据库和数据模型，只是少写了部分代码。在Uncle Bob的《代码整洁之道》一书里，作者用了一个非常形象的描述： 硬件（Hardware）：指创造了之后不可（或者很难）变更的东西。数据库对于开发来说，就属于”硬件“，数据库选型后基本上后面不会再变，比如：用了MySQL就很难再改为MongoDB，改造成本过高。 软件（Software）：指创造了之后可以随时修改的东西。对于开发来说，业务代码应该追求做”软件“，因为业务流程、规则在不停的变化，我们的代码也应该能随时变化。 固件（Firmware）：即那些强烈依赖了硬件的软件。我们常见的是路由器里的固件或安卓的固件等等。固件的特点是对硬件做了抽象，但仅能适配某款硬件，不能通用。所以今天不存在所谓的通用安卓固件，而是每个手机都需要有自己的固件。从上面的描述我们能看出来，数据库在本质上属于”硬件“，DAO在本质上属于”固件“，而我们自己的代码希望是属于”软件“。但是，固件有个非常不好的特性，那就是会传播，也就是说当一个软件强依赖了固件时，由于固件的限制，会导致软件也变得难以变更，最终让软件变得跟固件一样难以变更。举个软件很容易被“固化”的例子：private OrderDAO orderDAO;public Long addOrder(RequestDTO request) { // 此处省略很多拼装逻辑 OrderDO orderDO = new OrderDO(); orderDAO.insertOrder(orderDO); return orderDO.getId();}public void updateOrder(OrderDO orderDO, RequestDTO updateRequest) { orderDO.setXXX(XXX); // 省略很多 orderDAO.updateOrder(orderDO);}public void doSomeBusiness(Long id) { OrderDO orderDO = orderDAO.getOrderById(id); // 此处省略很多业务逻辑}在上面的这段简单代码里，该对象依赖了DAO，也就是依赖了DB。虽然乍一看感觉并没什么毛病，但是假设未来要加一个缓存逻辑，代码则需要改为如下：private OrderDAO orderDAO;private Cache cache;public Long addOrder(RequestDTO request) { // 此处省略很多拼装逻辑 OrderDO orderDO = new OrderDO(); orderDAO.insertOrder(orderDO); cache.put(orderDO.getId(), orderDO); return orderDO.getId();}public void updateOrder(OrderDO orderDO, RequestDTO updateRequest) { orderDO.setXXX(XXX); // 省略很多 orderDAO.updateOrder(orderDO); cache.put(orderDO.getId(), orderDO);}public void doSomeBusiness(Long id) { OrderDO orderDO = cache.get(id); if (orderDO == null) { orderDO = orderDAO.getOrderById(id); } // 此处省略很多业务逻辑}这时，你会发现因为插入的逻辑变化了，导致在所有的使用数据的地方，都需要从1行代码改为至少3行。而当你的代码量变得比较大，然后如果在某个地方你忘记了查缓存，或者在某个地方忘记了更新缓存，轻则需要查数据库，重则是缓存和数据库不一致，导致bug。当你的代码量变得越来越多，直接调用DAO、缓存的地方越来越多时，每次底层变更都会变得越来越难，越来越容易导致bug。这就是软件被“固化”的后果。所以，我们需要一个模式，能够隔离我们的软件（业务逻辑）和固件/硬件（DAO、DB），让我们的软件变得更加健壮，而这个就是Repository的核心价值。2. 模型对象代码规范2.1 - 对象类型在讲Repository规范之前，我们需要先讲清楚3种模型的区别，Entity、Data Object (DO)和Data Transfer Object (DTO)： Data Object （DO、数据对象）： 实际上是我们在日常工作中最常见的数据模型。但是在DDD的规范里，DO应该仅仅作为数据库物理表格的映射，不能参与到业务逻辑中。为了简单明了，DO的字段类型和名称应该和数据库物理表格的字段类型和名称一一对应，这样我们不需要去跑到数据库上去查一个字段的类型和名称。（当然，实际上也没必要一摸一样，只要你在Mapper那一层做到字段映射） Entity（实体对象）：实体对象是我们正常业务应该用的业务模型，它的字段和方法应该和业务语言保持一致，和持久化方式无关。也就是说，Entity和DO很可能有着完全不一样的字段命名和字段类型，甚至嵌套关系。Entity的生命周期应该仅存在于内存中，不需要可序列化和可持久化。 DTO（传输对象）：主要作为Application层的入参和出参，比如CQRS里的Command、Query、Event，以及Request、Response等都属于DTO的范畴。DTO的价值在于适配不同的业务场景的入参和出参，避免让业务对象变成一个万能大对象。2.2 - 模型对象之间的关系在实际开发中DO、Entity和DTO不一定是1:1:1的关系。一些常见的非1:1关系如下：复杂的Entity拆分多张数据库表：常见的原因在于字段过多，导致查询性能降低，需要将非检索、大字段等单独存为一张表，提升基础信息表的检索效率。常见的案例如商品模型，将商品详细描述等大字段单独保存，提升查询性能：多个关联的Entity合并一张数据库表：这种情况通常出现在拥有复杂的Aggregate Root - Entity关系的情况下，且需要分库分表，为了避免多次查询和分库分表带来的不一致性，牺牲了单表的简洁性，提升查询和插入性能。常见的案例如主子订单模型：从复杂Entity里抽取部分信息形成多个DTO：这种情况通常在Entity复杂，但是调用方只需要部分核心信息的情况下，通过一个小的DTO降低信息传输成本。同样拿商品模型举例，基础DTO可能出现在商品列表里，这个时候不需要复杂详情：合并多个Entity为一个DTO：这种情况通常为了降低网络传输成本，降低服务端请求次数，将多个Entity、DP等对象合并序列化，并且让DTO可以嵌套其他DTO。同样常见的案例是在订单详情里需要展示商品信息：2.3 - 模型所在模块和转化器由于现在从一个对象变为3+个对象，对象间需要通过转化器（Converter/Mapper）来互相转化。而这三种对象在代码中所在的位置也不一样，简单总结如下：DTO Assembler：在Application层，Entity到DTO的转化器有一个标准的名称叫DTO Assembler。Martin Fowler在P of EAA一书里对于DTO 和 Assembler的描述：Data Transfer Object。DTO Assembler的核心作用就是将1个或多个相关联的Entity转化为1个或多个DTO。Data Converter：在Infrastructure层，Entity到DO的转化器没有一个标准名称，但是为了区分Data Mapper，我们叫这种转化器Data Converter。这里要注意Data Mapper通常情况下指的是DAO，比如Mybatis的Mapper。Data Mapper的出处也在P of EAA一书里：Data Mapper如果是手写一个Assembler，通常我们会去实现2种类型的方法，如下；Data Converter的逻辑和此类似，略过。public class DtoAssembler { // 通过各种实体，生成DTO public OrderDTO toDTO(Order order, Item item) { OrderDTO dto = new OrderDTO(); dto.setId(order.getId()); dto.setItemTitle(item.getTitle()); // 从多个对象里取值，且字段名称不一样 dto.setDetailAddress(order.getAddress.getDetail()); // 可以读取复杂嵌套字段 // 省略N行 return dto; } // 通过DTO，生成实体 public Item toEntity(ItemDTO itemDTO) { Item entity = new Item(); entity.setId(itemDTO.getId()); // 省略N行 return entity; }}我们能看出来通过抽象出一个Assembler/Converter对象，我们能把复杂的转化逻辑都收敛到一个对象中，并且可以很好的单元测试。这个也很好的收敛了常见代码里的转化逻辑。在调用方使用时是非常方便的（请忽略各种异常逻辑）：public class Application { private DtoAssembler assembler; private OrderRepository orderRepository; private ItemRepository itemRepository; public OrderDTO getOrderDetail(Long orderId) { Order order = orderRepository.find(orderId); Item item = itemRepository.find(order.getItemId()); return assembler.toDTO(order, item); // 原来的很多复杂转化逻辑都收敛到一行代码了 }}虽然Assembler/Converter是非常好用的对象，但是当业务复杂时，手写Assembler/Converter是一件耗时且容易出bug的事情，所以业界会有多种Bean Mapping的解决方案，从本质上分为动态和静态映射。动态映射方案包括比较原始的BeanUtils.copyProperties、能通过xml配置的Dozer等，其核心是在运行时根据反射动态赋值。动态方案的缺陷在于大量的反射调用，性能比较差，内存占用多，不适合特别高并发的应用场景。所以在这里我给用Java的同学推荐一个库叫MapStruct（MapStruct官网）。MapStruct通过注解，在编译时静态生成映射代码，其最终编译出来的代码和手写的代码在性能上完全一致，且有强大的注解等能力。如果你的IDE支持，甚至可以在编译后看到编译出来的映射代码，用来做check。在这里我就不细讲MapStruct的用法了，具体细节请见官网。用了MapStruct之后，会节省大量的成本，让代码变得简洁如下：@org.mapstruct.Mapperpublic interface DtoAssembler { // 注意这里变成了一个接口，MapStruct会生成实现类 DtoAssembler INSTANCE = Mappers.getMapper(DtoAssembler.class); // 在这里只需要指出字段不一致的情况，支持复杂嵌套 @Mapping(target = &quot;itemTitle&quot;, source = &quot;item.title&quot;) @Mapping(target = &quot;detailAddress&quot;, source = &quot;order.address.detail&quot;) OrderDTO toDTO(Order order, Item item); // 如果字段没有不一致，一行注解都不需要 Item toEntity(ItemDTO itemDTO);}在使用了MapStruct后，你只需要标注出字段不一致的情况，其他的情况都通过Convention over Configuration帮你解决了。还有很多复杂的用法我就不一一指出了。2.4 - 模型规范总结   DO Entity DTO 目的 数据库表映射 业务逻辑 适配业务场景 代码层级 Infrastructure Domain Application 命名规范 XxxDO Xxx XxxDTO XxxCommand XxxRequest等 字段名称标准 数据库表字段名 业务语言 和调用方商定 字段数据类型 数据库字段类型 尽量是有业务含义的类型，比如DP 和调用方商定 是否需要序列化 不需要 不需要 需要 转化器 Data Converter Data Converter DTO Assembler DTO Assembler 从使用复杂度角度来看，区分了DO、Entity、DTO带来了代码量的膨胀（从1个变成了3+2+N个）。但是在实际复杂业务场景下，通过功能来区分模型带来的价值是功能性的单一和可测试、可预期，最终反而是逻辑复杂性的降低。3. Repository代码规范3.1 - 接口规范上文曾经讲过，传统Data Mapper（DAO）属于“固件”，和底层实现（DB、Cache、文件系统等）强绑定，如果直接使用会导致代码“固化”。所以为了在Repository的设计上体现出“软件”的特性，主要需要注意以下三点： 接口名称不应该使用底层实现的语法：我们常见的insert、select、update、delete都属于SQL语法，使用这几个词相当于和DB底层实现做了绑定。相反，我们应该把Repository当成一个中性的类似Collection的接口，使用语法如find、save、remove。在这里特别需要指出的是区分insert/add和update本身也是一种和底层强绑定的逻辑，一些储存如缓存实际上不存在insert和update的差异，在这个case 里，使用中性的save接口，然后在具体实现上根据情况调用DAO的insert或update接口。 出参入参不应该使用底层数据格式：需要记得的是Repository操作的是Entity对象（实际上应该是Aggregate Root），而不应该直接操作底层的DO。更近一步，Repository接口实际上应该存在于Domain层，根本看不到DO的实现。这个也是为了避免底层实现逻辑渗透到业务代码中的强保障。 应该避免所谓的“通用”Repository模式：很多ORM框架都提供一个“通用”的Repository接口，然后框架通过注解自动实现接口，比较典型的例子是Spring Data、Entity Framework等，这种框架的好处是在简单场景下很容易通过配置实现，但是坏处是基本上无扩展的可能性（比如加定制缓存逻辑），在未来有可能还是会被推翻重做。当然，这里避免通用不代表不能有基础接口和通用的帮助类，具体如下。我们先定义一个基础的Repository基础接口类，以及一些Marker接口类：public interface Repository&amp;lt;T extends Aggregate&amp;lt;ID&amp;gt;, ID extends Identifier&amp;gt; { /** * 将一个Aggregate附属到一个Repository，让它变为可追踪。 * Change-Tracking在下文会讲，非必须 */ void attach(@NotNull T aggregate); /** * 解除一个Aggregate的追踪 * Change-Tracking在下文会讲，非必须 */ void detach(@NotNull T aggregate); /** * 通过ID寻找Aggregate。 * 找到的Aggregate自动是可追踪的 */ T find(@NotNull ID id); /** * 将一个Aggregate从Repository移除 * 操作后的aggregate对象自动取消追踪 */ void remove(@NotNull T aggregate); /** * 保存一个Aggregate * 保存后自动重置追踪条件 */ void save(@NotNull T aggregate);}// 聚合根的Marker接口public interface Aggregate&amp;lt;ID extends Identifier&amp;gt; extends Entity&amp;lt;ID&amp;gt; { }// 实体类的Marker接口public interface Entity&amp;lt;ID extends Identifier&amp;gt; extends Identifiable&amp;lt;ID&amp;gt; { }public interface Identifiable&amp;lt;ID extends Identifier&amp;gt; { ID getId();}// ID类型DP的Marker接口public interface Identifier extends Serializable {}业务自己的接口只需要在基础接口上进行扩展，举个订单的例子：// 代码在Domain层public interface OrderRepository extends Repository&amp;lt;Order, OrderId&amp;gt; { // 自定义Count接口，在这里OrderQuery是一个自定义的DTO Long count(OrderQuery query); // 自定义分页查询接口 Page&amp;lt;Order&amp;gt; query(OrderQuery query); // 自定义有多个条件的查询接口 Order findInStore(OrderId id, StoreId storeId);}每个业务需要根据自己的业务场景来定义各种查询逻辑。这里需要再次强调的是Repository的接口是在Domain层，但是实现类是在Infrastructure层。3.2 - Repository基础实现先举个Repository的最简单实现的例子。注意OrderRepositoryImpl在Infrastructure层：// 代码在Infrastructure层@Repository // Spring的注解public class OrderRepositoryImpl implements OrderRepository { private final OrderDAO dao; // 具体的DAO接口 private final OrderDataConverter converter; // 转化器 public OrderRepositoryImpl(OrderDAO dao) { this.dao = dao; this.converter = OrderDataConverter.INSTANCE; } @Override public Order find(OrderId orderId) { OrderDO orderDO = dao.findById(orderId.getValue()); return converter.fromData(orderDO); } @Override public void remove(Order aggregate) { OrderDO orderDO = converter.toData(aggregate); dao.delete(orderDO); } @Override public void save(Order aggregate) { if (aggregate.getId() != null &amp;amp;&amp;amp; aggregate.getId().getValue() &amp;gt; 0) { // update OrderDO orderDO = converter.toData(aggregate); dao.update(orderDO); } else { // insert OrderDO orderDO = converter.toData(aggregate); dao.insert(orderDO); aggregate.setId(converter.fromData(orderDO).getId()); } } @Override public Page&amp;lt;Order&amp;gt; query(OrderQuery query) { List&amp;lt;OrderDO&amp;gt; orderDOS = dao.queryPaged(query); long count = dao.count(query); List&amp;lt;Order&amp;gt; result = orderDOS.stream().map(converter::fromData).collect(Collectors.toList()); return Page.with(result, query, count); } @Override public Order findInStore(OrderId id, StoreId storeId) { OrderDO orderDO = dao.findInStore(id.getValue(), storeId.getValue()); return converter.fromData(orderDO); }}从上面的实现能看出来一些套路：所有的Entity/Aggregate会被转化为DO，然后根据业务场景，调用相应的DAO方法进行操作，事后如果需要则把DO转换回Entity。代码基本很简单，唯一需要注意的是save方法，需要根据Aggregate的ID是否存在且大于0来判断一个Aggregate是否需要更新还是插入。3.3 - Repository复杂实现针对单一Entity的Repository实现一般比较简单，但是当涉及到多Entity的Aggregate Root时，就会比较麻烦，最主要的原因是在一次操作中，并不是所有Aggregate里的Entity都需要变更，但是如果用简单的写法，会导致大量的无用DB操作。举一个常见的例子，在主子订单的场景下，一个主订单Order会包含多个子订单LineItem，假设有个改某个子订单价格的操作，会同时改变主订单价格，但是对其他子订单无影响：如果用一个非常naive的实现来完成，会导致多出来两个无用的更新操作，如下：public class OrderRepositoryImpl extends implements OrderRepository { private OrderDAO orderDAO; private LineItemDAO lineItemDAO; private OrderDataConverter orderConverter; private LineItemDataConverter lineItemConverter; // 其他逻辑省略 @Override public void save(Order aggregate) { if (aggregate.getId() != null &amp;amp;&amp;amp; aggregate.getId().getValue() &amp;gt; 0) { // 每次都将Order和所有LineItem全量更新 OrderDO orderDO = orderConverter.toData(aggregate); orderDAO.update(orderDO); for (LineItem lineItem: aggregate.getLineItems()) { save(lineItem); } } else { // 插入逻辑省略 } } private void save(LineItem lineItem) { if (lineItem.getId() != null &amp;amp;&amp;amp; lineItem.getId().getValue() &amp;gt; 0) { LineItemDO lineItemDO = lineItemConverter.toData(lineItem); lineItemDAO.update(lineItemDO); } else { LineItemDO lineItemDO = lineItemConverter.toData(lineItem); lineItemDAO.insert(lineItemDO); lineItem.setId(lineItemConverter.fromData(lineItemDO).getId()); } }}在这个情况下，会导致4个UPDATE操作，但实际上只需要2个。在绝大部分情况下，这个成本不高，可以接受，但是在极端情况下（当非Aggregate Root的Entity非常多时），会导致大量的无用写操作。3.4 - Change-Tracking 变更追踪在上面那个案例里，核心的问题是由于Repository接口规范的限制，让调用方仅能操作Aggregate Root，而无法单独针对某个非Aggregate Root的Entity直接操作。这个和直接调用DAO的方式很不一样。这个的解决方案是需要能识别到底哪些Entity有变更，并且只针对那些变更过的Entity做操作，就需要加上变更追踪的能力。换一句话说就是原来很多人为判断的代码逻辑，现在可以通过变更追踪来自动实现，让使用方真正只关心Aggregate的操作。在上一个案例里，通过变更追踪，系统可以判断出来只有LineItem2 和 Order 有变更，所以只需要生成两个UPDATE即可。业界有两个主流的变更追踪方案： 基于Snapshot的方案：当数据从DB里取出来后，在内存中保存一份snapshot，然后在数据写入时和snapshot比较。常见的实现如Hibernate 基于Proxy的方案：当数据从DB里取出来后，通过weaving的方式将所有setter都增加一个切面来判断setter是否被调用以及值是否变更，如果变更则标记为Dirty。在保存时根据Dirty判断是否需要更新。常见的实现如Entity Framework。Snapshot方案的好处是比较简单，成本在于每次保存时全量Diff的操作（一般用Reflection），以及保存Snapshot的内存消耗。Proxy方案的好处是性能很高，几乎没有增加的成本，但是坏处是实现起来比较困难，且当有嵌套关系存在时不容易发现嵌套对象的变化（比如子List的增加和删除等），有可能导致bug。由于Proxy方案的复杂度，业界主流（包括EF Core）都在使用Snapshot方案。这里面还有另一个好处就是通过Diff可以发现哪些字段有变更，然后只更新变更过的字段，再一次降低UPDATE的成本。在这里我简单贴一下我们自己Snapshot的实现，代码并不复杂，每个团队自己实现起来也很简单，部分代码仅供参考：DbRepositorySupport// 这个类是一个通用的支撑类，为了减少开发者的重复劳动。在用的时候需要继承这个类public abstract class DbRepositorySupport&amp;lt;T extends Aggregate&amp;lt;ID&amp;gt;, ID extends Identifier&amp;gt; implements Repository&amp;lt;T, ID&amp;gt; { @Getter private final Class&amp;lt;T&amp;gt; targetClass; // 让AggregateManager去维护Snapshot @Getter(AccessLevel.PROTECTED) private AggregateManager&amp;lt;T, ID&amp;gt; aggregateManager; protected DbRepositorySupport(Class&amp;lt;T&amp;gt; targetClass) { this.targetClass = targetClass; this.aggregateManager = AggregateManager.newInstance(targetClass); } /** * 这几个方法是继承的子类应该去实现的 */ protected abstract void onInsert(T aggregate); protected abstract T onSelect(ID id); protected abstract void onUpdate(T aggregate, EntityDiff diff); protected abstract void onDelete(T aggregate); /** * Attach的操作就是让Aggregate可以被追踪 */ @Override public void attach(@NotNull T aggregate) { this.aggregateManager.attach(aggregate); } /** * Detach的操作就是让Aggregate停止追踪 */ @Override public void detach(@NotNull T aggregate) { this.aggregateManager.detach(aggregate); } @Override public T find(@NotNull ID id) { T aggregate = this.onSelect(id); if (aggregate != null) { // 这里的就是让查询出来的对象能够被追踪。 // 如果自己实现了一个定制查询接口，要记得单独调用attach。 this.attach(aggregate); } return aggregate; } @Override public void remove(@NotNull T aggregate) { this.onDelete(aggregate); // 删除停止追踪 this.detach(aggregate); } @Override public void save(@NotNull T aggregate) { // 如果没有ID，直接插入 if (aggregate.getId() == null) { this.onInsert(aggregate); this.attach(aggregate); return; } // 做Diff EntityDiff diff = aggregateManager.detectChanges(aggregate); if (diff.isEmpty()) { return; } // 调用UPDATE this.onUpdate(aggregate, diff); // 最终将DB带来的变化更新回AggregateManager aggregateManager.merge(aggregate); }}使用方只需要继承DbRepositorySupport：public class OrderRepositoryImpl extends DbRepositorySupport&amp;lt;Order, OrderId&amp;gt; implements OrderRepository { private OrderDAO orderDAO; private LineItemDAO lineItemDAO; private OrderDataConverter orderConverter; private LineItemDataConverter lineItemConverter; // 部分代码省略，见上文 @Override protected void onUpdate(Order aggregate, EntityDiff diff) { if (diff.isSelfModified()) { OrderDO orderDO = converter.toData(aggregate); orderDAO.update(orderDO); } Diff lineItemDiffs = diff.getDiff(&quot;lineItems&quot;); if (lineItemDiffs instanceof ListDiff) { ListDiff diffList = (ListDiff) lineItemDiffs; for (Diff itemDiff : diffList) { if (itemDiff.getType() == DiffType.Removed) { LineItem line = (LineItem) itemDiff.getOldValue(); LineItemDO lineDO = lineItemConverter.toData(line); lineItemDAO.delete(lineDO); } if (itemDiff.getType() == DiffType.Added) { LineItem line = (LineItem) itemDiff.getNewValue(); LineItemDO lineDO = lineItemConverter.toData(line); lineItemDAO.insert(lineDO); } if (itemDiff.getType() == DiffType.Modified) { LineItem line = (LineItem) itemDiff.getNewValue(); LineItemDO lineDO = lineItemConverter.toData(line); lineItemDAO.update(lineDO); } } } }}AggregateManager实现，主要是通过ThreadLocal避免多线程公用同一个Entity的情况class ThreadLocalAggregateManager&amp;lt;T extends Aggregate&amp;lt;ID&amp;gt;, ID extends Identifier&amp;gt; implements AggregateManager&amp;lt;T, ID&amp;gt; { private ThreadLocal&amp;lt;DbContext&amp;lt;T, ID&amp;gt;&amp;gt; context; private Class&amp;lt;? extends T&amp;gt; targetClass; public ThreadLocalAggregateManager(Class&amp;lt;? extends T&amp;gt; targetClass) { this.targetClass = targetClass; this.context = ThreadLocal.withInitial(() -&amp;gt; new DbContext&amp;lt;&amp;gt;(targetClass)); } public void attach(T aggregate) { context.get().attach(aggregate); } @Override public void attach(T aggregate, ID id) { context.get().setId(aggregate, id); context.get().attach(aggregate); } @Override public void detach(T aggregate) { context.get().detach(aggregate); } @Override public T find(ID id) { return context.get().find(id); } @Override public EntityDiff detectChanges(T aggregate) { return context.get().detectChanges(aggregate); } public void merge(T aggregate) { context.get().merge(aggregate); }}class DbContext&amp;lt;T extends Aggregate&amp;lt;ID&amp;gt;, ID extends Identifier&amp;gt; { @Getter private Class&amp;lt;? extends T&amp;gt; aggregateClass; private Map&amp;lt;ID, T&amp;gt; aggregateMap = new HashMap&amp;lt;&amp;gt;(); public DbContext(Class&amp;lt;? extends T&amp;gt; aggregateClass) { this.aggregateClass = aggregateClass; } public void attach(T aggregate) { if (aggregate.getId() != null) { if (!aggregateMap.containsKey(aggregate.getId())) { this.merge(aggregate); } } } public void detach(T aggregate) { if (aggregate.getId() != null) { aggregateMap.remove(aggregate.getId()); } } public EntityDiff detectChanges(T aggregate) { if (aggregate.getId() == null) { return EntityDiff.EMPTY; } T snapshot = aggregateMap.get(aggregate.getId()); if (snapshot == null) { attach(aggregate); } return DiffUtils.diff(snapshot, aggregate); } public T find(ID id) { return aggregateMap.get(id); } public void merge(T aggregate) { if (aggregate.getId() != null) { T snapshot = SnapshotUtils.snapshot(aggregate); aggregateMap.put(aggregate.getId(), snapshot); } } public void setId(T aggregate, ID id) { ReflectionUtils.writeField(&quot;id&quot;, aggregate, id); }}跑个单测（注意在这个case里我把Order和LineItem合并单表了）：@Testpublic void multiInsert() { OrderDAO dao = new MockOrderDAO(); OrderRepository repo = new OrderRepositoryImpl(dao); Order order = new Order(); order.setUserId(new UserId(11L)); order.setStatus(OrderState.ENABLED); order.addLineItem(new ItemId(13L), new Quantity(5), new Money(4)); order.addLineItem(new ItemId(14L), new Quantity(2), new Money(3)); System.out.println(&quot;第一次保存前&quot;); System.out.println(order); repo.save(order); System.out.println(&quot;第一次保存后&quot;); System.out.println(order); order.getLineItems().get(0).setQuantity(new Quantity(3)); order.pay(); repo.save(order); System.out.println(&quot;第二次保存后&quot;); System.out.println(order);}单测结果：第一次保存前Order(id=null, userId=11, lineItems=[LineItem(id=null, itemId=13, quantity=5, price=4), LineItem(id=null, itemId=14, quantity=2, price=3)], status=ENABLED)INSERT OrderDO: OrderDO(id=null, parentId=null, itemId=0, userId=11, quantity=0, price=0, status=2)UPDATE OrderDO: OrderDO(id=1001, parentId=1001, itemId=0, userId=11, quantity=0, price=0, status=2)INSERT OrderDO: OrderDO(id=null, parentId=1001, itemId=13, userId=11, quantity=5, price=4, status=2)INSERT OrderDO: OrderDO(id=null, parentId=1001, itemId=14, userId=11, quantity=2, price=3, status=2)第一次保存后Order(id=1001, userId=11, lineItems=[LineItem(id=1002, itemId=13, quantity=5, price=4), LineItem(id=1003, itemId=14, quantity=2, price=3)], status=ENABLED)UPDATE OrderDO: OrderDO(id=1001, parentId=1001, itemId=0, userId=11, quantity=0, price=0, status=3)UPDATE OrderDO: OrderDO(id=1002, parentId=1001, itemId=13, userId=11, quantity=3, price=4, status=3)第二次保存后Order(id=1001, userId=11, lineItems=[LineItem(id=1002, itemId=13, quantity=3, price=4), LineItem(id=1003, itemId=14, quantity=2, price=3)], status=PAID)3.5 - 其他注意事项并发乐观锁在高并发情况下，如果使用上面的Change-Tracking方法，由于Snapshot在本地内存的数据有可能 和DB数据不一致，会导致并发冲突的问题，这个时候需要在更新时加入乐观锁。当然，正常数据库操作的Best Practice应该也要有乐观锁，只不过在这个case 里，需要在乐观锁冲突后，记得更新本地Snapshot里的值。一个可能的BUG这个其实算不上bug，但是单独指出来希望大家能注意一下，使用Snapshot的一个副作用就是如果没更新Entity然后调用了save方法，这时候实际上是不会去更新DB的。这个逻辑跟Hibernate的逻辑一致，是Snapshot方法的天生特性。如果要强制更新到DB，建议手动更改一个字段如gmtModified，然后再调用save。4. Repository迁移路径在我们日常的代码中，使用Repository模式是一个很简单，但是又能得到很多收益的事情。最大的收益就是可以彻底和底层实现解耦，让上层业务可以快速自发展。我们假设现有的传统代码包含了以下几个类（还是用订单举例）： OrderDO OrderDAO可以通过以下几个步骤逐渐的实现Repository模式： 生成Order实体类，初期字段可以和OrderDO保持一致 生成OrderDataConverter，通过MapStruct基本上2行代码就能完成 写单元测试，确保Order和OrderDO之间的转化100%正确 生成OrderRepository接口和实现，通过单测确保OrderRepository的正确性 将原有代码里使用了OrderDO的地方改为Order 将原有代码里使用了OrderDAO的地方都改为用OrderRepository 通过单测确保业务逻辑的一致性。恭喜你！从现在开始Order实体类和其业务逻辑可以随意更改，每次修改你唯一需要做的就是变更一下Converter，已经和底层实现完全解藕了。5. 写在后面感谢你，能有耐心看到这里的都是DDD真爱。一个问题，你是否在日常工作中能大量的利用DDD的架构来推进你的业务？你是否有一个环境能把你的所学用到真正实战中去？我们是阿里巴巴淘系（淘宝+天猫）技术部的行业团队，负责了天猫和淘宝的所有行业垂直业务，比如天猫服饰、淘宝iFashion、消费电子、大快消、企业服务等核心业务，直接对接行业的一线小二、商家和消费者。由于外部竞争环境的激烈，我们的业务也在快速的迭代，需要在保证代码质量的前提下，能够让业务小步快跑、快速上线。这也是为什么我们团队在大量的使用DDD的思想进行开发，并且有一个横向的架构小组来维护我们自己内部用的DDD框架（未来一年内希望能开源）。如果你对我们的工作感兴趣，或者在架构方面有好的想法和建议，欢迎把简历投过来，我们还有大量HC等着你我的邮箱：guangmiao.lgm@alibaba-inc.com" }, { "title": "殷浩详解DDD系列 第二讲 - 应用架构", "url": "/posts/%E6%AE%B7%E6%B5%A9%E8%AF%A6%E8%A7%A3DDD%E7%B3%BB%E5%88%97-%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-03-12 00:00:00 +0800", "snippet": "简介： 架构这个词源于英文里的“Architecture“，源头是土木工程里的“建筑”和“结构”，而架构里的”架“同时又包含了”架子“（scaffolding）的含义，意指能快速搭建起来的固定结构。而今天的应用架构，意指软件系统中固定不变的代码结构、设计模式、规范和组件间的通信方式。在应用开发中架构之所以是最重要的第一步，因为一个好的架构能让系统安全、稳定、快速迭代第二讲 - 应用架构架构这个词源于英文里的“Architecture“，源头是土木工程里的“建筑”和“结构”，而架构里的”架“同时又包含了”架子“（scaffolding）的含义，意指能快速搭建起来的固定结构。而今天的应用架构，意指软件系统中固定不变的代码结构、设计模式、规范和组件间的通信方式。在应用开发中架构之所以是最重要的第一步，因为一个好的架构能让系统安全、稳定、快速迭代。在一个团队内通过规定一个固定的架构设计，可以让团队内能力参差不齐的同学们都能有一个统一的开发规范，降低沟通成本，提升效率和代码质量。在做架构设计时，一个好的架构应该需要实现以下几个目标： 独立于框架：架构不应该依赖某个外部的库或框架，不应该被框架的结构所束缚。 独立于UI：前台展示的样式可能会随时发生变化（今天可能是网页、明天可能变成console、后天是独立app），但是底层架构不应该随之而变化。 独立于底层数据源：无论今天你用MySQL、Oracle还是MongoDB、CouchDB，甚至使用文件系统，软件架构不应该因为不同的底层数据储存方式而产生巨大改变。 独立于外部依赖：无论外部依赖如何变更、升级，业务的核心逻辑不应该随之而大幅变化。 可测试：无论外部依赖了什么数据库、硬件、UI或者服务，业务的逻辑应该都能够快速被验证正确性。这就好像是建筑中的楼宇：一个好的楼宇，无论内部承载了什么人、有什么样的活动、还是外部有什么风雨，一栋楼都应该屹立不倒，而且可以确保它不会倒。但是今天我们在做业务研发时，更多的会去关注一些宏观的架构，比如SOA架构、微服务架构，而忽略了应用内部的架构设计，很容易导致代码逻辑混乱，很难维护，容易产生bug而且很难发现。今天，我希望能够通过案例的分析和重构，来推演出一套高质量的DDD架构。1. 案例分析我们先看一个简单的案例需求如下： 用户可以通过银行网页转账给另一个账号，支持跨币种转账。 同时因为监管和对账需求，需要记录本次转账活动。拿到这个需求之后，一个开发可能会经历一些技术选型，最终可能拆解需求如下： 从MySql数据库中找到转出和转入的账户，选择用MyBatis的mapper实现DAO 从Yahoo（或其他渠道）提供的汇率服务获取转账的汇率信息（底层是http开放接口） 计算需要转出的金额，确保账户有足够余额，并且没超出每日转账上限 实现转入和转出操作，扣除手续费，保存数据库 发送Kafka审计消息，以便审计和对账用而一个简单的代码实现如下：public class TransferController { private TransferService transferService; public Result&amp;lt;Boolean&amp;gt; transfer(String targetAccountNumber, BigDecimal amount, HttpSession session) { Long userId = (Long) session.getAttribute(&quot;userId&quot;); return transferService.transfer(userId, targetAccountNumber, amount, &quot;CNY&quot;); }}public class TransferServiceImpl implements TransferService { private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;; private AccountMapper accountDAO; private KafkaTemplate&amp;lt;String, String&amp;gt; kafkaTemplate; private YahooForexService yahooForex; @Override public Result&amp;lt;Boolean&amp;gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) { // 1. 从数据库读取数据，忽略所有校验逻辑如账号是否存在等 AccountDO sourceAccountDO = accountDAO.selectByUserId(sourceUserId); AccountDO targetAccountDO = accountDAO.selectByAccountNumber(targetAccountNumber); // 2. 业务参数校验 if (!targetAccountDO.getCurrency().equals(targetCurrency)) { throw new InvalidCurrencyException(); } // 3. 获取外部数据，并且包含一定的业务逻辑 // exchange rate = 1 source currency = X target currency BigDecimal exchangeRate = BigDecimal.ONE; if (sourceAccountDO.getCurrency().equals(targetCurrency)) { exchangeRate = yahooForex.getExchangeRate(sourceAccountDO.getCurrency(), targetCurrency); } BigDecimal sourceAmount = targetAmount.divide(exchangeRate, RoundingMode.DOWN); // 4. 业务参数校验 if (sourceAccountDO.getAvailable().compareTo(sourceAmount) &amp;lt; 0) { throw new InsufficientFundsException(); } if (sourceAccountDO.getDailyLimit().compareTo(sourceAmount) &amp;lt; 0) { throw new DailyLimitExceededException(); } // 5. 计算新值，并且更新字段 BigDecimal newSource = sourceAccountDO.getAvailable().subtract(sourceAmount); BigDecimal newTarget = targetAccountDO.getAvailable().add(targetAmount); sourceAccountDO.setAvailable(newSource); targetAccountDO.setAvailable(newTarget); // 6. 更新到数据库 accountDAO.update(sourceAccountDO); accountDAO.update(targetAccountDO); // 7. 发送审计消息 String message = sourceUserId + &quot;,&quot; + targetAccountNumber + &quot;,&quot; + targetAmount + &quot;,&quot; + targetCurrency; kafkaTemplate.send(TOPIC_AUDIT_LOG, message); return Result.success(true); }}我们可以看到，一段业务代码里经常包含了参数校验、数据读取存储、业务计算、调用外部服务、发送消息等多种逻辑。在这个案例里虽然是写在了同一个方法里，在真实代码中经常会被拆分成多个子方法，但实际效果是一样的，而在我们日常的工作中，绝大部分代码都或多或少的接近于此类结构。在Martin Fowler的 P of EAA书中，这种很常见的代码样式被叫做Transaction Script（事务脚本）。虽然这种类似于脚本的写法在功能上没有什么问题，但是长久来看，他有以下几个很大的问题：可维护性差、可扩展性差、可测试性差。问题1 - 可维护性差一个应用最大的成本一般都不是来自于开发阶段，而是应用整个生命周期的总维护成本，所以代码的可维护性代表了最终成本。 可维护性 = 当依赖变化时，有多少代码需要随之改变参考以上的案例代码，事务脚本类的代码很难维护因为以下几点： 数据结构的不稳定性：AccountDO类是一个纯数据结构，映射了数据库中的一个表。这里的问题是数据库的表结构和设计是应用的外部依赖，长远来看都有可能会改变，比如数据库要做Sharding，或者换一个表设计，或者改变字段名。 依赖库的升级：AccountMapper依赖MyBatis的实现，如果MyBatis未来升级版本，可能会造成用法的不同（可以参考iBatis升级到基于注解的MyBatis的迁移成本）。同样的，如果未来换一个ORM体系，迁移成本也是巨大的。 第三方服务依赖的不确定性：第三方服务，比如Yahoo的汇率服务未来很有可能会有变化：轻则API签名变化，重则服务不可用需要寻找其他可替代的服务。在这些情况下改造和迁移成本都是巨大的。同时，外部依赖的兜底、限流、熔断等方案都需要随之改变。 第三方服务API的接口变化：YahooForexService.getExchangeRate返回的结果是小数点还是百分比？入参是（source, target）还是（target, source）？谁能保证未来接口不会改变？如果改变了，核心的金额计算逻辑必须跟着改，否则会造成资损。 中间件更换：今天我们用Kafka发消息，明天如果要上阿里云用RocketMQ该怎么办？后天如果消息的序列化方式从String改为Binary该怎么办？如果需要消息分片该怎么改？我们发现案例里的代码对于任何外部依赖的改变都会有比较大的影响。如果你的应用里有大量的此类代码，你每一天的时间基本上会被各种库升级、依赖服务升级、中间件升级、jar包冲突占满，最终这个应用变成了一个不敢升级、不敢部署、不敢写新功能、并且随时会爆发的炸弹，终有一天会给你带来惊喜。问题2 - 可扩展性差事务脚本式代码的第二大缺陷是：虽然写单个用例的代码非常高效简单，但是当用例多起来时，其扩展性会变得越来越差。 可扩展性 = 做新需求或改逻辑时，需要新增/修改多少代码参考以上的代码，如果今天需要增加一个跨行转账的能力，你会发现基本上需要重新开发，基本上没有任何的可复用性： 数据来源被固定、数据格式不兼容：原有的AccountDO是从本地获取的，而跨行转账的数据可能需要从一个第三方服务获取，而服务之间数据格式不太可能是兼容的，导致从数据校验、数据读写、到异常处理、金额计算等逻辑都要重写。 业务逻辑无法复用：数据格式不兼容的问题会导致核心业务逻辑无法复用。每个用例都是特殊逻辑的后果是最终会造成大量的if-else语句，而这种分支多的逻辑会让分析代码非常困难，容易错过边界情况，造成bug。 逻辑和数据存储的相互依赖：当业务逻辑增加变得越来越复杂时，新加入的逻辑很有可能需要对数据库schema或消息格式做变更。而变更了数据格式后会导致原有的其他逻辑需要一起跟着动。在最极端的场景下，一个新功能的增加会导致所有原有功能的重构，成本巨大。在事务脚本式的架构下，一般做第一个需求都非常的快，但是做第N个需求时需要的时间很有可能是呈指数级上升的，绝大部分时间花费在老功能的重构和兼容上，最终你的创新速度会跌为0，促使老应用被推翻重构。问题3 - 可测试性差除了部分工具类、框架类和中间件类的代码有比较高的测试覆盖之外，我们在日常工作中很难看到业务代码有比较好的测试覆盖，而绝大部分的上线前的测试属于人肉的“集成测试”。低测试率导致我们对代码质量很难有把控，容易错过边界条件，异常case只有线上爆发了才被动发现。而低测试覆盖率的主要原因是业务代码的可测试性比较差。 可测试性 = 运行每个测试用例所花费的时间 * 每个需求所需要增加的测试用例数量参考以上的一段代码，这种代码有极低的可测试性： 设施搭建困难：当代码中强依赖了数据库、第三方服务、中间件等外部依赖之后，想要完整跑通一个测试用例需要确保所有依赖都能跑起来，这个在项目早期是及其困难的。在项目后期也会由于各种系统的不稳定性而导致测试无法通过。 运行耗时长：大多数的外部依赖调用都是I/O密集型，如跨网络调用、磁盘调用等，而这种I/O调用在测试时需要耗时很久。另一个经常依赖的是笨重的框架如Spring，启动Spring容器通常需要很久。当一个测试用例需要花超过10秒钟才能跑通时，绝大部分开发都不会很频繁的测试。 耦合度高：假如一段脚本中有A、B、C三个子步骤，而每个步骤有N个可能的状态，当多个子步骤耦合度高时，为了完整覆盖所有用例，最多需要有N N N个测试用例。当耦合的子步骤越多时，需要的测试用例呈指数级增长。在事务脚本模式下，当测试用例复杂度远大于真实代码复杂度，当运行测试用例的耗时超出人肉测试时，绝大部分人会选择不写完整的测试覆盖，而这种情况通常就是bug很难被早点发现的原因。分析我们重新来分析一下为什么以上的问题会出现？因为以上的代码违背了至少以下几个软件设计的原则： 单一性原则（Single Responsibility Principle）：单一性原则要求一个对象/类应该只有一个变更的原因。但是在这个案例里，代码可能会因为任意一个外部依赖或计算逻辑的改变而改变。 依赖反转原则（Dependency Inversion Principle）：依赖反转原则要求在代码中依赖抽象，而不是具体的实现。在这个案例里外部依赖都是具体的实现，比如YahooForexService虽然是一个接口类，但是它对应的是依赖了Yahoo提供的具体服务，所以也算是依赖了实现。同样的KafkaTemplate、MyBatis的DAO实现都属于具体实现。 开放封闭原则（Open Closed Principle）：开放封闭原则指开放扩展，但是封闭修改。在这个案例里的金额计算属于可能会被修改的代码，这个时候该逻辑应该需要被包装成为不可修改的计算类，新功能通过计算类的拓展实现。我们需要对代码重构才能解决这些问题。2. 重构方案在重构之前，我们先画一张流程图，描述当前代码在做的每个步骤：这是一个传统的三层分层结构：UI层、业务层、和基础设施层。上层对于下层有直接的依赖关系，导致耦合度过高。在业务层中对于下层的基础设施有强依赖，耦合度高。我们需要对这张图上的每个节点做抽象和整理，来降低对外部依赖的耦合度。2.1 - 抽象数据存储层第一步常见的操作是将Data Access层做抽象，降低系统对数据库的直接依赖。具体的方法如下： 新建Account实体对象：一个实体（Entity）是拥有ID的域对象，除了拥有数据之外，同时拥有行为。Entity和数据库储存格式无关，在设计中要以该领域的通用严谨语言（Ubiquitous Language）为依据。 新建对象储存接口类AccountRepository：Repository只负责Entity对象的存储和读取，而Repository的实现类完成数据库存储的细节。通过加入Repository接口，底层的数据库连接可以通过不同的实现类而替换。具体的简单代码实现如下：Account实体类：@Datapublic class Account { private AccountId id; private AccountNumber accountNumber; private UserId userId; private Money available; private Money dailyLimit; public void withdraw(Money money) { // 转出 } public void deposit(Money money) { // 转入 }}和AccountRepository及MyBatis实现类：public interface AccountRepository { Account find(AccountId id); Account find(AccountNumber accountNumber); Account find(UserId userId); Account save(Account account);}public class AccountRepositoryImpl implements AccountRepository { @Autowired private AccountMapper accountDAO; @Autowired private AccountBuilder accountBuilder; @Override public Account find(AccountId id) { AccountDO accountDO = accountDAO.selectById(id.getValue()); return accountBuilder.toAccount(accountDO); } @Override public Account find(AccountNumber accountNumber) { AccountDO accountDO = accountDAO.selectByAccountNumber(accountNumber.getValue()); return accountBuilder.toAccount(accountDO); } @Override public Account find(UserId userId) { AccountDO accountDO = accountDAO.selectByUserId(userId.getId()); return accountBuilder.toAccount(accountDO); } @Override public Account save(Account account) { AccountDO accountDO = accountBuilder.fromAccount(account); if (accountDO.getId() == null) { accountDAO.insert(accountDO); } else { accountDAO.update(accountDO); } return accountBuilder.toAccount(accountDO); }}Account实体类和AccountDO数据类的对比如下： Data Object数据类：AccountDO是单纯的和数据库表的映射关系，每个字段对应数据库表的一个column，这种对象叫Data Object。DO只有数据，没有行为。AccountDO的作用是对数据库做快速映射，避免直接在代码里写SQL。无论你用的是MyBatis还是Hibernate这种ORM，从数据库来的都应该先直接映射到DO上，但是代码里应该完全避免直接操作DO。 Entity实体类：Account是基于领域逻辑的实体类，它的字段和数据库储存不需要有必然的联系。Entity包含数据，同时也应该包含行为。在Account里，字段也不仅仅是String等基础类型，而应该尽可能用上一讲的Domain Primitive代替，可以避免大量的校验代码。DAO和Repository类的对比如下： DAO对应的是一个特定的数据库类型的操作，相当于SQL的封装。所有操作的对象都是DO类，所有接口都可以根据数据库实现的不同而改变。比如，insert 和 update 属于数据库专属的操作。 Repository对应的是Entity对象读取储存的抽象，在接口层面做统一，不关注底层实现。比如，通过 save 保存一个Entity对象，但至于具体是 insert 还是 update 并不关心。Repository的具体实现类通过调用DAO来实现各种操作，通过Builder/Factory对象实现AccountDO 到 Account之间的转化2.1.1 Repository和Entity 通过Account对象，避免了其他业务逻辑代码和数据库的直接耦合，避免了当数据库字段变化时，大量业务逻辑也跟着变的问题。 通过Repository，改变业务代码的思维方式，让业务逻辑不再面向数据库编程，而是面向领域模型编程。 Account属于一个完整的内存中对象，可以比较容易的做完整的测试覆盖，包含其行为。 Repository作为一个接口类，可以比较容易的实现Mock或Stub，可以很容易测试。 AccountRepositoryImpl实现类，由于其职责被单一出来，只需要关注Account到AccountDO的映射关系和Repository方法到DAO方法之间的映射关系，相对于来说更容易测试。2.2 - 抽象第三方服务类似对于数据库的抽象，所有第三方服务也需要通过抽象解决第三方服务不可控，入参出参强耦合的问题。在这个例子里我们抽象出ExchangeRateService的服务，和一个ExchangeRate的Domain Primitive类：【ExchangeRate的实现参考上一篇文章】public interface ExchangeRateService { ExchangeRate getExchangeRate(Currency source, Currency target);}public class ExchangeRateServiceImpl implements ExchangeRateService { @Autowired private YahooForexService yahooForexService; @Override public ExchangeRate getExchangeRate(Currency source, Currency target) { if (source.equals(target)) { return new ExchangeRate(BigDecimal.ONE, source, target); } BigDecimal forex = yahooForexService.getExchangeRate(source.getValue(), target.getValue()); return new ExchangeRate(forex, source, target); }}2.2.1 防腐层（ACL）这种常见的设计模式叫做Anti-Corruption Layer（防腐层或ACL）。很多时候我们的系统会去依赖其他的系统，而被依赖的系统可能包含不合理的数据结构、API、协议或技术实现，如果对外部系统强依赖，会导致我们的系统被”腐蚀“。这个时候，通过在系统间加入一个防腐层，能够有效的隔离外部依赖和内部逻辑，无论外部如何变更，内部代码可以尽可能的保持不变。ACL不仅仅只是多了一层调用，在实际开发中ACL能够提供更多强大的功能： 适配器：很多时候外部依赖的数据、接口和协议并不符合内部规范，通过适配器模式，可以将数据转化逻辑封装到ACL内部，降低对业务代码的侵入。在这个案例里，我们通过封装了ExchangeRate和Currency对象，转化了对方的入参和出参，让入参出参更符合我们的标准。 缓存：对于频繁调用且数据变更不频繁的外部依赖，通过在ACL里嵌入缓存逻辑，能够有效的降低对于外部依赖的请求压力。同时，很多时候缓存逻辑是写在业务代码里的，通过将缓存逻辑嵌入ACL，能够降低业务代码的复杂度。 兜底：如果外部依赖的稳定性较差，一个能够有效提升我们系统稳定性的策略是通过ACL起到兜底的作用，比如当外部依赖出问题后，返回最近一次成功的缓存或业务兜底数据。这种兜底逻辑一般都比较复杂，如果散落在核心业务代码中会很难维护，通过集中在ACL中，更加容易被测试和修改。 易于测试：类似于之前的Repository，ACL的接口类能够很容易的实现Mock或Stub，以便于单元测试。 功能开关：有些时候我们希望能在某些场景下开放或关闭某个接口的功能，或者让某个接口返回一个特定的值，我们可以在ACL配置功能开关来实现，而不会对真实业务代码造成影响。同时，使用功能开关也能让我们容易的实现Monkey测试，而不需要真正物理性的关闭外部依赖。2.3 - 抽象中间件类似于2.2的第三方服务的抽象，对各种中间件的抽象的目的是让业务代码不再依赖中间件的实现逻辑。因为中间件通常需要有通用型，中间件的接口通常是String或Byte[] 类型的，导致序列化/反序列化逻辑通常和业务逻辑混杂在一起，造成胶水代码。通过中间件的ACL抽象，减少重复胶水代码。在这个案例里，我们通过封装一个抽象的AuditMessageProducer和AuditMessage DP对象，实现对底层kafka实现的隔离：@Value@AllArgsConstructorpublic class AuditMessage { private UserId userId; private AccountNumber source; private AccountNumber target; private Money money; private Date date; public String serialize() { return userId + &quot;,&quot; + source + &quot;,&quot; + target + &quot;,&quot; + money + &quot;,&quot; + date; } public static AuditMessage deserialize(String value) { // todo return null; }}public interface AuditMessageProducer { SendResult send(AuditMessage message);}public class AuditMessageProducerImpl implements AuditMessageProducer { private static final String TOPIC_AUDIT_LOG = &quot;TOPIC_AUDIT_LOG&quot;; @Autowired private KafkaTemplate&amp;lt;String, String&amp;gt; kafkaTemplate; @Override public SendResult send(AuditMessage message) { String messageBody = message.serialize(); kafkaTemplate.send(TOPIC_AUDIT_LOG, messageBody); return SendResult.success(); }}具体的分析和2.2类似，在此略过。2.4 - 封装业务逻辑在这个案例里，有很多业务逻辑是跟外部依赖的代码混合的，包括金额计算、账户余额的校验、转账限制、金额增减等。这种逻辑混淆导致了核心计算逻辑无法被有效的测试和复用。在这里，我们的解法是通过Entity、Domain Primitive和Domain Service封装所有的业务逻辑：2.4.1 - 用Domain Primitive封装跟实体无关的无状态计算逻辑在这个案例里使用ExchangeRate来封装汇率计算逻辑：BigDecimal exchangeRate = BigDecimal.ONE;if (sourceAccountDO.getCurrency().equals(targetCurrency)) { exchangeRate = yahooForex.getExchangeRate(sourceAccountDO.getCurrency(), targetCurrency);}BigDecimal sourceAmount = targetAmount.divide(exchangeRate, RoundingMode.DOWN);变为：ExchangeRate exchangeRate = exchangeRateService.getExchangeRate(sourceAccount.getCurrency(), targetMoney.getCurrency());Money sourceMoney = exchangeRate.exchangeTo(targetMoney);2.4.2 - 用Entity封装单对象的有状态的行为，包括业务校验用Account实体类封装所有Account的行为，包括业务校验如下：@Datapublic class Account { private AccountId id; private AccountNumber accountNumber; private UserId userId; private Money available; private Money dailyLimit; public Currency getCurrency() { return this.available.getCurrency(); } // 转入 public void deposit(Money money) { if (!this.getCurrency().equals(money.getCurrency())) { throw new InvalidCurrencyException(); } this.available = this.available.add(money); } // 转出 public void withdraw(Money money) { if (this.available.compareTo(money) &amp;lt; 0) { throw new InsufficientFundsException(); } if (this.dailyLimit.compareTo(money) &amp;lt; 0) { throw new DailyLimitExceededException(); } this.available = this.available.subtract(money); }}原有的业务代码则可以简化为：sourceAccount.deposit(sourceMoney);targetAccount.withdraw(targetMoney);2.4.3 - 用Domain Service封装多对象逻辑在这个案例里，我们发现这两个账号的转出和转入实际上是一体的，也就是说这种行为应该被封装到一个对象中去。特别是考虑到未来这个逻辑可能会产生变化：比如增加一个扣手续费的逻辑。这个时候在原有的TransferService中做并不合适，在任何一个Entity或者Domain Primitive里也不合适，需要有一个新的类去包含跨域对象的行为。这种对象叫做Domain Service。我们创建一个AccountTransferService的类：public interface AccountTransferService { void transfer(Account sourceAccount, Account targetAccount, Money targetMoney, ExchangeRate exchangeRate);}public class AccountTransferServiceImpl implements AccountTransferService { private ExchangeRateService exchangeRateService; @Override public void transfer(Account sourceAccount, Account targetAccount, Money targetMoney, ExchangeRate exchangeRate) { Money sourceMoney = exchangeRate.exchangeTo(targetMoney); sourceAccount.deposit(sourceMoney); targetAccount.withdraw(targetMoney); }}而原始代码则简化为一行：accountTransferService.transfer(sourceAccount, targetAccount, targetMoney, exchangeRate);2.5 - 重构后结果分析这个案例重构后的代码如下：public class TransferServiceImplNew implements TransferService { private AccountRepository accountRepository; private AuditMessageProducer auditMessageProducer; private ExchangeRateService exchangeRateService; private AccountTransferService accountTransferService; @Override public Result&amp;lt;Boolean&amp;gt; transfer(Long sourceUserId, String targetAccountNumber, BigDecimal targetAmount, String targetCurrency) { // 参数校验 Money targetMoney = new Money(targetAmount, new Currency(targetCurrency)); // 读数据 Account sourceAccount = accountRepository.find(new UserId(sourceUserId)); Account targetAccount = accountRepository.find(new AccountNumber(targetAccountNumber)); ExchangeRate exchangeRate = exchangeRateService.getExchangeRate(sourceAccount.getCurrency(), targetMoney.getCurrency()); // 业务逻辑 accountTransferService.transfer(sourceAccount, targetAccount, targetMoney, exchangeRate); // 保存数据 accountRepository.save(sourceAccount); accountRepository.save(targetAccount); // 发送审计消息 AuditMessage message = new AuditMessage(sourceAccount, targetAccount, targetMoney); auditMessageProducer.send(message); return Result.success(true); }}可以看出来，经过重构后的代码有以下几个特征： 业务逻辑清晰，数据存储和业务逻辑完全分隔。 Entity、Domain Primitive、Domain Service都是独立的对象，没有任何外部依赖，但是却包含了所有核心业务逻辑，可以单独完整测试。 原有的TransferService不再包括任何计算逻辑，仅仅作为组件编排，所有逻辑均delegate到其他组件。这种仅包含Orchestration（编排）的服务叫做Application Service（应用服务）。我们可以根据新的结构重新画一张图：然后通过重新编排后该图变为：我们可以发现，通过对外部依赖的抽象和内部逻辑的封装重构，应用整体的依赖关系变了： 最底层不再是数据库，而是Entity、Domain Primitive和Domain Service。这些对象不依赖任何外部服务和框架，而是纯内存中的数据和操作。这些对象我们打包为Domain Layer（领域层）。领域层没有任何外部依赖关系。 再其次的是负责组件编排的Application Service，但是这些服务仅仅依赖了一些抽象出来的ACL类和Repository类，而其具体实现类是通过依赖注入注进来的。Application Service、Repository、ACL等我们统称为Application Layer（应用层）。应用层 依赖 领域层，但不依赖具体实现。 最后是ACL，Repository等的具体实现，这些实现通常依赖外部具体的技术实现和框架，所以统称为Infrastructure Layer（基础设施层）。Web框架里的对象如Controller之类的通常也属于基础设施层。如果今天能够重新写这段代码，考虑到最终的依赖关系，我们可能先写Domain层的业务逻辑，然后再写Application层的组件编排，最后才写每个外部依赖的具体实现。这种架构思路和代码组织结构就叫做Domain-Driven Design（领域驱动设计，或DDD）。所以DDD不是一个特殊的架构设计，而是所有Transction Script代码经过合理重构后一定会抵达的终点。3. DDD的六边形架构在我们传统的代码里，我们一般都很注重每个外部依赖的实现细节和规范，但是今天我们需要敢于抛弃掉原有的理念，重新审视代码结构。在上面重构的代码里，如果抛弃掉所有Repository、ACL、Producer等的具体实现细节，我们会发现每一个对外部的抽象类其实就是输入或输出，类似于计算机系统中的I/O节点。这个观点在CQRS架构中也同样适用，将所有接口分为Command（输入）和Query（输出）两种。除了I/O之外其他的内部逻辑，就是应用业务的核心逻辑。基于这个基础，Alistair Cockburn在2005年提出了Hexagonal Architecture（六边形架构），又被称之为Ports and Adapters（端口和适配器架构）。在这张图中： I/O的具体实现在模型的最外层 每个I/O的适配器在灰色地带 每个Hex的边是一个端口 Hex的中央是应用的核心领域模型在Hex中，架构的组织关系第一次变成了一个二维的内外关系，而不是传统一维的上下关系。同时在Hex架构中我们第一次发现UI层、DB层、和各种中间件层实际上是没有本质上区别的，都只是数据的输入和输出，而不是在传统架构中的最上层和最下层。除了2005年的Hex架构，2008年 Jeffery Palermo的Onion Architecture（洋葱架构）和2017年 Robert Martin的Clean Architecture（干净架构），都是极为类似的思想。除了命名不一样、切入点不一样之外，其他的整体架构都是基于一个二维的内外关系。这也说明了基于DDD的架构最终的形态都是类似的。Herberto Graca有一个很全面的图包含了绝大部分现实中的端口类，值得借鉴。3.1 - 代码组织结构为了有效的组织代码结构，避免下层代码依赖到上层实现的情况，在Java中我们可以通过POM Module和POM依赖来处理相互的关系。通过Spring/SpringBoot的容器来解决运行时动态注入具体实现的依赖的问题。一个简单的依赖关系图如下：3.1.1 - Types模块Types模块是保存可以对外暴露的Domain Primitives的地方。Domain Primitives因为是无状态的逻辑，可以对外暴露，所以经常被包含在对外的API接口中，需要单独成为模块。Types模块不依赖任何类库，纯POJO。3.1.2 - Domain模块Domain模块是核心业务逻辑的集中地，包含有状态的Entity、领域服务Domain Service、以及各种外部依赖的接口类（如Repository、ACL、中间件等。Domain模块仅依赖Types模块，也是纯POJO。3.1.3 - Application模块Application模块主要包含Application Service和一些相关的类。Application模块依赖Domain模块。还是不依赖任何框架，纯POJO。3.1.4 - Infrastructure模块Infrastructure模块包含了Persistence、Messaging、External等模块。比如：Persistence模块包含数据库DAO的实现，包含Data Object、ORM Mapper、Entity到DO的转化类等。Persistence模块要依赖具体的ORM类库，比如MyBatis。如果需要用Spring-Mybatis提供的注解方案，则需要依赖Spring。3.1.5 - Web模块Web模块包含Controller等相关代码。如果用SpringMVC则需要依赖Spring。3.1.6 - Start模块Start模块是SpringBoot的启动类。【每个模块/组件的详细设计规范会在后续文章中详解】3.2 - 测试 Types，Domain模块都属于无外部依赖的纯POJO，基本上都可以100%的被单元测试覆盖。 Application模块的代码依赖外部抽象类，需要通过测试框架去Mock所有外部依赖，但仍然可以100%被单元测试。 Infrastructure的每个模块的代码相对独立，接口数量比较少，相对比较容易写单测。但是由于依赖了外部I/O，速度上不可能很快，但好在模块的变动不会很频繁，属于一劳永逸。 Web模块有两种测试方法：通过Spring的MockMVC测试，或者通过HttpClient调用接口测试。但是在测试时最好把Controller依赖的服务类都Mock掉。一般来说当你把Controller的逻辑都后置到Application Service中时，Controller的逻辑变得极为简单，很容易100%覆盖。 Start模块：通常应用的集成测试写在start里。当其他模块的单元测试都能100%覆盖后，集成测试用来验证整体链路的真实性。【DDD的测试规范在后续文章中详解】3.3 - 代码的演进/变化速度在传统架构中，代码从上到下的变化速度基本上是一致的，改个需求需要从接口、到业务逻辑、到数据库全量变更，而第三方变更可能会导致整个代码的重写。但是在DDD中不同模块的代码的演进速度是不一样的： Domain层属于核心业务逻辑，属于经常被修改的地方。比如：原来不需要扣手续费，现在需要了之类的。通过Entity能够解决基于单个对象的逻辑变更，通过Domain Service解决多个对象间的业务逻辑变更。 Application层属于Use Case（业务用例）。业务用例一般都是描述比较大方向的需求，接口相对稳定，特别是对外的接口一般不会频繁变更。添加业务用例可以通过新增Application Service或者新增接口实现功能的扩展。 Infrastructure层属于最低频变更的。一般这个层的模块只有在外部依赖变更了之后才会跟着升级，而外部依赖的变更频率一般远低于业务逻辑的变更频率。所以在DDD架构中，能明显看出越外层的代码越稳定，越内层的代码演进越快，真正体现了领域“驱动”的核心思想。总结DDD不是一个什么特殊的架构，而是任何传统代码经过合理的重构之后最终一定会抵达的终点。DDD的架构能够有效的解决传统架构中的问题： 高可维护性：当外部依赖变更时，内部代码只用变更跟外部对接的模块，其他业务逻辑不变。 高可扩展性：做新功能时，绝大部分的代码都能复用，仅需要增加核心业务逻辑即可。 高可测试性：每个拆分出来的模块都符合单一性原则，绝大部分不依赖框架，可以快速的单元测试，做到100%覆盖。 代码结构清晰：通过POM module可以解决模块间的依赖关系，所有外接模块都可以单独独立成Jar包被复用。当团队形成规范后，可以快速的定位到相关代码。在后续的文章中会陆续的讲解每个DDD模块的开发规范。" }, { "title": "殷浩详解DDD系列 第一讲 - Domain Primitive", "url": "/posts/%E6%AE%B7%E6%B5%A9%E8%AF%A6%E8%A7%A3DDD%E7%B3%BB%E5%88%97-%E7%AC%AC%E4%B8%80%E8%AE%B2-Domain-Primitive/", "categories": "领域驱动设计DDD", "tags": "DDD", "date": "2022-03-11 00:00:00 +0800", "snippet": "简介： 写在最前面 对于一个架构师来说，在软件开发中如何降低系统复杂度是一个永恒的挑战，无论是94年GoF的Design Patterns，99年的Martin Fowler的Refactoring，02年的P of EAA，还是03年的Enterprise Integration Patterns，都是通过一系列的设计模式或范例来降低一些常见的复杂度。但是问题在于，这些书的理念是通过技术手段解决写在最前面对于一个架构师来说，在软件开发中如何降低系统复杂度是一个永恒的挑战，无论是94年GoF的Design Patterns，99年的Martin Fowler的Refactoring，02年的P of EAA，还是03年的Enterprise Integration Patterns，都是通过一系列的设计模式或范例来降低一些常见的复杂度。但是问题在于，这些书的理念是通过技术手段解决技术问题，但并没有从根本上解决业务的问题。所以03年Eric Evans的Domain Driven Design一书，以及后续Vaughn Vernon的Implementing DDD，Uncle Bob的Clean Architecture等书，真正的从业务的角度出发，为全世界绝大部分做纯业务的开发提供了一整套的架构思路。由于DDD不是一套框架，而是一种架构思想，所以在代码层面缺乏了足够的约束，导致DDD在实际应用中上手门槛很高，甚至可以说绝大部分人都对DDD的理解有所偏差。举个例子，Martin Fowler在他个人博客里描述的一个Anti-pattern，Anemic Domain Model （贫血域模型）在实际应用当中层出不穷，而一些仍然火热的ORM工具比如Hibernate，Entity Framework实际上助长了贫血模型的扩散。同样的，传统的基于数据库技术以及MVC的四层应用架构（UI、Business、Data Access、Database），在一定程度上和DDD的一些概念混淆，导致绝大部分人在实际应用当中仅仅用到了DDD的建模的思想，而其对于整个架构体系的思想无法落地。我第一次接触DDD应该是2012年，当时除了大型互联网公司，基本上商业应用都还处于单机的时代，服务化的架构还局限于单机+LB用MVC提供Rest接口供外部调用，或者用SOAP或WebServices做RPC调用，但其实更多局限于对外部依赖的协议。让我关注到DDD思想的是一个叫Anti-Corruption Layer（防腐层）的概念，特别是其在解决外部依赖频繁变更的情况下，如何将核心业务逻辑和外部依赖隔离的机制。到了2014年，SOA开始大行其道，微服务的概念开始冒头，而如何将一个Monolith应用合理的拆分为多个微服务成为了各大论坛的热门话题，而DDD里面的Bounded Context（限界上下文）的思想为微服务拆分提供了一套合理的框架。而在今天，在一个所有的东西都能被称之为“服务”的时代（XAAS），DDD的思想让我们能冷静下来，去思考到底哪些东西可以被服务化拆分，哪些逻辑需要聚合，才能带来最小的维护成本，而不是简单的去追求开发效率。所以今天，我开始这个关于DDD的一系列文章，希望能继续在总结前人的基础上发扬光大DDD的思想，但是通过一套我认为合理的代码结构、框架和约束，来降低DDD的实践门槛，提升代码质量、可测试性、安全性、健壮性。未来会覆盖的内容包括： 最佳架构实践：六边形应用架构 / Clean架构的核心思想和落地方案 持续发现和交付：Event Storming &amp;gt; Context Map &amp;gt; Design Heuristics &amp;gt; Modelling 降低架构腐败速度：通过Anti-Corruption Layer集成第三方库的模块化方案 标准组件的规范和边界：Entity, Aggregate, Repository, Domain Service, Application Service, Event, DTO Assembler等 基于Use Case重定义应用服务的边界 基于DDD的微服务化改造及颗粒度控制 CQRS架构的改造和挑战 基于事件驱动的架构的挑战 等等第一讲 - Domain Primitive今天先给大家带来一篇最基础，但极其有价值的Domain Primitive的概念.就好像在学任何语言时首先需要了解的是基础数据类型一样，在全面了解DDD之前，首先给大家介绍一个最基础的概念: Domain Primitive（DP）。Primitive的定义是： 不从任何其他事物发展而来 初级的形成或生长的早期阶段就好像Integer、String是所有编程语言的Primitive一样，在DDD里，DP可以说是一切模型、方法、架构的基础，而就像Integer、String一样，DP又是无所不在的。所以，第一讲会对DP做一个全面的介绍和分析，但我们先不去讲概念，而是从案例入手，看看为什么DP是一个强大的概念。1. 案例分析我们先看一个简单的例子，这个case的业务逻辑如下： 一个新应用在全国通过 地推业务员 做推广，需要做一个用户注册系统，同时希望在用户注册后能够通过用户电话（先假设仅限座机）的地域（区号）对业务员发奖金。先不要去纠结这个根据用户电话去发奖金的业务逻辑是否合理，也先不要去管用户是否应该在注册时和业务员做绑定，这里我们看的主要还是如何更加合理的去实现这个逻辑。一个简单的用户和用户注册的代码实现如下：public class User { Long userId; String name; String phone; String address; Long repId;}public class RegistrationServiceImpl implements RegistrationService { private SalesRepRepository salesRepRepo; private UserRepository userRepo; public User register(String name, String phone, String address) throws ValidationException { // 校验逻辑 if (name == null || name.length() == 0) { throw new ValidationException(&quot;name&quot;); } if (phone == null || !isValidPhoneNumber(phone)) { throw new ValidationException(&quot;phone&quot;); } // 此处省略address的校验逻辑 // 取电话号里的区号，然后通过区号找到区域内的SalesRep String areaCode = null; String[] areas = new String[]{&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;}; for (int i = 0; i &amp;lt; phone.length(); i++) { String prefix = phone.substring(0, i); if (Arrays.asList(areas).contains(prefix)) { areaCode = prefix; break; } } SalesRep rep = salesRepRepo.findRep(areaCode); // 最后创建用户，落盘，然后返回 User user = new User(); user.name = name; user.phone = phone; user.address = address; if (rep != null) { user.repId = rep.repId; } return userRepo.save(user); } private boolean isValidPhoneNumber(String phone) { String pattern = &quot;^0[1-9]{2,3}-?\\\\d{8}$&quot;; return phone.matches(pattern); }}我们日常绝大部分代码和模型其实都跟这个是类似的，乍一看貌似没啥问题，但我们再深入一步，从以下四个维度去分析一下：接口的清晰度（可阅读性）、数据验证和错误处理、业务逻辑代码的清晰度、和可测试性。问题1 - 接口的清晰度在Java代码中，对于一个方法来说所有的参数名在编译时丢失，留下的仅仅是一个参数类型的列表，所以我们重新看一下以上的接口定义，其实在运行时仅仅是：User register(String, String, String);所以以下的代码是一段编译器完全不会报错的，很难通过看代码就能发现的bug：service.register(&quot;殷浩&quot;, &quot;浙江省杭州市余杭区文三西路969号&quot;, &quot;0571-12345678&quot;);当然，在真实代码中运行时会报错，但这种bug是在运行时被发现的，而不是在编译时。普通的Code Review也很难发现这种问题，很有可能是代码上线后才会被暴露出来。这里的思考是，有没有办法在编码时就避免这种可能会出现的问题？另外一种常见的，特别是在查询服务中容易出现的例子如下：User findByName(String name);User findByPhone(String phone);User findByNameAndPhone(String name, String phone);在这个场景下，由于入参都是String类型，不得不在方法名上面加上ByXXX来区分，而findByNameAndPhone同样也会陷入前面的入参顺序错误的问题，而且和前面的入参不同，这里参数顺序如果输错了，方法不会报错只会返回null，而这种bug更加难被发现。这里的思考是，有没有办法让方法入参一目了然，避免入参错误导致的bug？问题2 - 数据验证和错误处理在前面这段数据校验代码：if (phone == null || !isValidPhoneNumber(phone)) { throw new ValidationException(&quot;phone&quot;);}在日常编码中经常会出现，一般来说这种代码需要出现在方法的最前端，确保能够fail-fast。但是假设你有多个类似的接口和类似的入参，在每个方法里这段逻辑会被重复。而更严重的是如果未来我们要拓展电话号去包含手机时，很可能需要加入以下代码：if (phone == null || !isValidPhoneNumber(phone) || !isValidCellNumber(phone)) { throw new ValidationException(&quot;phone&quot;);}如果你有很多个地方用到了phone这个入参，但是有个地方忘记修改了，会造成bug。这是一个DRY原则被违背时经常会发生的问题。如果有个新的需求，需要把入参错误的原因返回，那么这段代码就变得更加复杂：if (phone == null) { throw new ValidationException(&quot;phone不能为空&quot;);} else if (!isValidPhoneNumber(phone)) { throw new ValidationException(&quot;phone格式错误&quot;);}可以想像得到，代码里充斥着大量的类似代码块时，维护成本要有多高。最后，在这个业务方法里，会（隐性或显性的）抛ValidationException，所以需要外部调用方去try/catch，而业务逻辑异常和数据校验异常被混在了一起，是否是合理的？在传统Java架构里有几个办法能够去解决一部分问题，常见的如BeanValidation注解或ValidationUtils类，比如：// Use Bean ValidationUser registerWithBeanValidation( @NotNull @NotBlank String name, @NotNull @Pattern(regexp = &quot;^0?[1-9]{2,3}-?\\\\d{8}$&quot;) String phone, @NotNull String address);// Use ValidationUtils:public User registerWithUtils(String name, String phone, String address) { ValidationUtils.validateName(name); // throws ValidationException ValidationUtils.validatePhone(phone); ValidationUtils.validateAddress(address); ...}但这几个传统的方法同样有问题，BeanValidation： 通常只能解决简单的校验逻辑，复杂的校验逻辑一样要写代码实现定制校验器 在添加了新校验逻辑时，同样会出现在某些地方忘记添加一个注解的情况，DRY原则还是会被违背ValidationUtils类： 当大量的校验逻辑集中在一个类里之后，违背了Single Responsibility单一性原则，导致代码混乱和不可维护 业务异常和校验异常还是会混杂所以，有没有一种方法，能够一劳永逸的解决所有校验的问题以及降低后续的维护成本和异常处理成本呢？问题3 - 业务代码的清晰度在这段代码里：String areaCode = null;String[] areas = new String[]{&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;};for (int i = 0; i &amp;lt; phone.length(); i++) { String prefix = phone.substring(0, i); if (Arrays.asList(areas).contains(prefix)) { areaCode = prefix; break; }}SalesRep rep = salesRepRepo.findRep(areaCode);实际上出现了另外一种常见的情况，那就是从一些入参里抽取一部分数据，然后调用一个外部依赖获取更多的数据，然后通常从新的数据中再抽取部分数据用作其他的作用。这种代码通常被称作“胶水代码”，其本质是由于外部依赖的服务的入参并不符合我们原始的入参导致的。比如，如果SalesRepRepository包含一个findRepByPhone的方法，则上面大部分的代码都不必要了。所以，一个常见的办法是将这段代码抽离出来，变成独立的一个或多个方法：private static String findAreaCode(String phone) { for (int i = 0; i &amp;lt; phone.length(); i++) { String prefix = phone.substring(0, i); if (isAreaCode(prefix)) { return prefix; } } return null;}private static boolean isAreaCode(String prefix) { String[] areas = new String[]{&quot;0571&quot;, &quot;021&quot;}; return Arrays.asList(areas).contains(prefix);}然后原始代码变为：String areaCode = findAreaCode(phone);SalesRep rep = salesRepRepo.findRep(areaCode);而为了复用以上的方法，可能会抽离出一个静态工具类PhoneUtils。但是这里要思考的是，静态工具类是否是最好的实现方式呢？当你的项目里充斥着大量的静态工具类，业务代码散在多个文件当中时，你是否还能找到核心的业务逻辑呢？问题4 - 可测试性为了保证代码质量，每个方法里的每个入参的每个可能出现的条件都要有TC覆盖（假设我们先不去测试内部业务逻辑），所以在我们这个方法里需要以下的TC： 条件 入参 name phone address 入参为null ️ ️ ️ 入参为空 ️ ️ ️ 入参不符合要求（可能多个） ️ ️ ️ 假如一个方法有$N$个参数，每个参数有$M$个校验逻辑，至少要有$N * M$ 个TC 。如果这时候在该方法中加入一个新的入参字段fax，即使fax和phone的校验逻辑完全一致，为了保证TC覆盖率，也一样需要$M$个新的TC。而假设有$P$个方法中都用到了phone这个字段，这$P$个方法都需要对该字段进行测试，也就是说整体需要：P∗N∗M个测试用例才能完全覆盖所有数据验证的问题，在日常项目中，这个测试的成本非常之高，导致大量的代码没被覆盖到。而没被测试覆盖到的代码才是最有可能出现问题的地方。在这个情况下，降低测试成本 == 提升代码质量，如何能够降低测试的成本呢？2. 解决方案我们回头先重新看一下原始的use case，并且标注其中可能重要的概念：一个新应用在全国通过 地推业务员 做推广，需要做一个用户的注册系统，在用户注册后能够通过用户电话号的区号对业务员发奖金。在分析了use case后，发现其中地推业务员、用户本身自带ID属性，属于Entity（实体），而注册系统属于Application Service（应用服务），这几个概念已经有存在。但是发现电话号这个概念却完全被隐藏到了代码之中。我们可以问一下自己，取电话号的区号的逻辑是否属于用户（用户的区号？）？是否属于注册服务（注册的区号？）？如果都不是很贴切，那就说明这个逻辑应该属于一个独立的概念。所以这里引入我们第一个原则：Make Implicit Concepts Expecit——将 隐性的概念 显性化在这里，我们可以看到，原来电话号仅仅是用户的一个参数，属于隐形概念，但实际上电话号的区号才是真正的业务逻辑，而我们需要将电话号的概念显性化，通过写一个Value Object：public class PhoneNumber { private final String number; public String getNumber() { return number; } public PhoneNumber(String number) { if (number == null) { throw new ValidationException(&quot;number不能为空&quot;); } else if (isValid(number)) { throw new ValidationException(&quot;number格式错误&quot;); } this.number = number; } public String getAreaCode() { for (int i = 0; i &amp;lt; number.length(); i++) { String prefix = number.substring(0, i); if (isAreaCode(prefix)) { return prefix; } } return null; } private static boolean isAreaCode(String prefix) { String[] areas = new String[]{&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;}; return Arrays.asList(areas).contains(prefix); } public static boolean isValid(String number) { String pattern = &quot;^0?[1-9]{2,3}-?\\\\d{8}$&quot;; return number.matches(pattern); }}这里面有几个很重要的元素： 通过private final String number确保PhoneNumber是一个（Immutable）Value Object。（一般来说VO都是Immutable的，这里只是重点强调一下） 校验逻辑都放在了constructor里面，确保只要PhoneNumber类被创建出来后，一定是校验通过的。 之前的findAreaCode方法变成了PhoneNumber类里的getAreaCode，突出了areaCode是PhoneNumber的一个计算属性。这样做完之后，我们发现把PhoneNumber显性化之后，其实是生成了一个Type（数据类型）和一个Class（类）： Type指我们在今后的代码里可以通过PhoneNumber去显性的标识电话号这个概念 Class指我们可以把所有跟电话号相关的逻辑完整的收集到一个文件里这两个概念加起来，构造成了本文标题的Domain Primitive（DP）。我们看一下全面使用了DP之后效果：public class User { UserId userId; Name name; PhoneNumber phone; Address address; RepId repId;}public User register( @NotNull Name name, @NotNull PhoneNumber phone, @NotNull Address address) { // 找到区域内的SalesRep SalesRep rep = salesRepRepo.findRep(phone.getAreaCode()); // 最后创建用户，落盘，然后返回，这部分代码实际上也能用Builder解决 User user = new User(); user.name = name; user.phone = phone; user.address = address; if (rep != null) { user.repId = rep.repId; } return userRepo.saveUser(user);}我们可以看到在使用了DP之后，所有的数据验证逻辑和非业务流程的逻辑都消失了，剩下都是核心业务逻辑，可以一目了然。我们重新用上面的四个维度评估一下：评估1 - 接口的清晰度重构后的方法签名变成了很清晰的：public User register(Name, PhoneNumber, Address)而之前容易出现的bug，如果按照现在的写法service.register(new Name(&quot;殷浩&quot;), new Address(&quot;浙江省杭州市余杭区文三西路969号&quot;), new PhoneNumber(&quot;0571-12345678&quot;));在编译时就会报错，从而很容易的被及时发现同样的，查询方法可以充分的使用method overloading：User find(Name name);User find(PhoneNumber phone);User find(Name name, PhoneNumber phone);让接口API变得很干净，易拓展。评估2 - 数据验证和错误处理public User register( @NotNull Name name, @NotNull PhoneNumber phone, @NotNull Address address) // no throws如前文代码展示的，重构后的方法里，完全没有了任何数据验证的逻辑，也不会抛ValidationException。原因是因为DP的特性，只要是能够带到入参里的一定是正确的或null（Bean Validation或lombok的注解能解决null的问题）。所以我们把数据验证的工作量前置到了调用方，而调用方本来就是应该提供合法数据的，所以更加合适。再展开来看，使用DP的另一个好处就是代码遵循了DRY原则和单一性原则，如果未来需要修改PhoneNumber的校验逻辑，只需要在一个文件里修改即可，所有使用到了PhoneNumber的地方都会生效。评估3 - 业务代码的清晰度SalesRep rep = salesRepRepo.findRep(phone.getAreaCode());User user = xxx;return userRepo.save(user);除了在业务方法里不需要校验数据之外，原来的一段胶水代码findAreaCode被改为了PhoneNumber类的一个计算属性getAreaCode，让代码清晰度大大提升。而且胶水代码通常都不可复用，但是使用了DP后，变成了可复用、可测试的代码。我们能看到，在刨除了数据验证代码、胶水代码之后，剩下的都是核心业务逻辑。（Entity相关的重构在后面文章会谈到，这次先忽略）评估4 - 可测试性 条件 入参 PhoneNumber phone fax 入参为null ️ ️ ️ 入参为空 ️     入参不符合要求（可能多个） ️     当我们将PhoneNumber抽取出来之后，在来看测试的TC： 首先PhoneNumber本身还是需要$M$个测试用例，但是由于我们只需要测试单一对象，每个用例的代码量会大大降低，维护成本降低。 每个方法里的每个参数，现在只需要覆盖为null的情况就可以了，其他的case不可能发生（因为只要不是null就一定是合法的）所以，单个方法的TC从原来的$N * M$变成了今天的$N + M$。同样的，多个方法的TC数量变成了N+M+P这个数量一般来说要远低于原来的数量$N* M * P$，让测试成本极大的降低。评估总结 维度 传统代码 使用Domain Primitive API接口清晰度 含混不清 接口清晰可读 数据校验、错误处理 校验逻辑分布多个地方，大量重复代码 校验逻辑内聚，在接口边界外完成 业务代码的清晰度 校验代码，胶水代码，业务逻辑混杂 无胶水代码，业务逻辑清晰可读 测试复杂度 N M P N + M + P 其他好处   将隐含的概念显性化 整体安全性大大提升 Immutability不可变 线程安全 3. 进阶使用在上文我介绍了DP的第一个原则：将隐性的概念显性化。在这里我将介绍DP的另外两个原则，用一个新的案例。案例1 - 转账假设现在要实现一个功能，让A用户可以支付x元给用户B，可能的实现如下：public void pay(BigDecimal money, Long recipientId) { BankService.transfer(money, &quot;CNY&quot;, recipientId);}如果这个是境内转账，并且境内的货币永远不变，该方法貌似没啥问题，但如果有一天货币变更了（比如欧元区曾经出现的问题），或者我们需要做跨境转账，该方法是明显的bug，因为money对应的货币不一定是CNY。在这个case里，当我们说“支付x元”时，除了x本身的数字之外，实际上是有一个隐含的概念那就是货币“元”。但是在原始的入参里，之所以只用了BigDecimal的原因是我们认为CNY货币是默认的，是一个隐含的条件，但是在我们写代码时，需要把所有隐性的条件显性化，而这些条件整体组成当前的上下文。所以DP的第二个原则是：Make Implicit Context Expecit ——将 隐性的 上下文 显性化所以当我们做这个支付功能时，实际上需要的一个入参是支付金额 + 支付货币。我们可以把这两个概念组合成为一个独立的完整概念：Money。@Valuepublic class Money { private BigDecimal amount; private Currency currency; public Money(BigDecimal amount, Currency currency) { this.amount = amount; this.currency = currency; }}而原有的代码则变为：public void pay(Money money, Long recipientId) { BankService.transfer(money, recipientId);}通过将默认货币这个隐性的上下文概念显性化，并且和金额合并为Money，我们可以避免很多当前看不出来，但未来可能会暴雷的bug。案例2 - 跨境转账前面的案例升级一下，假设用户可能要做跨境转账从CNY到USD，并且货币汇率随时在波动：public void pay(Money money, Currency targetCurrency, Long recipientId) { if (money.getCurrency().equals(targetCurrency)) { BankService.transfer(money, recipientId); } else { BigDecimal rate = ExchangeService.getRate(money.getCurrency(), targetCurrency); BigDecimal targetAmount = money.getAmount().multiply(new BigDecimal(rate)); Money targetMoney = new Money(targetAmount, targetCurrency); BankService.transfer(targetMoney, recipientId); }}在这个case里，由于targetCurrency不一定和money的Curreny一致，需要调用一个服务去取汇率，然后做计算。最后用计算后的结果做转账。这个case最大的问题在于，金额的计算被包含在了支付的服务中，涉及到的对象也有2个Currency，2个Money，1个BigDecimal，总共5个对象。这种涉及到多个对象的业务逻辑，需要用DP包装掉，所以这里引出DP的第三个原则：Encapsulate Multi-Object Behavior——封装 多对象 行为在这个case 里，可以将转换汇率的功能，封装到一个叫做ExchangeRate的DP里：@Valuepublic class ExchangeRate { private BigDecimal rate; private Currency from; private Currency to; public ExchangeRate(BigDecimal rate, Currency from, Currency to) { this.rate = rate; this.from = from; this.to = to; } public Money exchange(Money fromMoney) { notNull(fromMoney); isTrue(this.from.equals(fromMoney.getCurrency())); BigDecimal targetAmount = fromMoney.getAmount().multiply(rate); return new Money(targetAmount, to); }}ExchangeRate汇率对象，通过封装金额计算逻辑以及各种校验逻辑，让原始代码变得极其简单：public void pay(Money money, Currency targetCurrency, Long recipientId) { ExchangeRate rate = ExchangeService.getRate(money.getCurrency(), targetCurrency); Money targetMoney = rate.exchange(money); BankService.transfer(targetMoney, recipientId);}4. 讨论和总结Domain Primitive的定义让我们重新来定义一下Domain Primitive：Domain Primitive是一个在特定领域里，拥有精准定义的、可自我验证的、拥有行为的Value Object。 DP是一个传统意义上的Value Object，拥有Immutable的特性 DP是一个完整的概念整体，拥有精准定义 DP使用业务域中的原生语言 DP可以是业务域的最小组成部分、也可以构建复杂组合注：Domain Primitive的概念和命名来自于Dan Bergh Johnsson &amp;amp; Daniel Deogun的书 Secure by Design。使用Domain Primitive的三原则 让隐性的概念显性化 让隐性的上下文显性化 封装多对象行为Domain Primitive和DDD里Value Object的区别在DDD中，Value Object这个概念其实已经存在： 在Evans的DDD蓝皮书中，Value Object更多的是一个非Entity的值对象 在Vernon的IDDD红皮书中，作者更多的关注了Value Object的Immutability、Equals方法、Factory方法等Domain Primitive是Value Object的进阶版，在原始VO的基础上要求每个DP拥有概念的整体，而不仅仅是值对象。在VO的Immutable基础上增加了Validity和行为。当然同样的要求无副作用（side-effect free）。Domain Primitive和Data Transfer Object (DTO)的区别在日常开发中经常会碰到的另一个数据结构是DTO，比如方法的入参和出参。DP和DTO的区别如下：   DTO DP 功能 数据传输 属于技术细节 代表业务域中的概念 数据的关联 只是一堆数据放在一起 不一定有关联度 数据之间的高相关性 行为 无行为 丰富的行为和业务逻辑 什么情况下应该用Domain Primitive常见的DP的使用场景包括： 有格式限制的String：比如Name，PhoneNumber，OrderNumber，ZipCode，Address等 有限制的Integer：比如OrderId（&amp;gt;0），Percentage（0-100%），Quantity（&amp;gt;=0）等 可枚举的int：比如Status（一般不用Enum因为反序列化问题） Double或BigDecimal：一般用到的Double或BigDecimal都是有业务含义的，比如Temperature、Money、Amount、ExchangeRate、Rating等 复杂的数据结构：比如Map&amp;lt;String, List&amp;lt;Integer&amp;gt;&amp;gt;等，尽量能把Map的所有操作包装掉，仅暴露必要行为5. 实战 - 老应用重构的流程在新应用中使用DP是比较简单的，但在老应用中使用DP是可以遵循以下流程按部就班的升级。在此用本文的第一个case为例。第一步 - 创建Domain Primitive，收集所有DP行为在前文中，我们发现取电话号的区号这个是一个可以独立出来的、可以放入PhoneNumber这个Class的逻辑。类似的，在真实的项目中，以前散落在各个服务或工具类里面的代码，可以都抽出来放在DP里，成为DP自己的行为或属性。这里面的原则是：所有抽离出来的方法要做到无状态，比如原来是static的方法。如果原来的方法有状态变更，需要将改变状态的部分和不改状态的部分分离，然后将无状态的部分融入DP。因为DP本身不能带状态，所以一切需要改变状态的代码都不属于DP的范畴。(代码参考PhoneNumber的代码，这里不再重复)第二步 - 替换数据校验和无状态逻辑为了保障现有方法的兼容性，在第二步不会去修改接口的签名，而是通过代码替换原有的校验逻辑和根DP相关的业务逻辑。比如：public User register(String name, String phone, String address) throws ValidationException { if (name == null || name.length() == 0) { throw new ValidationException(&quot;name&quot;); } if (phone == null || !isValidPhoneNumber(phone)) { throw new ValidationException(&quot;phone&quot;); } String areaCode = null; String[] areas = new String[]{&quot;0571&quot;, &quot;021&quot;, &quot;010&quot;}; for (int i = 0; i &amp;lt; phone.length(); i++) { String prefix = phone.substring(0, i); if (Arrays.asList(areas).contains(prefix)) { areaCode = prefix; break; } } SalesRep rep = salesRepRepo.findRep(areaCode); // 其他代码...}通过DP替换代码后：public User register(String name, String phone, String address) throws ValidationException { Name _name = new Name(name); PhoneNumber _phone = new PhoneNumber(phone); Address _address = new Address(address); SalesRep rep = salesRepRepo.findRep(_phone.getAreaCode()); // 其他代码...}通过new PhoneNumber(phone)这种代码，替代了原有的校验代码。通过_phone.getAreaCode()替换了原有的无状态的业务逻辑。第三步 - 创建新接口创建新接口，将DP的代码提升到接口参数层：public User register(Name name, PhoneNumber phone, Address address) { SalesRep rep = salesRepRepo.findRep(phone.getAreaCode());}第四步 - 修改外部调用外部调用方需要修改调用链路，比如：service.register(&quot;殷浩&quot;, &quot;0571-12345678&quot;, &quot;浙江省杭州市余杭区文三西路969号&quot;);改为：service.register(new Name(&quot;殷浩&quot;), new PhoneNumber(&quot;0571-12345678&quot;), new Address(&quot;浙江省杭州市余杭区文三西路969号&quot;));通过以上4步，就能让你的代码变得更加简洁、优雅、健壮、安全。你还在等什么？今天就去尝试吧！" }, { "title": "Oracle Apex 19.2 升级至21.2", "url": "/posts/oracle-apex19.2%E5%8D%87%E7%BA%A721.2/", "categories": "Oracle Apex", "tags": "Oracle Apex", "date": "2022-03-10 00:00:00 +0800", "snippet": "两年前按照王方钢 Oracle APEX 系列文章1：Oracle APEX, 让你秒变全栈开发的黑科技 系列文章安装Oracle apex当时Oracle数据库版本为XE-11g，APEX版本为19.2，ORDS版本为18.2。现在截止2022年3月，APEX最新版本为21.2，新版本增添许多强大的功能，所以想对原来的APEX进行升级。APEX 21.2 需要数据库版本至少为12及以上，ORDS版本至少为19及以上，因此也需要同时升级ORACLE 数据库和ORDS。系统版本Centos 7.3Oracle xe-11g --&amp;gt; xe-18gORDS 18.2 --&amp;gt; 21.2APEX 19.2 --&amp;gt; 21.2Oracle 官方都给出了详尽软件升级文档，本教程主要参考官方文档给出。升级前准备备份首先在阿里云做好备份，选用阿里云快照功能对整个磁盘做备份。下载安装包数据库选择XE-18c版本，因为官方支持将APEX数据从XE-11g导入到XE-18c。安装包地址如下： ORACLE 数据库 APEX ORDS cd /u01/media/wget https://download.oracle.com/otn-pub/otn_software/db-express/oracle-database-xe-18c-1.0-1.x86_64.rpmwget https://download.oracle.com/otn_software/apex/apex_21.2.zipwget https://download.oracle.com/otn_software/java/ords/ords-21.4.1.025.0904.zip升级Oracle 数据库至XE 18c从11g导出数据参考文档：Exporting and Importing Data between Oracle Database XE 11.2 and 18c新建数据导出目录su - rootmkdir -p /u01/dump_folder连接数据库进行授权sqlplus &quot;/ AS SYSDBA&quot;SQL&amp;gt; CREATE DIRECTORY DUMP_DIR AS &#39;/u01/dump_folder&#39;;SQL&amp;gt; GRANT READ, WRITE ON DIRECTORY DUMP_DIR TO SYSTEM;SQL&amp;gt; exit导出数据（替换system_password为你的数据库系统管理员密码）expdp system/system_password full=Y directory=DUMP_DIR dumpfile=DB11G.dmp logfile=expdpDB11G.log导出成功后可以看到导出目录下的DB11G.dmp和expdpDB11G.log文件卸载11g数据库卸载数据库需要谨慎，确保自己已经做好数据备份。参考文档： Deinstalling Oracle Database XE执行下面命令进行数据库卸载，会清空一切数据文件和数据库软件，只留下数据库根目录和少许日志。su - rootrpm -e oracle-xe安装18c数据库参考文档： Installing Oracle Database XE Using RPM Packages首先安装预环境设置cd /u01/media/curl -o oracle-database-preinstall-18c-1.0-1.el7.x86_64.rpm https://yum.oracle.com/repo/OracleLinux/OL7/latest/x86_64/getPackage/oracle-database-preinstall-18c-1.0-1.el7.x86_64.rpmyum -y localinstall oracle-database-preinstall-18c-1.0-1.el7.x86_64.rpm安装数据库确保已经下载了xe-18c的安装包到/u01/media目录下yum -y localinstall oracle-database-xe-18c-1.0-1.x86_64.rpm优化内存占用安装完成后，需要初始化数据库。但是这一步特别消耗内存，至少需要1GB空闲内存。如果内存不足很可能中途卡死。可以按以下步骤提高系统可用内存。首先关闭tomcat 和 nginx （以及其他可能占用内存的进程）systemctl stop tomcatsystemctl stop nginx使用Swap 分区，调整swappiness参数vi /etc/sysctl.conf#修改vm.swappiness 参数（0~20之间，鉴于你机器物理内存的大小）vm.swappiness = 20# 保存退出后执行sysctl -p通过free命令查看内存占用情况初始化数据库初始化过程中将输入新数据库的sys、system的密码，记得做好记录保存，之后也要用到。整个流程会比较久，需要耐心等待。/etc/init.d/oracle-xe-18c configure设置oracle 用户环境变量为oracle账号设置环境变量，以便保证每次切换到oracle用户时，都可以直接使用sqlplus等命令su - oracleecho &#39;ORACLE_SID=XE&#39; &amp;gt;&amp;gt; ~/.bash_profileecho &#39;ORAENV_ASK=NO&#39; &amp;gt;&amp;gt; ~/.bash_profileecho &#39;. /opt/oracle/product/18c/dbhomeXE/bin/oraenv&#39; &amp;gt;&amp;gt; ~/.bash_profile测试一下环境变量是否已设置成功。source ~/.bash_profile测试一下数据库是否安装完成。直接用sqlplus连接数据库，用户名输入system，密码输入安装时的密码，看是否能够正常连上数据库。sqlplus导入数据至18c参考文档：Exporting and Importing Data between Oracle Database XE 11.2 and 18c导入数据连接数据库进行授权sqlplus / AS SYSDBASQL&amp;gt; ALTER SESSION SET CONTAINER=XEPDB1;SQL&amp;gt; CREATE DIRECTORY DUMP_DIR AS &#39;/u01/dump_folder&#39;;SQL&amp;gt; GRANT READ, WRITE ON DIRECTORY DUMP_DIR TO SYSTEM;SQL&amp;gt; exit;导入数据，system_password替换为刚刚设置的system 密码。impdp system/system_password@localhost/xepdb1 full=Y REMAP_DIRECTORY=&#39;/u01/app/oracle/oradata/XE/&#39;:&#39;/opt/oracle/oradata/XE/XEPDB1&#39; directory=DUMP_DIR dumpfile=DB11G.dmp logfile=impdpDB11G.log运行数据库导入后脚本下载 https://www.oracle.com/technetwork/developer-tools/apex/application-express/apxfix-5137274.zip and extract the apfix.sql script on your server.cd /u01/mediawget https://www.oracle.com/technetwork/developer-tools/apex/application-express/apxfix-5137274.zip## 解压至apex目录unzip apxfix-5137274.zip /u01/apex连接数据库执行脚本，运行 apxfix.sql，参数是老版本APEX的schema 名称。比如，我apex是19.2版本，那么参数名称是APEX_190200 。sqlplus / AS SYSDBASQL&amp;gt; ALTER SESSION SET CONTAINER=XEPDB1;SQL&amp;gt; @apxfix.sql APEX_190200 SQL&amp;gt; EXIT升级APEX确保已经下载APEX最新版至/u01/media目录安装最新版本APEX将老版本apex 备份mv /u01/apex /u01/apex_19_2解压新版本apexcd /u01/mediamkdir -p /u01/apexunzip apex_20.2.zip -d /u01/chown -R oracle:dba /u01/apex现在新的APEX安装文件已经放在/u01/apex/目录下了，登录数据库执行升级。cd /u01/apex-- 以超级管理员身份登录数据库sqlplus / as sysdba--切换至xepdb1SQL&amp;gt; ALTER SESSION SET CONTAINER=XEPDB1;-- 安装APEX，指定默认表空间和静态文件别名SQL&amp;gt; @apexins.sql SYSAUX SYSAUX TEMP /i/ -- 安装完毕后数据库会话会自动断开，再次以超级管理员身份登录数据库sqlplus / as sysdba-- 创建APEX实例管理员（Instance Administration）及密码，这个密码必须包含特殊符号，否则设置不上。这个密码很重要，是管理APEX平台的账号密码，以后创建新的应用schema、解锁账号等都靠它，第一次登录APEX时也要用到。SQL&amp;gt; @apxchpwd.sql-- 配置RESTful Services服务，记录好配置的两位用户密码SQL&amp;gt; @apex_rest_config.sql-- 禁用数据库内置的PL/SQL网关SQL&amp;gt; exec dbms_xdb.sethttpport(0);SQL&amp;gt; exec dbms_xdb.setftpport(0);-- 解锁ORDS用户账号SQL&amp;gt; alter user apex_public_user account unlock;SQL&amp;gt; alter user apex_public_user identified by &quot;your password&quot;;-- 断开数据库会话SQL&amp;gt; exit复制APEX静态文件到Tomcat目录## 切换到root用户su - root## 删除老版本apex的静态文件rm -rf /u01/tomcat/webapps/i/## 在Tomcat的webapps目录下新建一个名为`i`的文件夹mkdir -p /u01/tomcat/webapps/i/## 将APEX静态文件复制过去cp -a /u01/apex/images/* /u01/tomcat/webapps/i/## 重启Tomcat服务systemctl restart tomcat升级ORDS卸载老版本ORDScd /u01/ordsjava -jar ords.war uninstall输入数据库安装配置 ORDS解压缩安装包mkdir -p /u01/ordsunzip /u01/media/ords-21.4.1.025.0904.zip -d /u01/ords/执行安装脚本cd /u01/ordsjava -jar ords.war install advanced按照提示完成ORDS的安装配置。这里的参数要认真填写。注意database service name 是xepdb1。数据库名称name of the database server，一定要跟数据库监听器里 /opt/app/oracle/product/18c/xe/network/admin/listener.ora 保持一致，否则后面会因为ORDS连接不上数据库，导致访问报错。另外这里要设置好几个数据库账号的密码，建议第一次安装时统一设置成一个，并做好记录，避免后面错乱。若安装出错，重新安装如果配置过程中出现参数配错的情况，或者ords报错的情况，可以重新设置ORDS的各项参数java -jar ords.war setup如果实在不行，就卸载重装java -jar ords.war uninstall java -jar ords.war install advanced为tomcat账号授权（需切换到root用户）确保tomcat账号（安装Tomcat服务器时自动创建的）可以访问/u01/ords/config目录。su - rootchown -R tomcat:tomcat /u01/ords/config将 ords.war 部署到 Tomcat现在我们可以将刚才生成的ords.war文件部署到Tomcat上了。cp -a /u01/ords/ords.war /u01/tomcat/webapps/## 重启Tomcat服务systemctl restart tomcat验证是否正常工作记得启动tomcat和nginx 服务## 重启Tomcat服务systemctl restart tomcat## 重启nginx服务systemctl restart nginx打开浏览器，访问http://your_ip:8080/ords，如果一切正常，应该可以访问到APEX的页面了。后续步骤（可选）安装中文语言包语言安装包在apex/builder/zh-cn下su - oraclecd /u01/apex/builder/zh-cn## 登录数据库sqlplus / as sysdbaSQL&amp;gt; ALTER SESSION SET CONTAINER=XEPDB1;SQL&amp;gt; alter session set current_schema=APEX_210200;SQL&amp;gt; @load_zh-cn.sql重新进入apex选择中文对系统进行优化参考 Oracle APEX 系列文章5：在阿里云上打造属于你自己的APEX完整开发环境 (进一步优化) 对数据库，ORDS，Tomcat，Nginx 进行优化" }, { "title": "软件建模——论文阅读Product Backlog", "url": "/posts/%E8%BD%AF%E4%BB%B6%E5%BB%BA%E6%A8%A1%E8%AF%BE%E7%A8%8B_product_backlog/", "categories": "课程", "tags": "课程, 软件建模", "date": "2022-03-09 00:00:00 +0800", "snippet": "原文：https://ieeexplore.ieee.org/document/8812076脑图论文简述本文的领域是软件开发，本文的研究对象是产品需求列表（Product Backlog），结果是分析了敏捷开发过程的13个实践流程，6个困难风险带点，然后在理论层面给出了产品需求列表的定义，开发过程中的扮演角色，以及何时会产生需求列表。敏捷开发过程 Balanced teams ：敏捷的团队应该由项目经理，产品经理，开发团队组成。 Dual track agile：双轨并行 轨道一主要是是产品经理工作，包括识别与调查利益相关方，绘制界面示意图，编写用户故事 轨道二主要是开发团队工作，包括软件构建，测试，架构设计，重构，部署，运维。 在此之中，项目经理负责调度和制定计划，弥补两者工作的缝隙。 Stakeholder mapping：利益相关方识别，识别那些人是利益相关方（包括用户，产品赞助商等等），以及将利益相关方进行分类。 Interviewing：与不同类别利益相关开会讨论需求 Persona modeling：角色建模，将系统最终用户分成为各类虚拟的角色。 Affinity mapping：用户需求整理，整理来自用户访谈或会议的数据，以产生一些结论和全局的认识。 Design studio ：产品设计，产品经理提出和讨论产品设计方案，在产品概念上达成共识。 Sketching / mockups ：绘制草图和界面示意图 Usability testing / validation testing：对界面示意图作可用性测试 Writing user stories：编写用户故事，use case Story showcase：在敏捷团队中分享用户故事，是每个人理解要做的事 Backlog grooming：对用户故事排优先级，安排开发任务 Accepting stories：编写验收故事，确定用户故事开发后如何进行验证。困难和风险点 Preconceiving Problems：先入为主的问题 Preconceiving Solutions：先入为主的解决方案 Pressure to Converge：项目整合压力 Ambiguity：需求含糊不清 Time Pressure：时间压力 Blocking Access to Users：产品不可用Product Backlog 的理论产品需求列表的定义：描述产品要完成的需求列表，包括优先级，工作量估计，需求的依赖关系以及详细要求。产品需求列表的作用：产品需求列表是一个边界对象，它弥合了实际需求与实际产品的差距，是产品经理和开发团队的中间产品。产品需求列表既不是需求规范，也不是设计规范。产品需求从那些地方产生： 需求调研时：产品经理对需求进行调研时产生需求列表，包括前面提到的与利益相关方讨论，角色建模，用户需求整理等 产品实现时：开发团队开发产品时会产生需求列表，包括重新设计需求，针对开发测试过程提出需求等。 举例：作为开发者的你，开发完整系统需要完成日志、网关、数据库配置等任务。而这些任务对于产品经理是透明的，所以就需要开发者提出这些需求加到排期中。 产品经理与开发团队间的交流（学术名词为边界跨越）：意思时在产品经理与开发团队介绍用户故事、验收故事时，可能产生新的事项需要去做。 举例：作为开发者的你，发现产品经理给的产品设计漏洞百出，或者根本不可能实现。你对其方案进行猛喷，迫使产品经理修改需求或者增加排期 评价优点 全面的介绍了敏捷开发过程的各个实践过程，对没有相关经验的同学是很好的入门引导文章。 文中描述的敏捷过程与实际敏捷开发过程较为符合，有实际工作经验的同学比较容易理解其中的概念，借此可以提升理论水平。 在理论层次对产品需求列表的定义，角色，由来进行了分析，并给出了令人信服结论。缺点/改进 缺少具体案例，有点空中楼阁的味道。如果可以加入更多更详细的具体案例分析就好了。" }, { "title": "SSH学习", "url": "/posts/ssh%E5%AD%A6%E4%B9%A0/", "categories": "学习", "tags": "ssh", "date": "2022-03-08 00:00:00 +0800", "snippet": "SSH学习修改ssh默认端口原因：之前在阿里云的服务器被黑客给攻击了，黑客把我数据库锁了，要我支付0.1个BTC来解锁（我的数据哪里值那么多钱啊~~~）。经过这件事情后，我深刻反思服务器安全问题，以下几点改进： 最最重要的，所有账户不使用弱密码（我就是root账户弱密码） 修改默认ssh端口22，避免被黑客机器人扫到 ssh不允许root直接登录，只能其他用户登录后切换至root。 使用防火墙关闭服务器的不用的端口，只保留必要的少数几个多外端口。 尝试按照安全软件（因影响系统性能而放弃） 做定期备份，利用阿里云快照对整个磁盘备份，出现问题也能回滚减少损失。操作步骤从默认22端口修改至1022端口如果你的系统启用了防火墙服务，首先要把防火墙端口打开。# 开启防火墙端口1022firewall-cmd --add-port=1022/tcp --permanent#重启防火墙service iptables restart开始修改ssh端口# 先备份cp /etc/ssh/sshd_config /etc/ssh/sshd_config_bak# 编辑ssh端口文件，修改Port参数为1022，如下图vim /etc/ssh/sshd_config# 重启ssh服务systemctl restart sshd添加阿里云安全组规则在阿里云上开启TCP协议的1022端口，之后使用新端口就可以连接上服务器了" }, { "title": "Linux命令学习", "url": "/posts/linux%E5%AD%A6%E4%B9%A0/", "categories": "学习", "tags": "linux", "date": "2022-03-01 00:00:00 +0800", "snippet": "Swap 分区的使用swap 分区是磁盘上的一个文件，作用就是，当系统物理内存吃紧时，Linux会将内存中不常访问的数据保存到swap上，这样系统就有更多的物理内存为各个进程服务，而当系统需要访问swap上存储的内容时，再将swap上的数据加载到内存中。查看系统中已经配置的swapswapon -s修改swappinessswappiness的值的大小对如何使用swap分区是有着很大的联系的。swappiness=0的时候表示最大限度使用物理内存，然后才是swap空间，swappiness＝100的时候表示积极的使用swap分区，并且把内存上的数据及时的搬运到swap空间里面。linux的基本默认设置为60，具体如下：cat /proc/sys/vm/swappiness#60也就是说，你的内存在使用到100-60=40%的时候，就开始出现有交换分区的使用。大家知道，内存的速度会比磁盘快很多，这样子会加大系统IO，同时造的成大量页的换进换出，严重影响系统的性能。所以对该参数进行调整。要想永久调整的话，需要在/etc/sysctl.conf修改：sudo vim /etc/sysctl.conf修改vm.swappiness 参数（0~20之间，鉴于你机器物理内存的大小）# Controls the maximum number of shared memory segments, in pagesvm.swappiness = 20运行命令生效sudo sysctl -p这样便完成修改设置！Systemctl 的使用service服务就是在系统中运行的软件，这个软件主要是对外提供某项功能，把这一类软件叫做服务。例如数据库mysqld，tomcat，nginx服务。目前常用的服务管理是systemctl命令（用于取代systemv\\service\\chkconfig命令）常用管理命令# 开机启动systemctl enable mysqld# 关闭开机启动systemctl disable mysqld# 启动服务systemctl start mysqld# 停止服务systemctl stop mysqld# 重启服务systemctl restart mysqld# 查看服务状态systemctl status mysqldsystemctl is-active sshd.service# 结束服务进程(服务无法停止时)systemctl kill mysqld修改配置文件后刷新服务# 重新加载配置文件sudo systemctl daemon-reload# 重启相关服务sudo systemctl restart foobar查看进程 PS 与 TOP 命令使用PS 命令查看进程（静态的）ps ##进程查看工具 -a ##列出所有进程 -x ##与shell无关运行的进程（与shell无关的进程） -u ##查看进程用户 -l ##显示进程的详细信息 -f ##完全信息full的缩写（从属关系） -o ##控制输出，指定信息 -e ##显示系统中的所有进程和a相同（附加的拓展信息） -aux ##显示所有包含其他使用者的行程ps -aux | grep ##进程关键字（进行过滤）ps -aux | less ##ps命令结果较长，可结合less命令ps -aux --sort +或-%cpu或%mem... ##进程按指定方式排序，--sort排序，+降序，-升序常用命令#查看所有用户进程 ps -aux #查看所有java进程ps -aux |grep java #按内存排序查看所有进程ps -aux --sort -%memTOP命令查看进程（动态的）#按cpu排序查看所有进程,top # 按小写m查看总体内存情况m# 按大写M将进程按照内存使用排序M# 按大写P将进程按照内存使用排序P" }, { "title": "Git设置代理解决被墙", "url": "/posts/git%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%E8%A7%A3%E5%86%B3%E8%A2%AB%E5%A2%99/", "categories": "小技巧", "tags": "github, git", "date": "2022-02-19 00:00:00 +0800", "snippet": "经常发现，自己虽然有梯子，可以正常访问github页面，但是在发现“git clone”命令速度特别慢，有时还经常卡掉。本文通过设置git 代理，解决被墙问题。着急的同学直接看第三节-设置ssh代理（终极解决方案）前置要求 首先你得一个可用的梯子（代理） 其次，确认你的梯子的代理端口号比如我用的trojan，在常规设置一栏可以看到Socks5端口号为51837，Http端口号为58591，记下这两个端口号，之后要用到。git 有几种传输协议，在Github上主要用到的是Https和SSH协议。所以我们要做的是对git 命令的https 以及ssh流量做代理。设置https 代理Git代理有两种设置方式，分别是全局代理和只对Github代理。建议只对github 代理代理协议也有两种，分别是使用http代理和使用socks5代理。建议使用socks5代理 注意下面代码的端口号需要根据你自己的代理端口设定，比如我的代理socks端口是51837。全局设置（不推荐）#使用http代理 git config --global http.proxy http://127.0.0.1:58591git config --global https.proxy https://127.0.0.1:58591#使用socks5代理git config --global http.proxy socks5://127.0.0.1:51837git config --global https.proxy socks5://127.0.0.1:51837只对Github代理（推荐）#使用socks5代理（推荐）git config --global http.https://github.com.proxy socks5://127.0.0.1:51837#使用http代理（不推荐）git config --global http.https://github.com.proxy http://127.0.0.1:58591取消代理当你不需要使用代理时，可以取消之前设置的代理。git config --global --unset http.proxy git config --global --unset https.proxy设置ssh代理（终极解决方案）https代理存在一个局限，那就是没有办法做身份验证，每次拉取私库或者推送代码时，都需要输入github的账号和密码，非常痛苦。 设置ssh代理前，请确保你已经设置ssh key。可以参考在 github 上添加 SSH key 完成设置更进一步是设置ssh代理。只需要配置一个config就可以了。# Linux、MacOSvi ~/.ssh/config# Windows 到C:\\Users\\your_user_name\\.ssh目录下，新建一个config文件（无后缀名）将下面内容加到config文件中即可 对于windows用户，注意代理会用到connect.exe，你如果安装了Git都会自带connect.exe，如我的路径为C:\\APP\\Git\\mingw64\\bin\\connect#Windows用户，注意替换你的端口号和connect.exe的路径ProxyCommand &quot;C:\\APP\\Git\\mingw64\\bin\\connect&quot; -S 127.0.0.1:51837 -a none %h %p#MacOS用户用下方这条命令，注意替换你的端口号#ProxyCommand nc -v -x 127.0.0.1:51837 %h %pHost github.com User git Port 22 Hostname github.com # 注意修改路径为你的路径 IdentityFile &quot;C:\\Users\\Your_User_Name\\.ssh\\id_rsa&quot; TCPKeepAlive yesHost ssh.github.com User git Port 443 Hostname ssh.github.com # 注意修改路径为你的路径 IdentityFile &quot;C:\\Users\\Your_User_Name\\.ssh\\id_rsa&quot; TCPKeepAlive yes保存后文件后测试方法，返回successful之类的就成功了。# 测试是否设置成功ssh -T git@github.com之后都推荐走ssh拉取代码，再github 上选择clone地址时，选择ssh地址，入下图。这样git push 与 git clone 都可以直接走代理了，并且不需要输入密码。原理部分代理服务器就是你的电脑和互联网的中介。当您访问外网时（如google.com) , 你的请求首先转发到代理服务器，然后代理服务器替你访问外网，并将结果原封不动的给你的电脑，这样你的电脑就可以看到外网的内容啦。路径如下：你的电脑-&amp;gt;代理服务器-&amp;gt;外网外网-&amp;gt;代理服务器-&amp;gt;你的电脑很多朋友配置代理之后，可以正常访问github 网页了，但是发现在本地克隆github仓库（git clone xxx）时还是报网络错误。那是因为git clone 没有走你的代理，所以需要设置git走你的代理才行。" }, { "title": "在github上添加SSH key", "url": "/posts/%E5%9C%A8github%E4%B8%8A%E6%B7%BB%E5%8A%A0SSH-key/", "categories": "小技巧", "tags": "github, ssh", "date": "2022-02-18 00:00:00 +0800", "snippet": "github每次push 和 clone 时都输入密码，很是繁琐。本文介绍在 github 上添加 SSH key，实现免密登录，不需要每次push 和 clone 时都输入密码。在 github 上添加 SSH key实现免密登录 ：1、检查是否已经有 SSH key运行 git Bash 客户端，输入如下代码：cd ~/.sshls这两个命令就是检查是否已经存在 id_rsa.pub 或 id_dsa.pub 文件，如果文件已经存在，那么你可以跳过步骤2，直接进入步骤3。2、创建新的 SSH keyssh-keygen -t rsa -C &quot;your_email@example.com&quot;代码参数含义： -t 指定密钥类型，默认是 rsa ，可以省略。 -C 设置注释文字，比如邮箱。 -f 指定密钥文件存储文件名。以上代码省略了 -f 参数，因此，运行上面那条命令后会让你输入一个文件名，用于保存刚才生成的 SSH key 代码，如：Generating public/private rsa key pair.# Enter file in which to save the key (/c/Users/you/.ssh/id_rsa): [Press enter]当然，你也可以不输入文件名，使用默认文件名（推荐），那么就会生成 id_rsa 和 id_rsa.pub 两个秘钥文件。接着又会提示你输入两次密码（该密码是你push文件的时候要输入的密码，而不是github管理者的密码），当然，你也可以不输入密码，直接按回车（推荐）。那么push的时候就不需要输入密码，直接提交到github上了，如：Enter passphrase (empty for no passphrase): # Enter same passphrase again:接下来，就会显示如下代码提示，如：Your identification has been saved in /c/Users/you/.ssh/id_rsa.# Your public key has been saved in /c/Users/you/.ssh/id_rsa.pub.# The key fingerprint is:# 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@example.com当你看到上面这段代码就说明，你的 SSH key 已经创建成功，你只需要添加到github的SSH key上就可以了。创建成功3、添加 SSH key 到 github 首先你需要拷贝 id_rsa.pub 文件的内容，你可以用编辑器打开文件复制。 登录你的github账号，从右上角的设置（ Settings ）进入，然后点击菜单栏的 SSH and GPG keys 进入页面。 点击 Add SSH key 按钮添加一个 SSH key 。把你复制的 SSH key 代码粘贴到 key 所对应的输入框中，记得 SSH key 代码的前后不要留有空格或者回车。上面的 Title 所对应的输入框你也可以输入一个该 SSH key 显示在 github 上的一个别名，也可以不输入，默认会使用你的邮件名称。4、验证是否配置成功ssh -T git@github.com成功的话如下所示之后都推荐走ssh拉取代码，再github 上选择clone地址时，选择ssh地址，入下图。这样git push 与 git clone 都可以直接走代理了，并且不需要输入密码。原理部分：Https和 SSH 的区别 Https可以随意克隆github上的项目，而不管是谁的；而SSH 则是你必须是你要克隆的项目的拥有者或管理员，且需要先添加 SSH key ，否则无法克隆。 Https 在push的时候是需要验证用户名和密码的；而 SSH 在push的时候，是不需要输入用户名的，如果配置SSH key的时候设置了密码，则需要输入密码的，否则直接是不需要输入密码的。" }, { "title": "Oracle创建定时任务", "url": "/posts/oracle-apex%E5%88%9B%E5%BB%BA%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/", "categories": "Oracle Apex", "tags": "Oracle Apex", "date": "2021-07-09 00:00:00 +0800", "snippet": "使用Oracle创建定时任务，配合发送邮件代码，实现每日发送汇报邮件。 可以参考 Oracle Apex发送邮件 ，配合本文实现定时发送邮件。创建不带参数Job每天早上8点执行过程DAILY_REPORT，job_id由数据库分配DECLARE job_id number; BEGIN SYS.DBMS_JOB.SUBMIT ( job =&amp;gt; job_id ,what =&amp;gt; &#39;DAILY_REPORT&#39; ,next_date =&amp;gt; sysdate ,interval =&amp;gt; &#39;TRUNC(SYSDATE + 1) + （8*60）/(24*60)&#39; ---每天早上8:00,no_parse =&amp;gt; TRUE ); SYS.DBMS_OUTPUT.PUT_LINE(&#39;Job Number is: &#39; || to_char(job_id)); COMMIT; END; / 创建带参数JobDECLARE job_id number; BEGIN SYS.DBMS_JOB.SUBMIT ( job =&amp;gt; job_id ,what =&amp;gt; &#39;DAILY_REPORT(&quot;1234567@qq.com&quot;)&#39; ,next_date =&amp;gt; sysdate ,interval =&amp;gt; &#39;TRUNC(SYSDATE + 1) + （8*60）/(24*60)&#39; ---每天早上8:00,no_parse =&amp;gt; TRUE ); SYS.DBMS_OUTPUT.PUT_LINE(&#39;Job Number is: &#39; || to_char(job_id)); COMMIT; END; / 对于字符串参数，需要加二个单引号，类似 ‘P_XXX(‘‘参数值’’);Oracle Job 管理查看Jobselect * from user_jobs; --可以查看当前用户所有Jobselect * from all_jobs; --查看所有Job删除Jobbegin dbms_job.remove(12); --12为具体的job ID，可以通过select * from user_jobs查询得到end;立即执行Jobbegin dbms_job.run(12);--运行指定Jobend;执行频率下面是一些常用的执行频率，通过设置Job的interval参数修改 每天运行一次 ‘SYSDATE + 1’ 每小时运行一次 ‘SYSDATE + 1/24’ 每10分钟运行一次‘SYSDATE + 10/（60*24）’ 每30秒运行一次‘SYSDATE + 30/(602460)’ 每隔一星期运行一次‘SYSDATE + 7’ 每个月最后一天运行一次‘TRUNC(LAST_DAY(ADD_MONTHS(SYSDATE,1))) + 23/24’ 每年1月1号零时‘TRUNC(LAST_DAY(TO_DATE(EXTRACT(YEAR FROM SYSDATE)||’12’||’01’,’YYYY-MM-DD’))+1)’ 每天午夜12点‘TRUNC(SYSDATE + 1)’ 每天早上8点30分‘TRUNC(SYSDATE + 1) + (860+30)/(2460)’ 每星期二中午12点‘NEXT_DAY(TRUNC(SYSDATE ), ‘‘TUESDAY’’ ) + 12/24’ 每个月第一天的午夜12点‘TRUNC(LAST_DAY(SYSDATE ) + 1)’ 每个月最后一天的23点‘TRUNC (LAST_DAY (SYSDATE)) + 23 / 24’ 每个季度最后一天的晚上11点‘TRUNC(ADD_MONTHS(SYSDATE + 2/24, 3 ), ‘Q’ ) -1/24’ 每星期六和日早上6点10分‘TRUNC(LEAST(NEXT_DAY(SYSDATE, ‘‘SATURDAY”), NEXT_DAY(SYSDATE, “SUNDAY”))) + (660+10)/(2460)’" }, { "title": "Oracle Apex 手动发送邮件", "url": "/posts/oracle-apex%E6%89%8B%E5%8A%A8%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6/", "categories": "Oracle Apex", "tags": "Oracle Apex", "date": "2021-07-08 00:00:00 +0800", "snippet": "开启ACL权限首先要开启用户的ACL权限（ Access Control List）新建ACL配置文件dbms_network_acl_admin.create_acl(acl =&amp;gt; &#39;email.xml&#39;, --xml名称 DESCRIPTION =&amp;gt; &#39;email&#39;, --描述 principal =&amp;gt; &#39;APEX_190200&#39;, --数据库名,大小写敏感 is_grant =&amp;gt; TRUE, PRIVILEGE =&amp;gt; &#39;connect&#39;, --权限名 start_date =&amp;gt; NULL, end_date =&amp;gt; NULL);ACL分配给用户 dbms_network_acl_admin.add_privilege(acl =&amp;gt; &#39;email.xml&#39;, --同上xml名称 principal =&amp;gt; &#39;APEX_190200&#39;, --数据库名 is_grant =&amp;gt; TRUE, privilege =&amp;gt; &#39;connect&#39;, --权限名 start_date =&amp;gt; null, end_date =&amp;gt; null);ACL注册dbms_network_acl_admin.assign_acl ( -- 该段命令意思是允许访问acl名为utl_sendmail.xml下授权的用户，使用oracle网络访问包，所允许访问的目的主机，及其端口范围。 acl =&amp;gt; &#39;email.xml&#39;, host =&amp;gt; &#39;*&#39; -- ip地址或者域名 );使用下面命令查看是否分配成功SELECT * From dba_network_acls;SELECT acl, principal, privilege, is_grant, TO_CHAR(start_date, &#39;DD-MON-YYYY&#39;) AS start_date, TO_CHAR(end_date, &#39;DD-MON-YYYY&#39;) AS end_dateFrom dba_network_acl_privileges;如果要将权限分配给更多的用户, 执行下面命令EXECUTE DBMS_NETWORK_ACL_ADMIN.ADD_PRIVILEGE(&#39;email.xml&#39;,&#39;APEX_190200&#39;, TRUE, &#39;connect&#39;);发送邮件上面ACL部分就完成了，这时候APEX报错应该就不会再是ORA-24247: 网络访问被访问控制列表 (ACL) 拒绝要使用plsql发送邮件,一般会用到UTL.SMTP和UTL.TCP的函数包。Oracle 默认用户是不能使用这两个函数包的，所以要手动赋予用户权限grant execute on UTL_TCP to APEX_190200;grant execute on UTL_SMTP to APEX_190200;上面APEX的发送邮件配置就完成了，下面是plsql 发送邮件的示例代码,支持HTMLcreate or replace PROCEDURE send_mail (p_to IN VARCHAR2, p_subject IN VARCHAR2, p_text_msg IN VARCHAR2 DEFAULT NULL, p_html_msg IN VARCHAR2 DEFAULT NULL)AS l_mail_conn UTL_SMTP.connection; l_boundary VARCHAR2(50) := &#39;----=*#abc1234321cba#*=&#39;; --下面四个变量请根据实际邮件服务器进行赋值 v_smtphost VARCHAR2(30 ) := &#39;smtp.qq.com&#39;; --SMTP服务器地址(hotmail为smtp.live.com,测试未通过) v_smtpport number(5 ) := 587; --smtp服务端口 v_user VARCHAR2(30 ) := &#39;1509390230@qq.com&#39; ; --登录SMTP服务器的用户名 v_pass VARCHAR2(20 ) := &#39;tqxobadrrneohdah&#39;; --登录SMTP服务器的密码 p_from VARCHAR2(20 ) := &#39;1509390230@qq.com&#39;; BEGIN l_mail_conn := UTL_SMTP.open_connection(v_smtphost, v_smtpport); --是用 ehlo() 而不是 helo() 函数 UTL_SMTP.ehlo(l_mail_conn, v_smtphost); -- smtp服务器登录校验 UTL_SMTP.command(l_mail_conn, &#39;AUTH LOGIN&#39;); UTL_SMTP.command(l_mail_conn,UTL_RAW.cast_to_varchar2(UTL_ENCODE.base64_encode(UTL_RAW.cast_to_raw(v_user)))); UTL_SMTP.command(l_mail_conn,UTL_RAW.cast_to_varchar2(UTL_ENCODE.base64_encode(UTL_RAW.cast_to_raw(v_pass)))); UTL_SMTP.mail(l_mail_conn, p_from); UTL_SMTP.rcpt(l_mail_conn, p_to); UTL_SMTP.open_data(l_mail_conn); UTL_SMTP.write_data(l_mail_conn, &#39;Date: &#39; || TO_CHAR(SYSDATE, &#39;DD-MON-YYYY HH24:MI:SS&#39;) || UTL_TCP.crlf); UTL_SMTP.write_data(l_mail_conn, &#39;To: &#39; || p_to || UTL_TCP.crlf); UTL_SMTP.write_data(l_mail_conn, &#39;From: &#39; || p_from || UTL_TCP.crlf); UTL_SMTP.write_raw_data(l_mail_conn, UTL_RAW.cast_to_raw(&#39;Subject: &#39; || p_subject || UTL_TCP.crlf)); UTL_SMTP.write_data(l_mail_conn, &#39;Reply-To: &#39; || p_from || UTL_TCP.crlf); UTL_SMTP.write_data(l_mail_conn, &#39;MIME-Version: 1.0&#39; || UTL_TCP.crlf); UTL_SMTP.write_data(l_mail_conn, &#39;Content-Type: multipart/alternative; boundary=&quot;&#39; || l_boundary || &#39;&quot;&#39; || UTL_TCP.crlf || UTL_TCP.crlf); IF p_text_msg IS NOT NULL THEN UTL_SMTP.write_data(l_mail_conn, &#39;--&#39; || l_boundary || UTL_TCP.crlf); UTL_SMTP.write_data(l_mail_conn, &#39;Content-Type: text/plain; charset=&quot;utf8&quot;&#39; || UTL_TCP.crlf || UTL_TCP.crlf); UTL_SMTP.write_raw_data(l_mail_conn, UTL_RAW.cast_to_raw(p_text_msg)); UTL_SMTP.write_data(l_mail_conn, UTL_TCP.crlf || UTL_TCP.crlf); END IF; IF p_html_msg IS NOT NULL THEN UTL_SMTP.write_data(l_mail_conn, &#39;--&#39; || l_boundary || UTL_TCP.crlf); UTL_SMTP.write_data(l_mail_conn, &#39;Content-Type: text/html; charset=&quot;utf8&quot;&#39; || UTL_TCP.crlf || UTL_TCP.crlf); UTL_SMTP.write_raw_data(l_mail_conn, UTL_RAW.cast_to_raw(p_html_msg)); UTL_SMTP.write_data(l_mail_conn, UTL_TCP.crlf || UTL_TCP.crlf); END IF; UTL_SMTP.write_data(l_mail_conn, &#39;--&#39; || l_boundary || &#39;--&#39; || UTL_TCP.crlf); UTL_SMTP.close_data(l_mail_conn); UTL_SMTP.quit(l_mail_conn);END;/使用示例DECLARE l_html VARCHAR2(32767);BEGIN l_html := &#39; &amp;lt;title&amp;gt;Test HTML message&amp;lt;/title&amp;gt; &amp;lt;p&amp;gt;This is a &amp;lt;b&amp;gt;HTML&amp;lt;/b&amp;gt; &amp;lt;i&amp;gt;version&amp;lt;/i&amp;gt; of the test message.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;img src=&quot;https://github.com/TjFish/Image-Hosting/raw/master/images/background/bg1.jpg&quot;&amp;gt; &#39;; send_mail(p_to =&amp;gt; &#39;1509390230@qq.com&#39;, p_subject =&amp;gt; &#39;Test Message&#39;, p_text_msg =&amp;gt; &#39;This is a test message.&#39;, p_html_msg =&amp;gt; l_html);END;/" }, { "title": "Oracle Apex 代码获取页面值", "url": "/posts/oracle-apex%E4%BB%A3%E7%A0%81%E8%8E%B7%E5%8F%96%E9%A1%B5%E9%9D%A2%E5%80%BC/", "categories": "Oracle Apex", "tags": "Oracle Apex", "date": "2021-07-06 00:00:00 +0800", "snippet": "Oracle Apex 使用代码（JavaScript和PLSQL）获取并修改页面中各个Item 值。JavaScript取值最基本的方法，使用$v() 取值和 $s()设置项目值，$x()确定当前页面上是否存在具有给定名称的项目$v(&#39;P2_NAME&#39;) //取值-- TjFish$s(&#39;P2_NAME&#39;,&#39;TjDog&#39;) //设置值如果需要访问数组中的多个项目，则$v2()很方便。 例如，可以将复选框或穿梭控件中的多个选择值作为数组提取，下面是示例JavaScript代码Arr = $v2(&quot;P2_SHUTTLE&quot;);for (idx=0; idx&amp;lt;Arr.length; idx++) { //do something with Arr[idx];}其实上述$v() $s() $x() $v2()是apex.item的语法糖如果需要Display值，这时可以用displayValueFor()函数。apex.item(&#39;P2_NAME&#39;).getValue()-- 21 //返回idapex.item(&#39;P2_NAME&#39;).displayValueFor(21)-- TjDog //返回显示值//结合上面两句 apex.item(&#39;P2_NAME&#39;).displayValueFor($v(&#39;P2_NAME&#39;))apex.item(&#39;P2_NAME&#39;).setValue(&#39;Display&#39;,&#39;Return&#39;)PLSQL取值# 在页项前加 &quot;:&quot; 即可取值:P2_NAME# 修改值,修改方法就是PLSQL赋值语句:P2_NAME:=&#39;修改&#39;;" }, { "title": "Oracle Apex 学习路线与安装", "url": "/posts/oracle-apex%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF/", "categories": "Oracle Apex", "tags": "Oracle Apex", "date": "2021-07-03 00:00:00 +0800", "snippet": "简介正常的应用程序一般分为三部分：前端，后端，数据库其工作流程为：前端页面点击，发送Http请求调用后端API，后端查询数据库，返回数据Oracle Apex 同理： 前端页面点击（各个区域，组件，按钮） 执行操作（相当于调用API，这里就是执行一些js/plsql代码，不过大多数区域组件已经预先实现了大多数操作） 查询数据加返回数据（源，SQL，PLSQL）所以学习oracle apex 要从下面几个方面入手Oracle Apex 的安装 Oracle APEX 系列文章1：Oracle APEX, 让你秒变全栈开发的黑科技 Oracle APEX 系列文章2：在阿里云上打造属于你自己的APEX完整开发环境 (准备工作) Oracle APEX 系列文章3：在阿里云上打造属于你自己的APEX完整开发环境 (安装CentOS, Tomcat, Nginx) Oracle APEX 系列文章4：在阿里云上打造属于你自己的APEX完整开发环境 (安装XE, ORDS, APEX) Oracle APEX 系列文章5：在阿里云上打造属于你自己的APEX完整开发环境 (进一步优化)基础部分学习APEX 预先实现的组件的使用这里面包括学习组件的样式（长什么样），功能（有什么用），如何交互，以及如何喂数据 区域（Region） 组件（Item） 按钮（Button） 学习操作操作主要是指动态操作和处理，这里就是自己定义的与用户交互的地方（绝大多数交互由组件已经完成，不过还有一些交互需要自己完成，同时有时我们需要自己定义一些交互）。 动态操作（Dynamic Action） 处理（Processing） js/plsql 自定义交互 查询数据加返回数据这一部分是使用组件必然需要用到的，主要是需要填充区域/组件 的源（Source）属性由于使用比较频繁，所以单拉出来学习。 数据库表/视图 （Tabel /View） 共享组件 （Share Component） SQL 查询 （SQL Query） 静态值 （Static Value）所以整个工作流程就是，选择预先定义的区域/组件，填充源（喂数据），定义动态操作（可选，比如页面间的跳转）进阶部分学完基本部分，对Oracle Apex 的基本工作流程有了了解，后面是一些进阶操作，可以提高开发效率，完善功能（导航/面包屑，SQL工作室，插件，使用第三方js/css，数据导入功能，图表）包括以下方面SQL工作室sql工作室使得我们可以点点点来进行数据库操作（DDL），从而极大提高开发效率。在此其中，对象浏览器经常使用，对象浏览器 创建 表/视图/触发器 修改 表/视图/触发器 定义 查看 表/视图 数据SQL命令 用来直接执行sql语句，通常用来验证自己的sql语句是否正确，是否查询出想要的值 保存sql 语句导航与面包屑导航与面包屑部分关乎用户的体验，一般的后台管理界面都有侧边导航栏。因此，如何定义导航栏和面包屑，在应用程序构建器 -&amp;gt; 选择应用 -&amp;gt; 共享组件 -&amp;gt; 导航部分中的导航菜单和面包屑。在这里面可以自己定义网页的导航栏和面包屑使用第三方js/css现在有许多功能十分强大的JS代码库，Oracle Apex 可以很方便的集成它们。要使用第三方JS 代码，在应用程序构建器 -&amp;gt; 选择应用 -&amp;gt; 共享组件 -&amp;gt; 文件部分中的静态工作区/应用程序文件导入希望引入的js文件，导入成功后，复制引用 部分（比如#WORKSPACE_IMAGES#canvas2image.js）将引用字符串，粘贴到想要引入js文件的页面属性 JavaScript 文件URL中，注意不要多空格，每个文件一行引用。之后在本页面就可以直接使用引入的js 文件中的函数了。css文件同理插件有许多朋友开发了apex的插件（插件就相当于扩展apex默认提供的区域/组件） apexworld 是关于apex的开源社区，里面提供了许多apex的插件。下载插件后，导入插件也很简单，流程和导入应用一样。 ronnyweiss APEXbyG.blogspot.com开源应用 官方应用 Apex 博客 Awesome Oracle APEX 更多学习资源 [Oracle Apex教程 Vinish Kapoor的博客](https://www.foxinfotech.in/2019/11/oracle-apex-tutorials-version-19-1-onwards.html) Ottmar with APEX [Oracle APEX 系列文章10：Oracle APEX Evangelion 钢钢更新](https://wangfanggang.com/Oracle/Oracle-APEX/apex-series-10/) https://github.com/Dani3lSun/awesome-orclapex#apps" }, { "title": "面试题目总结", "url": "/posts/%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/", "categories": "学习", "tags": "面试", "date": "2021-06-08 00:00:00 +0800", "snippet": "本文主要是为了总结记录下我的秋招之路，毕竟现在才发面经，也太特么晚了吧。当然，如果能为他人提供哪怕微微的帮助，我也会非常高兴。先做自我介绍，研究生，方向Java开发，无实习，无项目，无获奖，无竞赛，成绩差，基础差，准备时间晚，真菜得一笔，仅有的优势就是学历还行，起码不会被卡学历。另外，心态好，学习能力强，面经看得多，个人总结到位了。最后的结果超出预期的好，毕竟我的预期只是年薪三十万，达到街薪即可。行文主要分为三个部分：个人历程、基础面经总结以及手撕代码总结。作者：渺万里层云.原文链接：https://www.nowcoder.com/discuss/344311?type=2&amp;amp;order=0&amp;amp;pos=18&amp;amp;page=5&amp;amp;channel=666&amp;amp;source_id=discuss_tag来源：牛客网 个人历程 首先，我犯了一个极其严重的错误，那就是准备的太晚，没敢参加提前批，好多公司提前批都已经招得差不多了，正式批hc太少了。其次，实验室不让实习，没有实习经历，也没有可深谈的项目，太不利于秋招了。室友都是微软实习转正，太羡慕嫉妒了。我时常在想，如果我曾实习过，是不是会顺利很多？ 我正式参加简历投递已经是九月初了。简历投了三四十，笔试做了二三十。印象深刻的有两天，其中一天堆了7个笔试，我做了5个。还有一天，4个笔试加一个面试，而且其中两个笔试是并行的。并行的意思是，贝壳宣讲会和华为笔试都是晚上7点，所以我带着笔记本去参加宣讲会， 华为笔试40分钟提前交卷，然后接着写贝壳的现场笔试。我自认为笔试做得还行，但面试真的没几个，也就京东(三面加一块不到一小时)、猿辅导(二面代码写得慢挂)、美团(二面莫名其妙挂)、bigo(一面挂， 女面试官下手太狠了)、cvte(面试自我感觉良好，然后反手被挂)、滴滴(三面因为一个非常简单的问题被挂，太特么丢人了，就不说了)。这是我国庆之前的情况，非常的失落以及慌张，感觉大厂都不怎么招人了，小厂只笔试不面试，有面试也是刷KPI的。最让我烦的是快手，我笔试全对，然后挂了。哥们听我遭遇之后，笔试干脆不做，然后快手邀他去面试，这特么跟谁说理去？这时，我甚至已经做好了春招再战的准备。 我真的非常感谢字节跳动。大家都说头条早已招满，所以即使头条连续给我发两次笔试链接，我都选择了其他家的笔试。然而他们依然打电话约面试。我将我秋招最后也是最大的希望都放在了头条上。国庆期间，我在牛客网上刷了很多头条的面经，把基础题和代码题全部总结了下， 不会的全记录下来慢慢解决。其实，国庆七天也就努力了三天，后四天全玩了……国庆之后的那周，是我最努力的一周。因为，一起抱团取暖、同样苦比的哥们被头条收了，只剩我一人继续苦比了。老哥回头望，笑我还不快跟上。好吧，努力去跟上。那周，我就憋着一股气去学习，去面试，就为了通过下周一的头条面试。那周，我依次面了拼多多一面，招行三面，度小满三面，自我感觉都挺好的。在此我感谢度小满，他给了我很大的信心。度小满三面都是技术面，难度都不低，面试官人都很好。二面面试官在肯定了我之后，告诫我要注意表达，虽然技术面不怎么看注重表达能力，但第三面会考察，要注意一下。同时又指点了我个人学习要注意的一些事项。第三面面试官，嚯~~~~~~，特别漂亮的女面试官。漂亮且气质，用一个词来形容就是精致，精致的面孔，精致的发型，优雅的谈吐。而且还是个技术大拿。她给我讲了10多分钟的金融知识，在得知我无offer的慌张后，她肯定了我的水平，说我的情况拿五个完全没问题，根本不用慌。并劝告我，以后在选择offer的时候，要注重个人的发展，一定要先搞清楚自己感兴趣的方向是什么，不要什么都没弄明白就一股脑扎进去。然后就是字节跳动抖音部门的面试，三面下来，又褪了层皮，感觉再也不想面试了，心累了。度小满加抖音这两天六面，真的耗光了我所有的精力。幸运的是，第二天抖音hr就微信偷偷告诉我面试通过了，慢慢等oc吧。然后我就傲娇得拒绝了云从、奇安信、小米、贝壳的面试。抱着爱咋咋地得悠然心态，走完了拼多多、华为、有道的面试流程。 嗯，最后我选择了拼多多。 基础总结 关于一面二面的基础部分，只要准备的足够充分，百分之九十的问题都能完全答对。这里面真的丝毫技术含量都没有，全靠背。在我所有的面试里，只有头条二面是靠背解决不了的，因为他总能精准找到你的知识盲区，明明这块我自认为复习的足够全面，但依然被他怼得说不出话来。即使再给我两星期时间准备，我也很难复习到他所问的那些点。但比较好的是，准备得多了，即使被抓着盲点怼，也能够现场尝试给出猜测，不管对不对，起码能体现出相关知识储备以及思考能力。关于基础知识的总结，网上有很多，最好的就是github上JavaGuide那老哥。我的建议是，将他整理的PDF打印出来，好好看好好学。最后，再按照知识点用自己的语言习惯总结一下，务必简洁扼要有序，面试的时候直接按照总结的背起就行。他总结的并不能完全覆盖所有面试点，这就需要自己去不断的看别人的面经，整理一下他人被提问的问题，对于那些不会的、说不清的又很重要的问题，去网上搜答案，看懂之后再自我总结。不断的在面试和面经中，迭代自己的知识点总结。下面我分类将我认为比较重要的问题列出来。网络： 1、OSI七层模型与TCP/IP 五层模型, 2、常见应用层协议和运输层、网络层协议，以及硬件如路由器之类在哪一层 3、TCP与UDP区别和应用场景，基于TCP的协议有哪些，基于UDP的有哪些 （DNS， OICQ） 4、TCP可靠传输的保证，拥塞控制目和过程 5、TCP粘包现象原因和解决方法 6、TCP三次握手过程以及每次握手后的状态改变，为什么三次？为什么两次不行？如果你的答案是防止已失效的请求报文又传送到了服务端，建立了多余的链接，浪费资源，但这个答案被否定了，你还能给出什么答案？ 7、TCP四次挥手过程以及状态改变，为什么四次？CLOSE-WAIT和TIME-WAIT存在的意义？如何查看TIME-WAIT状态的链接数量？为什么会TIME-WAIT过多？解决方法是怎样的？ 8、TCP、UDP、IP、以太网报文格式以及重要字段，报文从一端到另一端传递的过程。 9、浏览器输入URL并回车的过程以及相关协议，DNS查询过程。 10、HTTP1.0、1.1、2.0之间的区别 11、HTTP与HTTPS之间的区别，HTTPS链接建立的过程，了解对称加密算法 AES，DES 和非对称加密算法RSA（ 大整数的因式分解）DSA？ 12、HTTP请求有哪些，多说点。Post和get区别。 （get，post，head，put，delete，options，connect，trace 13、HTTP常见响应状态码，从1xx到5xx都要说。如304,301,302，504， 100 继续 101 切换协议 200 OK，201 Created 202 Accepted（异步api），203 204 No Content （表示内容没更新） 301(永久移动) 302（临时移动） 304（未修改） 400 Bad Request 401 未授权 403 禁止（无权限） 404 Not Found 405 方法不允许 500 服务器内部错误 502（上游服务器Bad） 503 服务暂时不可用 504 网关超时（上游服务器超时） 505 http版本不支持 14、重定向和转发区别 重定向是客户端行为，转发是服务端行为。 15、cookie和session区别。操作系统： 1、进程和线程的区别 2、协程呢？ 3、进程间通信方式IPC 4、用户态和核心态 5、操作系统分配的进程空间是怎样的？线程能共享哪些？ 6、操作系统内存管理方式，分页分段以及段页式的优缺点 （ MMU，TLB） 7、页面置换算法有哪些，FIFO为什么不好？如何改进?LRU思想，手写LRU（FIFO先进先出，LRU 最近最久未使用，LFU Least Frequently Used最少使用） 8、死锁条件，解决方式。Java基础 1、面向对象特性介绍、与C++区别 封装继承多态 Java 没有指针，C++有指针概念 Java有GC，C++没有 Java只能单继承，C++可以多继承 Java字符串不可更改，C++ 字符串以’\\0’结尾 2、多态实现原理 接口 和 继承 3、抽象类和接口区别，以及各自的使用场景 4、泛型以及泛型擦除。List类型的list,可以加入无继承关系的B类型对象吗？如何加入？ Java泛型是伪泛型，在编译之后会泛型擦除，去掉所有泛型的信息。编译后 List -&amp;gt; List 通过反射机制在运行时调用add方法，将对象加入。 5、Java异常体系 Throwable是整个异常体系的父类，有两个子类Error 和Exception，Error是致命错误，一般会终止线程。Exception 下分为运行时异常和检查性异常。对于运行时异常可以不捕获 如 Nullpointer Exception，对于checked 异常必须显示处理异常，不然编译不通过 如IOException。 6、反射原理以及使用场景 7、ThreadLocal原理，如何使用？ 8、内存泄漏的场景 内存泄露最常见的场景是集合类的使用，即使集合内部元素不在使用，当集合本身没有释放，导致集合内部元素都不能释放，从而内存泄露 另外还有数据库连接，网络IO没有正常关闭都会导致内存泄露 还有一些比如不正确的使用单例模式，因为单例一般存活周期非常长，如果单例内部有其他对象的引用，其他对象就不能释放。 9、static关键字和final关键字使用情况，一个类不能被继承，除了final关键字之外，还有什么方法（从构造函数考虑）？ 如果类中只有private的构造方法，那么此类不可以被继承 10、序列化和反序列化。反序列化失败的场景。 序列化就是将对象转换为字节编码，反序列化就是将字节编码构建为对象。 不希望序列化的成员加上transient 关键字 一般需要序列化的对象都手动管理serialVersionUID，再对类进行了更改升级后需要手动修改serialVersionUID。（修改serialVersionUID 要谨慎，如果只是新增/删除了属性，不需要修改serialVersionUID，如果是修改了原有属性，需要修改） 11、ArrayList和LinkedList的区别和底层实现？如何实现线程安全？ 12、List遍历时如何删除元素？fail—fast是什么？fail—safe是什么？ 通过迭代器遍历删除 fail-fast 是再遍历集合时，如果发现元素已经被修改（被其他线程），立即抛出异常。 fail-safe是在遍历集合前，先复制一份，在复制上访问元素。 CopyOnWriteList 就是如此实现的，也可以是线程安全的List 13、详细介绍HashMap。角度：数据结构+扩容情况+put查找的详细过程+哈希函数+容量为什么始终都是2^N+JDK1.7与JDK1.8的区别。 14、HashMap如何实现线程安全？ConcurrentHashMap的底层实现？JDK1.7与JDK1.8的区别 15、正则表达式会写吗？ 16、设计模式了解吗？ 六大原则： 一、单一职责原则(Single Responsibility Principle) 二、开放封闭原则（Open Close Principle） 三、里氏替换原则（Liskov Substitution Principle） 四、依赖倒置原则（Dependence Inversion Principle） 五、 接口隔离原则（InterfaceSegregation Principles） 六、 迪米特原则（Law of Demeter）也称最少知识原则 17、linux指令知道哪些？ 文件操作相关的 cd，ls，mkdir，rm，cp，mv，cat，gzip，unzip，tar 进程管理相关 ps，top，nice， free 磁盘管理 df IO相关的 iostat 网络相关的 wget ping，netstat （查看端口占用之类的）JVM相关 1、JVM运行时内存划分？PC+虚拟机栈+本地方法栈+堆+方法区+JDK1.7与1.8区别 这三者每个线程一份 ———— 本地内存 虚拟机栈： 本地方法栈： 程序计数器： 堆和方法区所有线程共享 ———— 主内存 堆：所有对象（类对象和实例对象） 方法区（1.8之前也叫永久代，1.8之后叫元空间，移动给本地内存管理，不受jvm内存限制）：类的方法 常量池：包含两大类常量 （1.7之前在方法区中，1.7之后移动到了堆内） 字面量：如string 的文本字段，数字，声明为final的常量值 符号引用：类的完全限定名，方法名称，字段名称和描述符 2、堆内存分配策略 3、Full GC触发条件 4、如何判断对象是否存活？回收对象的两次标记过程。 5、垃圾回收算法以及垃圾回收器介绍，尤其是G1和CMS的优缺点 4种垃圾回收算法，标记-清除，标记-整理，复制算法，分代算法 7种具体实现的垃圾回收器， 新生代：Serial 收集器，ParNew 收集器，Parallel Scavenge收集器（注重吞吐量，自适应调节参数，不能和CMS一起工作，适合计算量的，不注重交互的服务器） 老年代：Serial Old 收集器，Parallel Old 收集器（吞吐量优先），CMS收集器（注重最短响应时间，减小停顿，真正的并发回收，采用标记-清除算法 优点：停顿时间短 缺点：更耗费CPU资源，标记-清除算法产生内存碎片，无法处理浮动垃圾） G1收集器：可同时工作在新生代和老年代 （并发和并行，注重停顿时间并可以预测停顿时间，空间整合，不产生内存碎片，直接管理整个堆-虽然保留了分代收集的概念， 缺点是：Region大小不好控制，占用内存大） 6、创建一个对象的步骤 类加载+分配内存+初始化零值+设置对象头+执行init方法 分配内存的两种方式：1 指针碰撞–堆内存规整 2. 空闲链表–堆内存不规整 并发分配内存的问题：1. CAS+失败重试 2. TLAB，预先给每个线程分配一块内存，线程先在TLAB创建对象，当TLAB空间不足时才用CAS算法。 init 方法（又叫实例构造器）包括：父类变量初始化 父类语句块 父类构造函数 子类变量初始化，子类语句块 子类构造函数 7、详细介绍类加载过程 加载+连接+初始化 加载的主要过程是获取Class字节流，在方法区生成对应数据结构，并在堆上最终生成类对象（类对象有指向方法区的指针，方法区就包含这个类的函数）。 连接又分为三步： 验证：检查Class字节流是否符合规范 准备：为类静态变量分配内存，初始化零值 解析：将符号引用替换为直接引用（方法名符号改为指向方法区的引用） 初始化：（执行cinit方法，类构造器）父类静态变量初始化，父类静态语句块，子类静态变量初始化，子类静态语句块 何时初始化：第一次使用类的时候才初始化 8、双亲委派机制，使用这个机制的好处？破坏双亲委派机制的场景？如何破坏？ 双亲委派机制就是类加载器加载类时，首先委派给父类加载器加载，如果父类加载器加载不了，再由自己加载。 好处是避免类重复加载，保证Java核心API不会被篡改 系统默认实现三个加载器，优先级为BootStrap （启动加载器）， Extension（扩展加载器），Application（应用加载器） 避免双亲委派机制：自己定义类加载器，继承ClassLoader类，重写loadclass 方法。 场景：Tomcat就是自己实现了自己的类加载器，部分理由：加载同一个类库的不同版本，Tomcat要部署多个web应用程序，每个web应用程序应该是隔离的（使用相同类库但版本不同，存在相同限定名的类），因此Tomcat实现了web应用类加载器，该加载器指加载该web应用目录下的类，而不会交给父类去加载。 注：这里的父类不是通过继承实现的，这里的父类更多是优先级的意思。 9、了解下tomcat的类加载机制 10、JVM性能调优，常用命令，以及工具对象访问的两种方法 句柄， 引用存储的是句柄的位置，句柄存储真实对象的位置 直接地址 差别：句柄的话 在对象移动时只需要修改句柄，不需要修改所以reference 直接地址 查询速度块，句柄需要查询两次地址。多线程并发 1、进程线程区别，线程安全和非线程安全区别 2、线程状态，start,run,wait,notify,yiled,sleep,join等方法的作用以及区别 start 是将子线程加入就绪队列，当子线程得到cpu时间片就开始执行。 run 是子线程运行期间要执行的内容，如果直接调用，就是普通的方法调用，相当于在父线程调用方法。 yiled 是将线程切换至ready状态，让出时间片给其他线程使用 3、wait,notify阻塞唤醒确切过程？在哪阻塞，在哪唤醒？为什么要出现在同步代码块中，为什么要处于while循环中？ 线程调用wait函数时，线程会释放目前占用的对象锁，状态变为waiting状态，线程会被放置到等待队列中，等待事件或中断的发生 线程调用notify函数，从等待队列中将一个等待的线程唤醒，唤醒的线程会重写尝试获取对象锁。获取成功后会从wait方法之后开始执行。 wait和notify都要拥有对象的锁才能调用，否则会抛出异常。 wait等待某个条件的情况下，即使被notify唤醒也不能保证条件还继续满足，因为可能其他线程在唤醒的这段时间里已经将条件变得不满足了。所有wait某个条件的话，唤醒了之后需要重新检查一遍，放在while循环中。 4、线程中断，守护线程 interrupt() 给线程的中断标志置为1，线程执行费时操作时应手动检查中断标志，如果中断标志出现，则应进行相关处理。 系统实现的大量阻塞函数再被中断时都会抛出中断异常，在需要中断的场景可以借鉴使用。 5、Java乐观锁机制，CAS思想？缺点？是否原子性？如何保证？ 6、synchronized使用方法？底层实现？ 减小加锁的范围， 底层实现通过monitor监视器方法，在进入同步代码前执行monitorenter 方法，获取对象锁，在退出同步代码是，调用monitorexit，释放对象锁。 7、ReenTrantLock使用方法？底层实现？和synchronized区别？ 两者都是可重入锁。ReenTrantLock 中文名是重入锁,synchronized 依赖于底层JVM,是java语法层面实现。而ReenTrantLock是依赖于API，是java代码层面实现的。 ReenTrantLock 比synchronized多了一些高级功能，等待可中断，可以实现公平锁，可以选择性通知。不过需要手动释放锁，不手动释放的话可能出现死锁。synchronized不需要手动释放。 synchronized经过优化后性能已不输ReenTrantLock 。 8、公平锁和非公平锁区别？为什么公平锁效率低？ 公平锁需要维护一个队列，有额外的开销 非公平锁后来的线程有几率不需要等待直接获取锁，减少了挂起的概率。公平锁只要队列有其他线程，线程就必须等待。 9、锁优化。自旋锁、自适应自旋锁、锁消除、锁粗化、偏向锁、轻量级锁、重量级锁解释 synchronized优化：锁粗化，锁细化，锁消除，偏向锁，轻量级锁，重量级锁。 10、Java内存模型 11、volatile作用？底层实现？禁止重排序的场景？单例模式中volatile的作用？ 底层实现是:内存屏障和lock 指令,lock指令保证了在对volatile 变量写操作后,会将缓存本地内存刷新回主内存.内存屏障保证了volatile的前后语句禁止重排序. 单例模式中volatile作用为禁止重排序,从而防止其他线程误读null. 12、AQS思想，以及基于AQS实现的lock, CountDownLatch、CyclicBarrier、Semaphore介绍 13、线程池构造函数7大参数，线程处理任务过程，线程拒绝策略 14、Execuors类实现的几种线程池类型，阿里为啥不让用？ CachedExcutor newFixedThreadPool SingleThreadExecutor 15、线程池大小如何设置？ 16、手写简单的线程池，体现线程复用 17、手写消费者生产者模式 18、手写阻塞队列 19、手写多线程交替打印ABCMySQL 1、事务4大特性，一致性具体指什么？这4个特性mysql如何保证实现的？ ACID, 原子性：通过事务回滚实现，undo logo 一致性：通过实现其他几个特性 隔离性：四大隔离级别，通过共享锁，独占锁，间隙锁，MVCC等实现 持久性：通过redo log，保存到磁盘实现 2、事务隔离级别，4个隔离级别分别有什么并发问题？ 3、Mysql默认隔离级别？如何保证并发安全？ 4、RR和RC如何实现的？RR使用场景？对比volatile可见性，为什么RR的事务要设计成不能读另一个事务已经提交的数据？ RC实现是修改的时候加锁，读取的采用MVCC，并行读。每次读都是已最新的数据生成数据视图。 RR实现修改同样加锁，读取MVCC是事务第一次执行Select语句会生成事务的数据视图，数据视图不会改变，保证可重复读。 一般大多数业务RC就可以满足需求。不过有些隔离性要求高的业务需要RR隔离级别（事务执行过程不能受到别人的影响），比如金融行业对账，计算上月余额-当月消费是否等于现在余额，如果余额在一直在变动，那就一直不一致了。 5、隔离级别的单位是数据表还是数据行？如串行化级别，两个事务访问不同的数据行，能并发？ 行，可以. 6、存储引擎Innodb和Myisam的区别以及使用场景 Innodb 支持事务，支持回滚，支持日志记录，支持崩溃恢复，支持行级锁，使用聚集索引，支持MVCC，支持外键。——适合 绝大数的场景 Myisam 上面不支持，默认表级锁，非聚焦索引，相对读性能较好，提供全文索引，压缩等特性。——写少，读密集应用 7、 介绍Inodb锁机制，行锁，表锁，意向锁 8、介绍MVCC. 9、哈希索引是如何实现的？ 10、B树索引为什么使用B+树，相对于B树有什么优点？为什么不能红黑树？要提到磁盘预读 红黑树太高，查询次数多。根据程序局部性原理，B+树可以有效的做到磁盘预读，而红黑树相邻内容相聚较远。 11、聚簇索引和非聚簇索引区别 聚焦索引叶子节点是数据节点，非聚焦索引叶子节点存储的是主键ID，需要回表查询主键索引才能取到数据。 12、回表查询和覆盖索引 覆盖索引是索引中包含原始数据，如果索引中数据满足查询要求，就直接返回数据，不需要回表查询。 回表查询是非主键索引只是查询到数据的主键ID，通过主键查询主键索引拿到数据。 13、如何创建索引？ 创建表的时候指定 在建表后使用Create Index命令建立 14、如何使用索引避免全表扫描？ 最左匹配原则,索引列不要参与计算,索引选择高区分度的字段进行建立。 15、Explain语句各字段的意义 16、最左前缀！！联合索引B+树是如何建立的？是如何查询的？当where子句中出现&amp;gt;时，联合索引命中是如何的? 如 where a &amp;gt; 10 and b = “111”时，联合索引如何创建？mysql优化器会针对得做出优化吗？ 17、MySQL中一条SQL语句的执行过程 首先是查缓存，缓存不匹配 给SQL解释器，解释SQL语句，之后送给SQL优化器，优化查询生成执行计划，最后送给SQL执行器执行获取数据。 18、数据库几大范式 19、数据库基本查询关键字使用，如left join on,where,beteen and,group by,having,limit,聚合函数等。 20、left join,right join,inner join,outer join的含义及区别 21、mysql主从复制过程，binlog记录格式，复制的异步半同步同步模式区别 22、主从复制或读写分离等数据不一致性问题以及如何解决 23、银行的话，可以会考mysql数据类型，如余额要用decimalRedis问题： 1、为什么使用Redis 2、分布式缓存和本地缓存有啥区别？让你自己设计本地缓存怎么设计？如何解决缓存过期问题？如何解决内存溢出问题？ 3、redis和mem***d的区别 4、redis常用数据结构和使用场景 5、Zset底层实现？跳表搜索插入删除过程？ 6、redis过期淘汰策略 7、redis持久化机制？都有什么优缺点？持久化的时候还能接受请求吗？ 8、redis事务 9、缓存雪崩和缓存穿透，以及解决方法 10、如何保证缓存和数据库的数据一致性？ 11、redis是单线程还是多线程？为什么那么快？ 12、五种IO模型的区别 13、select、poll、epoll的区别？ 14、redis热key问题？如何发现以及如何解决？ 15、redis数据分布方式？有什么优点？一致性hash呢？ 16、redis主从复制，主从切换，集群Spring 1、Spring IOC 2、Spring AOP，动态代理 3、Bean生命周期 4、Bean作用域？默认什么级别？是否线程安全？Spring如何保障线程安全的? 5、Spring事务隔离级别和事务传播属性 6、Spring以及Spring MVC常见注解 7、@autowired和@resource的区别，当UserDao存在不止一个bean或没有存在时，会怎样?怎么解决？ 8、mybatis如何防止sql注入？$#的区别是什么？传入表明用哪个？ 9、Spring MVC工作原理 10、SpringBoot自动配置的原理是什么？介绍SpringBootApplication注解. 11、Mybatis和Hibernate的区别 12、spring中的注解原理？例如事务注解，spring如何根据注解实现事务功能的 13、Spring中用到了哪些设计模式？单例、***、工厂、适配、观察者之类的说一说就行大数据和空间限制与系统设计10 亿 B = 1 GB = 2^30 1、100亿黑名单URL，每个64B,判断一个URL是否在黑名单中 将100亿黑名单通过hash分成1000个文件，相同url会被hash到同一个文件。根据target url的hash查找对应文件即可 为了实时效率可以考虑布隆过滤器 2、2GB内存在20亿整数中找到出现次数最多的数 hash方法，将20亿整数hash分成10个文件，然后逐个统计最多（开一个hash表，key是整数4B，value是整数4B）8B一条，一个文件1.6GB左右，再加起来 40亿整数计数的话，计数值初始化为-2^31 80亿整数的话，计数的时候判断是否超过2^31，超过就说明是最多的数了。 3、40亿个非负整数中找到没有出现的数 Bitmap 位图思考：建立Bitmap位图，一位代表一个整数，2^32 位代表整个整数范围。 空间上大概只需要500MB 4、40亿个非负整数中找到一个没有出现的数，内存限制10MB 分段划分，非负整数范围0~2^32，10MB内存，至少划分50份（保险期间划分64份），每份建立bitmap，判断未出现的数。 5、找到100亿个URL中重复的URL 将URLhash分配到20个文件中，平均一个文件5亿个URL（一个url的hash，2GB内存建立HashSet判断是否重复。 6、海量搜索词汇，找到最热TOP100词汇的方法 Hash 分成多份，每份统计前100，统计之后合并 7、40亿个无符号整数，1GB内存，找到所有出现两次的数 Bitmap 内存小就按照数字大小分段划分，每段建立Bitmap 8、10MB内存，找到40亿整数的中位数 按数字大小分段划分，每次统计一段范围的数的个数. 10MB内存，最多存250w个整数，40亿/250w=1600段，分为1600段，将40亿整数存到这些段的文件里面。根据统计第20亿个数即是中位数，找到中位数的文件，从里面找到中位数（快排）。 9、设计短域名系统，将长URL转化成短的URL.(知乎老哥给出了答案，博客有人根据他的总结了一下，很好) 10、让你系统的设计一个高并发的架构，你会从哪几个方面考虑？ 11、一个千万级的APP，你要搞定关注和粉丝列表，你用什么来做。要求最后一个关注的在最前面。新增和取关都要比较快的反馈你怎么做？如果一个人关注了之后，服务器宕机了怎么办？ 12、OOD design：计费停车场 13、假设有这么一个场景，有一条新闻，新闻的评论量可能很大，如何设计评论的读和写 14、显示网站的用户在线数的解决思路智力题 项目 如果个人没有好的项目，就用高并发秒杀。B站搜高并发秒杀，有慕课网视频，讲的很好。Github搜秒杀，有代码，跑一遍。有时间自己写一遍，没时间就看懂就好。然后准备问题，不要求每个功能都实现的特别好，但一定要多考虑系统可能出现的问题，因为很多人都是这个项目，你考虑的多，自然就显得稍微那么好了点。 1、如何解决超卖？mysql锁+redis预减库存+redis缓存卖完标记 2、如何解决重复下单？mysql唯一索引+分布式锁 3、如何防刷？IP限流+验证码 4、热key问题如何解决？redis集群+本地缓存+限流+key加随机值分布在多个实例中 5、消息队列的作用？如何保证消息的不丢失？异步削峰；发送方开启confirm+消息队列持久化+消费方关闭自动ACK,确保消费成功之后自动调用API进行确认。 6、缓存和数据库数据一致性如何保证？秒杀项目不用保证，其他项目就用延时双删或者先更新数据再是缓存失效，为防缓存失效这一信息丢失，可用消息队***保。 7、压测没有？用什么压测？什么情况？ 8、系统瓶颈在哪？如何查找，如何再优化？ 面经之手撕代码 但凡好点的互联网公司，必然要求手撕代码。如果代码都不让你写，要么这公司根本不行，要么这公司压根就不想要你，例如京东(三面加一块面我不到一个小时，果然没给做兄弟的机会)。就连华为都开始手撕代码了，想想吧——其实现场面试的手撕代码远比视频面试的在线答题好得多，因为在线可能需要调试运行，一点错都不能出，而现场手撕，只要思路没问题，时间复杂度和空间复杂度没问题，大致写出来就行，即使错了一点点，面试官可能也发现不了。 对很多大厂而言，手撕代码是万万不能出错的，大多题目都是LeetCode里面的，题一抛出，必须马上给出思路，然后直接写出来。如果需要面试官提示才能勉强答出，就已经落入下乘了(猿辅导可能就挂在二面代码题写得太慢了)。所以多刷题，这一环节的结果好坏其实就是取决于面试官的题目自己是否曾经刷到过，是否能够顺利答出。刷题的基本要求是完成剑指offer那70道题左右，加上自刷LeetCode130道中等难度的。然后将所面试公司的他人面经提到的代码题整理一下，查漏补缺，就没问题了。 代码题一般分为几类：排序、二分查找、数据结构、数组、字符串、链表、树、回溯、动态规划、贪心、数学。首先三大排序算法快排、归并、堆排序一定特别熟练，时间复杂度一定要特别熟悉。如果这三个都没有掌握，面试就先放放吧。快排如何写，什么思想？用快排思想求无序数组中第K小的值如何写？时间复杂度多少？归并如何写？什么思想？用归并思想给链表排序如何写？用归并思想求数组中逆序对数如何写？M个长度为N的数组如何排序？时间复杂度多少？M个长度为N的链表如何排序？堆排序如何写，什么思想？时间复杂度怎么算的？符合堆的结构，插入和删除函数如何实现？把这几个问题弄会，三大排序问题基本就没问题了。二分查找，有序数组直接考虑二分查找，如旋转有序数组的最小值、旋转有序数组查找目标值、有序数组目标值的最左和最右位置、二分查找寻找峰值，二分查找问题，要考虑while(left &amp;lt;= right)是否可以等于，right = mid还是right = mid +1,返回值是left还是right，反正多考虑考虑边界问题。数据结构，也就是栈、队列、双向队列之类的，问题不大。数组和字符串，简单的时候很简单，难的时候很伤脑子，这类题一出基本都是要求你使用最低时间复杂度和空间复杂度计算的，多背背题目，多看看面经总结总结吧。链表，掌握好链表逆序,快慢指针，保留pre节点和当前cur删除符合条件的节点，把这些掌握了，基本能解决八成的链表问题。树，掌握层序遍历+size+flag、非递归中序遍历、非递归前序遍历、二叉搜索树特点、完全二叉树特点及善用递归解决树深度，树是否平衡，树节点最大距离，树节点最长带权路径，最近公共父节点等问题，基本解决八成树问题。回溯，两大类排列和组合，总结排列组合类型的各自特点，以及考虑去重！动态规划，多做多总结，不过一般出现在笔试中，面试很少有特别难的动态规划，即使出也很并不难。贪心，少见，做几道就好。数学，挺难的，没做过类似的真未必想起来，主要就是位运算，其中&amp;amp;和异或最常见，还有就是0-n顺排，求第K位的数字是多少，以及0-N,数字7或者1出现的次数，利用random5实现random7等。 剑指offer的题目： No3.数组中重复的数字。n个数，范围0到n-1,找到任意一个重复的,时间复杂度O(1); 使用原有数组空间复杂度O（1），全-n 将问题转为环状链表，双指针找环入口点 No4,二维数组查找，从左到右递增，从上到下递增。 右上角排除法 No5,字符串空格替换，如果有空格，就将他替换成%20 No6,从尾到头打印链表,递归和非递归方法 No7 重建二叉树,前序遍历和中序遍历重构二叉树 根据规则来 No8 二叉树的下一个节点。中序遍历的二叉树下一个节点，这个树的节点，有指向父指针; 中序遍历右子树，右子树第一个节点 右子树为空，下一节点是父节点， No9,两个栈实现队列 No10 斐波那契数列; No 10-1,青蛙可以挑一阶，也可以2阶，跳到n阶，几种做法 斐波那契额数列 No10-2,变态跳，可以跳1-n阶，随便跳。求n阶几种方法; No11,旋转数组最小的数字：非减排序的数组，可能存在重复 二分查找，找到旋转点，旋转点就是最小的 No12,矩阵中的路径。矩阵有字符，可以从任意开始，上下左右任意走，但不能走走过的格子，求是否存在一条包含给定字符串的路径。 深度优先遍历 No13,机器人的运动范围，从(0,0)开始，上下左右移动，但不能进入行坐标和列坐标数位之和大于k的格子，给定k,能打几个格子。 广度优先遍历 No14,剪绳子，长为n，剪m段（m,n都是整数，且m&amp;gt;1），所有的长度的乘积最大为多少。 3最好 No15,二进制中1的个数; No16, 数值的整数次方 快速冥 No17, 打印从1到最大的n位数 相当于n位数的全排列 No18-1,删除节点。给定一链表头结点和指定节点，O(1)时间内，删除指定节点 No18-2.删除重复的节点。存在的重复节点，全都删了。 No19,实现函数来匹配包含‘.’和‘*’的正则表达式. 背 No20实现一个函数来判断字符串是否表示数值。 No20-2将字符串转换成整数；不合法返回0； No21,调整数组顺序使得奇数位于偶数之前。 patition操作 No22,返回链表的倒数第k个节点。 双指针 No23,链表中环的入口节点; 双指针 No24,反转链表,并返回反转之后的头结点。 No25,合并两个有序的链表，重建一个新的，包含所有的两个链表。—&amp;gt;单调不减 No26,输入A和B两个树，判断B是A的子树。空树不是子树 No27,二叉树的镜像。输入一个二叉树，将他变成他的镜像 左右交换 No28,输入一个二叉树，判断二叉树是不是对称的。 递归思想，拆分为两棵树，自己和自己 No29,顺时针打印矩阵。 No30,包含min函数的栈 单调队列 含有min的链表 No31,栈的压入弹出序列,第一个表示压入序列，判断第二个是不 是弹出序列。 模拟 No32,从上到下打印二叉树 层次遍历，一个queue，一个循环，一个反转走天下 No33,输入一个整数数组，判断是不是二叉搜索树的后序遍历序列 中序遍历是递增的 后序遍历，单调栈 No34,二叉树中和为某一值得路径 递归子树 No35复杂链表的复制;链表不仅有val,next,还有一个指向任意节点random O(N) 空间复杂度 正常复制,将新旧节点的对应关系存到map里,再重来一遍修改random O(1) 时间复杂度 每次复制新节点在老节点后面,复制完后拆分开 No36二叉搜索树和双向链表;二叉搜索树转换成一个排序的双向链表，不能创建新的节点,只能调整指针顺序。 中序遍历 No37实现函数序列化二叉树和反序列二叉树 序列化就是层次遍历,注意的可以加入null节点 反序列化就是重新建立二叉树,一样是层次遍历 No38输入一个字符串，给出所有的排列; 全排列问题，一个字符不可使用两次，比全排列多了个同一个位置不使用相同的字符就行。 No39,数组中出现超过一半的数字,如果不存在，返回0； 摩登投票 No40，最小的k个数 最大堆，求出最小的k个数 partion 分治法 No41，数据流中的中位数，奇数个，中间那个。偶数个，中间俩的平均. 两个堆，一个大顶堆，一个小顶堆 No42,连续子数组的最大和，数组中有正数有负数，求所有连续子数组的最大和。 状态转移方程。前面连续的&amp;gt;0加上，&amp;lt;0自立 No43,1-n所有数中，求所有十进制位出现1个总数 No44，从0-n,所有的从前到后排到一块，实现一个函数，求第k位的数字是几？ No45,给定一个数组，求组合到一块的最小数字 数组排序 No46,0-25翻译成a-z,给一个数字，求有几种翻译方法 状态转移 No47,礼物的最大价值，矩阵，每一步都有一个值，从左上到右下，只能向右或向下移动，求最大和 状态转移 No48，最长不含重复字符的子字符串， 一个Map，存字符出现的index，废弃的index存起来也无所谓。 滑动窗口，数组做hash set No49,只含2,3,5因子的数是丑数，1也是，求第n个丑数 动态规划 No50，第一个只出现一次的字符,返回的是位置 数组hash set No51，数组中的逆序对,归并排序！！！ No52,两个链表的第一个公共节点。 No53,在排序数组中查找数组，一个数字出现了多少次 No54,二叉搜索树中，第K小节点 No55-1，二叉树深度 No55-2,是不是平衡二叉树 No56-1,一个数组，一个出现一次，其他出现两次，求一次的。 一个一次 全部异或即是 两个一次，全部异或，找到最低位的1，根据最低位的1将所有数分成两边异或 No56-2,数组，一个出现一次，其他出现三次。求一次 No57-1，递增排序的数组，和target.找出一对和为target的数字 双指针 No57-2,正数s,给出连续正数序列，其和未s.所有的序列。序列最少有俩。 滑动窗口 No58-1,翻转单词顺序 No58-2,左旋转字符串 No59-1给出数组和窗口大小，求滑动窗口的最大值。 优先级队列—— 滑动窗口求连续数组最大值 No59-2,实现带有max函数的队列，出入队和max都是O(1) 优先级队列 No60,n个色子，点数和为s，s所有可能出现的值得概率 动态规划 No61,扑克牌抽5个数，是不是顺子。大小王可以为任何; No62,0到n-1,围一圈，从0开始，删除第m个数字。求最后的数字； 倒序思考，约瑟夫环问题 No63,数组，值为股票当时的价格，求最大利润 No65,不用加减乘除做加法 No66，构建乘积数组。 LeetCode的一些题目： 1. Two Sum:给定数组和target,返回两个元素之和为target的元素index; 2. Add Two Numbers：他是一个反转的链表，求真正链表代表数字的和的链表的翻转； 3. Longest Substring Without Repeating Characters：字符串最长的不重复子序列长度： 5. Longest Palindromic Substring：字符串最长的回文序列。 6. ZigZag Conversion：一个字符串，一个行数。然后字符竖折竖折来排序。然后一层一层的组合字符。 7. Reverse Integer：将整数值反转，如120变21，-21变-12，考虑值超过int溢出。 8. String to Integer (atoi)：将字符串转化为整数 9. Palindrome Number，一个数，正反来是否一样。负数不一样。 12. Integer to Roman，给定一个数，转化成罗马数字。 14. Longest Common Prefix；N个字符串，求这N个字符串最长公共前缀 15. 3Sum,数组中，找3个和为0为三个数，每个数不能用两次。要考虑重复的问题。 16. 3Sum Closest:一个数组，找出3个和最进阶target的。 17. Letter Combinations of a Phone Number，给定数字，求九宫格拼音下的所有组合。回溯 18. 4Sum.数组中4个数之和为0，考虑重复的数. 19,删除倒数第k个节点 20、括号序列是否匹配： 21、合并两个有序链表，递归和非递归方式。 22、n对小括号的全正确排序：回溯 24. 成对的翻转链表 26、有序数组，删除多余的重复数组，in-place删除，然后返回长度。 27、数组，删除指定的数字； 28、实现indexOf(String s1,String s2); 33、左移一部分的有序数组中，查询target。二分查找的经典题目 34、有序数组中，查询一个target第一次和最后一次出现的位置；二分查找的经典题目 35、将一个数插入到有序数组中，求位置 39：非重复数组，找出和为target的数组，一个不能用两次；回溯。 40，带有重复的数组，找出和未target的数组组合，一个不能用两次；回溯+组合 43，两字符串的数字，相乘，得到结果，输出字符串。 46,不重复数组的，所有全排列；回溯+排列 47，带有重复的数组，全排列；回溯+排列+去重 48：n x n矩阵，顺时针转动90度。 49，一群字符串，有些字符串字母组成一样，有的不一样，将一样的放一块； 50：计算power(x,n); x在正负100，n在正负 2^31; 54:一圈圈顺时针打印数组。 58：最后一个单词的长度。 59：给一个N,将1-N^2,整成上面外圈顺时针到里圈的矩阵。 60：1-N这n个数字的全排列，求第K小的； 61、翻转链表，链表右移K为，可以循环多移； 62、矩阵移动，mxn矩阵从左上角移动到右下角，只能横移和竖移，共有多少方法； 63、矩阵移动，从左上移动到右下：横竖移动，矩阵值为1的不能走；求多少种移动方式。 64、矩阵，从左上到右下，最短的路径和多少。 66、数组表示的数+1，然后返回之后的。 67、两个二进制字符串相加，得到的二进制字符串。 69. Sqrt(x) 实现int sqrt(int x)； 73、MxN的数组，如果有一个为0，那么就让起其所有列和行都为0，求最后的数组； 74、mxn的矩阵，层序遍历就是有序的数组。在矩阵中搜索target; 75：一个只有2、1、0的数组，将0分到左边，2移动右边。 77:1-N，挑选K个进行排列； 78：求数组的所有子集合。 80：删除有序数组中过于多余的数字，最多每个出现两次。 81：翻转部分的有序数组，可能存在重复，搜索target; 82：删除有序链表，将重复的全都删了； 83：删除有序链表多余的节点，重复的保留一个。 86：链表，给定target，将比target小的节点移动到左边。大的和等于的移动到后面。其他相对位置不变。 88：将两个有序数组合到第一个数组中； 90：可能有重复元素的数组，求所有的组合。 91：将数字解码成A-Z;1-26;给出数字字符串，有几种解码方式： 92：逆转链表的m-n这部分； 93：给定一数字字符串，输出可能的IP地址列表。 94,二叉树中序遍历： 96：n个节点，共有多少种二叉搜索树。 98:判断一个树，是否是二叉搜索树。 100：判断两个树是否一致。 101：判断一个树是否对称。 102：二叉树层序遍历。 103,之字形打印二叉树 104:二叉树的深度: 105：前序和中序构成二叉树。 106：中序和后序构成二叉树 107：层序遍历：从底层到上层； 108: 将有序数组转化成二叉搜索树 109：将有序链表转化成二叉搜索树： 110:是否平衡二叉树： 111：叶节点到根节点的最短路径。 112：二叉树是否存在从根节点到叶节点的路径和为target: 113:找出所有路径和为target的序列 114：将二叉树往一边偏； 116：满二叉树，将为每个节点加上next，就是每层的右边那个。 117：二叉树，将为每个节点加上next，就是每层的右边那个。 125：字符串是否回文，只考虑数字和字母，大小写不论。 129：二叉树，从头结点到尾节点：拼接成数字，然后所有的加一块。 136,一个数组，一个出现一次，其他的出现两次，求一次的那个 137：数组，有人出现三次，有人出现一次，求一次的。 138：带有random指针的复杂链表的复制： 141. 链表是否有环： 142：链表环入口节点 143：重排链表：1-2-3-4-5；先第一，再最后一个，先第二个，再倒二，就这样。 144：前序遍历二叉树： 146. LRU Cache 147：对链表进行插入排序， 148：NlongN的时间排序链表。 150、逆波兰 151、单词逆序： 155、带有最小值函数的栈 160：两个链表的相交点 165：比较版本号大小。如10.2.2大于3.1.2； 167：有序数组，找出两数之和为target的索引。 168：数字转化为26进制的A-Z； 169. Majority Element，超过一半的数字 171：26进制A-Z转化为十进制数字 172：Given an integer n, return the number of trailing zeroes in n!. 179：整数数组，求组合到一块的最大值。 189：数组左移K位 199：二叉树，从右边看到的节点数组。也就是每层最右边那个。 200：dfs求岛屿的个数。1表示陆地，0表示岛屿。 201：N-M数组的连续数，逐个&amp;amp;；求最后的结果。 209 : 最短的连续子序列之和大于等于target; 215：无序数组中找出最K大的数字： 216：1-9的数，选出n个，之和为k。求所有N的组合，一个数不能用两次 220：一个数组，是否存在不同的数i和j,使得nums[i]-nums[j]的绝对值最大是t,而i-j的绝对值最大是k; 221：矩阵，有0有1；找出最大的正方形，其里面数值都1； 222：给定一完全二叉树，求节点总个数。 223：以(A,B)和（C,D）为对角顶点，构建矩形，以EF，GH构建，求纵的矩形面积。 227：非负整数，有+-*/和空格数字的字符串组成的表达式，求表达式的值； 228：给出排序好的数组，没有重复，将连续的整成0-&amp;gt;2的形式，求所有的连续范围段。 229，数组中超过n/3的次数的数。 230:BST二叉搜索树中，第K小的节点，递归和非递归方式 236：二叉树中，两个节点最近的公共祖先节点。 238：构建乘积数组 240：在矩阵中遍历，每行都增加，每列都增加。 241：数字和操作符，+-*,请随便加括号改变计算顺序，然后得出所有可能的结果。 260：数组，只有两个出线一次，其他的出现两次，求出现一次的那俩 264：第K个丑数，因子只有2,3,5的数； 274：if h of his/her N papers have at least h citations each, and the other N − h papers have no more than h citations each. 275：同上，也是求H。但这个数组是有序的； 287：N+1数，都是1-n之间，求重复的那个数。 300：最长递增序列，可以不连续。但是前面的index比后面的小 306：数字字符串，我们将字符串分割最少三个数字，然后Fn = Fn-1+Fn-2; 313：求第N个超级丑数，并且给出数组因子，只有只包括该数组因子的数才是超级丑数 318: 字符串数组，求两个没有公共字母的字符串长度乘积的最大值； 319：开关转换，刚开始有N的灯，最初都是灭的，第一轮，每一个都按一个。第二轮每二个按一个开关。第N轮之后，亮着的灯的个数。 322：给一堆零钱，可以重复使用，然后给整钱。将整钱换成零钱，求最小的零钱数，如果不能换，返回-1； 324：无序数组重排序：nums[0] &amp;lt; nums[1] &amp;gt; nums[2] &amp;lt; nums[3]…. 328：链表重排序，将奇数位index,而不是奇数值的放到前面。偶数index的放后面： 331：将一个二叉树，他的空子节点记成#，给定一个字符串，问这个是不是前序遍历； 332 : 航班信息[from,to],一些列的这种数组信息，这人从JFK出发，求他的真正路线一次经过的城市； 334：数字数组，是否存在三个子序列，可以不连续，是递增的； 338：给定一num.求0-num这些数每个的二进制中1的个数。 347，TopK出现频率最高的数字，一个数组，每个数字可能出现多次，求最K多出现的那k个 343：给定一个整数，将他分成最小二段，和为这个整数，求这些段乘积的最大值。 357:n位数的数，求没有重复数字的总个数。11,121这种就不行。 面经中遇到的题： 1、数组的逆序数 2、LRU //hashMap加双向链表，双向链表有头尾节点， 3、最长回文序列 leetocde 5 4、矩阵中的最长递增路径，可以上下左右一起都走； leetcode329 5、判断一个二叉树是另一个二叉树的子树 6、归并排序的时间复杂度 // NlongN 7、求给出01矩阵中的最大正方形面积（全为1） lc221 8、求二叉树中距离最远的节点 leetcode543 9、判断字符串是否为合法IPV4地址 lc468 10、数组值为1-n，各出现一次，先加入x（x也是1-n的范围），找出x 11、给定n，计算15n，不用+*/ 12、给定字符数组chars，将其右移n位 13、100层楼，只有两个鸡蛋，找出鸡蛋会在哪一层楼被摔碎 14、reverse linked list in a group of k 15、如何空间O(0)实现两个数的互换 16、IP地址的Regex 17、the longest path in a binary tree lc124 18、the largest consecutive sum in an array 19、LeetCode 41 Find mis sing positive 20、给一个小于一亿的中文数字字符串,转化成数字格式 21、一个数组,把所有的0都移动到末尾,还要保持顺序不乱 维持临界点j,如果当前遍历不是0，就和j互换 22、罗马数字转整数 leetcode13 23、二叉树的序列化和反序列化 24、输入一个数组，输出数组中满足条件的数字，条件为：数组中当前元素的值大于等于它前面所有的元素，小于等于它后面所有的元素。 25、给出一个数字，对数字的两位进行交换，只能交换一次，输出可能结果中的最小数字 26、输入一个字符串，字符串中字符全部为数字，在字符串中插入 ‘.’ 使得结果为合法的ip地址，输出全部可能的结果 27、基数排序 28、链表是否为回文结构。不能用栈 29、最大不重复子串 30、复杂链表复制 31、长度为n的数组，元素大小是0~n-1,判断数组元素是否有重复的 32、list1/list2交替打印元素 33、36进制加法 34、合并区间 35、快排 36、生产者-消费者 模型 37、排序一个字符串时间要求O(n) 38、给一个有重复数字的数组，求集合{(a,b,c) | a+b+c=0} 39、两个栈实现队列 40、二叉树转化为双端链表 41、手写线程池 42、之字形打印二叉树 43、给定一个数组，调整该数组，使其满足堆的性质 44、给定n个单词，如果单词组成一致但是元素顺序不一致，该对单词为同位词，例如：abc,bca为同位词．求所有同位词的集合输出 45、链表，两个链表的公共点 46、二叉树的后续遍历非递归形式 47、买卖股票的最佳时机，只能一次买入和一次卖出 48、可以进行多次交易的结果，求赚取的最大利润 49、(A,B)(A,C)(B,D)(D,A)判断是否有循环引用，提示用拓扑排序 50、数组找是否存在和为M的两个数 51、KMP 52、实现一个阻塞队列（生产者消费者模型） 53、找出10000个数据中第 k 大的数 54、输入一个字符串，包含数字、加减乘除和括号，输出结果，编程 55、给定一个数x，要求使用k个数字求和可以得到x，数字从1-9中选择，不能重复。 56、输入一个正整数 N，返回 N 个 ‘(‘ 和 N 个 ‘)’ 的所有可能情况 57、76.minimum-window-substring、30.substring-with-concatenation-of-all-words、42.trapping-rain-water， 58、求树的最左下节点 59、无序数组中第k大的数（quick select） 60、求旋转数组找最小值（二分） 61、判断二叉树是否镜像（递归） 62、给定一个矩阵，从左上角开始只能往下或者右走，求到达右下角的最小权值路径 63、字符串转Int，如果越界就返回0 64、lc400 65、单向链表实现加法 66、打家劫舍 67、收到礼物最大值 68、五张牌，其中大小鬼为癞子，牌面为 0，判断这五张牌是否能组成顺子 69、给定一个字符串打印所有的子串，要求不重复 70、自然数1-n,排一块组成的字符串，求第k位是什么。 71.如果a[0]&amp;lt;a[1],a[n-2]&amp;gt;a[n-1]，那么请找出任意一个点使得a[i-1]&amp;lt;a[i]&amp;gt;a[i+1] 要求logN 72、a[-1]和a[n+1]设为负无穷大，二分查找找到数组中的一个峰值。 73、如果有一组数字，按照“拿出第一个数在桌上并然后将下一个数放到队尾”一直操作直到数字全部放在桌子上，给你最后在桌子上的数字，请返回最开始数字的顺序。 74、有序数组找到第一个小于0的数和第一个大于0的数 75、合并两个排序数组并去重 76、两个排序数组找中位数 77、两个超大整数的字符串做减法运算。 78、1~10000中7出现的次数，如17算1个，77算2个。 79、给一个字符串数组，统计每一个字符串出现的次数，要求不能用set,map.时间复杂度O(n). 80、手撕最大堆，实现对应的push和pop操作 81、找出一个字符串中所有的回文子串 82、重复次数最多的最长连续子串（即找到重复次数最多的子串，若有多个，输出最长的） 83、长度为n的数组，有一个长度为k的滑动窗口，询问各个滑动窗口内的中位数。 84、区间最大最小值。两个长度为n的序列a,b,问有多少区间[l,r] 满足max(a[l,r])&amp;lt;min(b[l,r])。即a[l,r]的最大值小于b[l,r]的最小值 85、8皇后问题共有多少种解法 86、一个数字串删除指定个数的数字字符，剩下的组成一个最大的数 87、N个长度为K的有序链表合并，时间复杂度，空间复杂度 88、N个长度为K的有序数组合并，时间复杂度，空间复杂度 89、用一个栈去排序另一个栈 90、一个数组实现两个栈 91、一个n位数，现在可以删除其中任意k位，使得剩下的数最小 92、实现有符号大数链表加法，靠近头结点位置为高位 93、找出来数组中每个元素后边第一个比它大的值 94、完全二叉树的最大深度与节点个数 95、两个有序数组交集、并集 96、用二分法对一个数字开根号 97、一个无序有正有负数组，求乘积最大的三个数的乘积 98、实现链表，无序链表，对链表值奇偶分离并排序，空间复杂度O(1) 99、无序数组构建一棵二叉排序树 100、打印出根节点到叶子节点的最长路径 101、字符串形式自定义进制大数相加 102、LeetCode 1038 103、任意一个整型数组，判断是否可以将数组分为三个区间，每个区间中数值的和相同 104、二叉树逆时针打印最外层节点 105、无向图最短路径 106、输入一个矩阵，起始点和目标点，判断是否存在可达路径" }, { "title": "Redis学习", "url": "/posts/Redis%E5%AD%A6%E4%B9%A0/", "categories": "学习", "tags": "Redis", "date": "2021-06-06 00:00:00 +0800", "snippet": "Redis学习Redis数据结构 string–&amp;gt;简单的key-value list–&amp;gt;有序列表(底层是双向链表)–&amp;gt;可做简单队列 set–&amp;gt;无序列表(去重)–&amp;gt;提供一系列的交集、并集、差集的命令 hash–&amp;gt;哈希表–&amp;gt;存储结构化数据 sortset–&amp;gt;有序集合映射(member-score)–&amp;gt;排行榜string encode_int encode_embstr encode_ramList ziplist linkedlisthash ziplist hash tableset intset 整数有序集合（一个数组） hash tablesortset ziplist skiplist 跳表为什么Redis选择使用跳表而不是红黑树来实现有序集合？Redis 中的有序集合(zset) 支持的操作： 插入一个元素 删除一个元素 查找一个元素 有序输出所有元素 按照范围区间查找元素（比如查找值在 [100, 356] 之间的数据）其中，前四个操作红黑树也可以完成，且时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高。按照区间查找数据时，跳表可以做到 O(logn) 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了，非常高效。Redis 持久化两种 RDB（快照持久化） RDB持久化所生成的RDB文件是一个经过压缩的二进制文件，Redis可以通过这个文件还原数据库的数据。 AOF文件 AOF是通过保存Redis服务器所执行的写命令来记录数据库的数据的。 -AOF 重写 主从复制模式三个阶段 连接建立阶段 保持主节点信息，建立socket连接，身份验证等等 数据同步阶段 完全同步：常见于服务器第一次同步，主服务器生成RDB文件，将RDB文件发给从服务器，从服务器由RDB文件建立缓存 部分同步：常见于短线重连，断线重连时只同步缺失的数据。主从服务器分别记录数据偏移量offset，主服务器不仅将命令发给从服务器，还将写命令写入复制积压缓冲区。在短线重连时，只要缺失的写命令还能从复制积压缓冲区中，就只需要部分同步。否则完全同步。 另外，从服务器会记录上次复制的主服务器的 Run ID，断线重连时如果run id 不一致，说明之前复制的主服务器和现在服务器时两台服务器，则进行完全同步 命令传播阶段 每次主服务器执行写命令后，将命令发给各个从服务器，从服务器执行相同的命令。 另外还会默认以1s一次向主服务器发送心跳检测 检测主从网络状态 检测是否由命令丢失，从服务器发送的包带有offset 主从复制的一些问题 数据存在延迟和不一致 数据过期的问题 数据已经过期，但是从服务器上还存在 可用性问题 主节点死掉，则无法执行写命令——通过哨兵机制解决。 写单点问题，存储能力受单机限制—— 通过集群模式解决 哨兵模式启动多个哨兵，哨兵是一种特殊的redis节点，本身不存储数据，只监视其他节点。定时任务：每个哨兵执行3个定时任务来监视其他节点 向主从节点发送info 命令获取最新的主从结构 接收其他哨兵节点的信息（推选，客观下线时有用） 向其他节点发送ping命令进行心跳检测，判断是否下线主观下线：心跳检测时节点超过一定时间没有回复，则认为主观下线客观下线：如果时主节点被主观下线后，哨兵节点会询问其他哨兵节点，如果超过一半的哨兵节点都认为其下线（所以哨兵节点数量应为奇数），则认为主节点下线，开始故障转移。选举领导哨兵节点：先到先得选取领导者故障转移：领导哨兵节点 选取最合适的从节点成为新的主节点。根据优先级/延迟/复制偏移量等选取。集群模式集群模式首先时将数据分片，将整体数据分布到多个redis节点上，从而扩展容量实现集群。数据分片算法采用哈希槽的分片算法。（1）Redis对数据的特征值（一般是key）计算哈希值，使用的算法是CRC16。（2）根据哈希值，计算数据属于哪个槽。（3）根据槽与节点的映射关系，计算数据属于哪个节点。将数据分开存储在不同主节点上后，就实现了数据的扩容和单写节点的问题。每个主节点还可以再搭配多个从节点，实现主从复制，实现读写分离。另外每个主节点充当集群中的哨兵，保障高可用性。集群模式就是再哨兵机制上加一层数据分片。在Redis3.0前，官方并不支持集群模式，可以通过一个中间件代理（Proxy），该中间件存储数据分片的位置，每个数据分片就是一个哨兵模式（主从+哨兵）。也可以实现集群模式。集群模式的缺点：实现复杂，资源开销大，不适合数据量小的情况。默认从节点不分担读请求，只作为数据备份保障高可靠性，资源开销大Key批量操作受限制，数据分布再多个节点上，对Key进行批量操作时只能对同一节点的key操作，不同节点受限制。Key 作为数据分区的最小粒度，不能将一个很大的键值对象如 hash、list 等映射到不同的节点。Redis 分布式锁https://xiaomi-info.github.io/2019/12/17/redis-distributed-lock/Redis 锁主要利用 Redis 的 setnx 命令。（set if not exit）加锁命令：SETNX key value锁超时：EXPIRE key timeout（必须要有的，保证即使锁未被显示释放，也能超时释放，避免死锁）解锁命令：DEL key SETNX 和 EXPIRE 非原子性——通过lua脚本解决（原子性操作） 业务执行时间大于锁超时—— 锁误消除，并发执行 锁的value记录占有锁的线程ID,释放锁时判断线程ID是否为本线程 将过期时间设置足够长，确保代码逻辑在锁释放之前能够执行完成。 为获取锁的线程增加守护线程，为将要过期但未释放的锁增加有效时间。 不可重入 本地记录锁重入次数 Redis Map 记录 无法等待锁释放：被锁阻挡的线程希望在锁释放后继续执行操作 轮询 使用Redis 发布订阅功能，订阅锁释放消息 常见问题缓存雪崩缓存雪崩的情况是说，当某一时刻发生大规模的缓存失效的情况，比如你的缓存服务宕机了/大量缓存过期，会有大量的请求进来直接打到DB上面。结果就是DB 称不住，挂掉。解决办法： 使用集群，保障高可用性 （事前） Mysql限流+本地缓存，避免主要业务挂掉（事中） 开启Redsi持久化机制，尽快恢复缓存（事后） 对于大量缓存同时间过期，给过期时间加上随机值缓存穿透请求去查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去，这种查询不存在数据的现象我们称为缓存穿透。解决办法： 缓存空值 Bool 过滤缓存击穿在平常高并发的系统中，大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为缓存击穿。解决办法：已知多个线程查询同一条数据，可以加上互斥锁，第一个线程从数据库查询，其他线程等待。第一个线程查询到数据后加入缓存，之后线程直接查缓存就OK。缓存与数据库双写一致可以先删除缓存，然后更新数据库。——延时双删也可以先更新数据库，然后删除缓存。——消息队列缓存和数据库会存在短期内的不一致，所以最简单的办法是设置缓存的过期时间（虽然存在短时间的脏数据，旧数据）复杂一些的解决办法是设置消息队列，热Key问题线上redis集群，如果一个key非常火（比如明星效应）导致所有访问都冲往一个redis节点，redis直接崩溃。咋解决：分两步：1.首先发现热key，发现热key的方法有许多： 事先估计，通过人为估计热key 客户端对key进行流量统计，流量超过阈值的视为热key 在代理层流量统计 无代码侵入式的，其他系统监听数据包/爬取其他top榜单等2.通知系统进行处理 客户端本地缓存（如开一个Map，或者ehcache） Redis 集群数据备份，开几个相同的Redis来顶。Redis 一致性hash算法将hash值映射到一个2^32大小的圆环上，每次查询顺时针方向上距离最近的服务器。如果新增/删除服务器，只会影响邻近的服务器，rehash影响有限。问题：数据倾斜不均衡解决：增加大量虚拟节点，保证虚拟节点的均衡，之后将虚拟节点映射到真实节点。通过这种方式，新增/删除服务器只需要修改虚拟节点的分配就行。" } ]
